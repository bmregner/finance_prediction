{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turn Json files into pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#nltk.download() #download text data sets, incl stop words\n",
    "\n",
    "import pandas as pd\n",
    "import ijson\n",
    "import json\n",
    "import gensim\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = 'Earnings_Preview_Facebook_Inc_NASDAQFB_moved_up_10_times_out_of_last_17_quarters__The_Independent_Republic.json'\n",
    "\n",
    "def read_Google_articles(filename):\n",
    "    \"\"\" \n",
    "    Converts Google News JSON file into a data frame. Takes in\n",
    "    a .json file and returns a dataframe using the json's dictionary-like\n",
    "    structure \n",
    "    \"\"\"\n",
    "    fin = open(filename)\n",
    "    mydict = json.load(fin)\n",
    "    fin.close()\n",
    "\n",
    "    df = pd.DataFrame.from_dict(mydict, orient = 'index')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>body</th>\n",
       "      <td>Facebook, Inc. (NASDAQ:FB) is projected to dec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <td>Facebook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <td>Earnings Preview: Facebook, Inc. (NASDAQ:FB) m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          0\n",
       "body      Facebook, Inc. (NASDAQ:FB) is projected to dec...\n",
       "category                                           Facebook\n",
       "title     Earnings Preview: Facebook, Inc. (NASDAQ:FB) m..."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydf = read_Google_articles(filename)\n",
    "mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Facebook, Inc. (NASDAQ:FB) is projected to declare fiscal fourth quarter financial results right after the stock market\\u2019s official close on February 01, 2017. The stock added about 1.4 percent in price since last results when it was at $127.17 a share. Based on the most relevant past-periods data, there is an 58.82 percent probability for this firm\\u2019s share price to go up following next quarterly results. Earnings reaction history tells us that the equity price moved up 10 times out of last 17 reported quarters. It has beaten earnings-per-share estimates 58.% of the time in its last 12 earnings reports. It fell short of earnings estimates on 4 occasions, and it has met expectations 1 time.\\n\\nHere\\u2019s how traders responded to FB earnings announcements over the past few quarters.\\n\\nGiven its history, the average earnings announcement surprise was 21.11 percent over the past four quarters. Back on November 2, 2016, it posted earnings per-share earnings at $0.88 which beat the consensus $0.77 projection (positive surprise of14.29%. For the quarter, revenue came in at 7.01B versus consensus estimate of 6.92B. The stock dropped -5.64 percent the session following the earnings reports were released, and on 7th day price change was -3.14 percent.\\n\\nOn July 27, 2016, it reported earnings at $0.76 a share compared with the consensus estimate of $0.62 per share (positive surprise of 22.58%). Revenue of 6.44B for that quarter was above the $6.02B analysts had expected. The stock climbed 1.35% the day following the earnings announcement, and on 7th day price change was -0.67%.\\n\\nOn April 27, 2016, it recorded $0.57 a share in earnings which topped the consensus estimate of $0.44 (positive surprise of 29.55%). Revenue for the quarter was $5.38B while analysts called for revenues to be $5.26B. The stock gained 7.2% the day following the earnings data was made public, and on 7th day price change was 8.42%.\\n\\nOn January 27, 2016, it announced earnings per share at $0.59 versus the consensus estimate of $0.5 per share (positive surprise of 18%). That came on revenues of $5.84B for that period. Analysts had expected $5.37B in revenue.\\n\\nAs Q4 earnings announcement date approaches, Wall Street is expecting earnings per share of $1.07. The analysts\\u2019 present consensus range is $0.89-$1.3 for EPS. The market consensus range for revenue is between $7.19B and $7.84B, with an average of $7.62B.\\n\\nFacebook, Inc. (NASDAQ:FB) last ended at $129.37, sending the company\\u2019s market cap near $372.87B. The consensus 12-month price target from analysts covering the stock is $153.72. The share price has declined -3.09% from its top level in 52 weeks and dropped 12.45% this year. It recently traded in a range of $128.38-$129.9 at a volume of 15136724 shares. The recent trading ended with the price nearly 1.17 higher for the last 5 trading days, rebounding 37.29% from its 52-week low.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article = mydf.iloc[0][0]\n",
    "article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now clean the body/article\n",
    "#### some things are not cleaned (such as B for billion and s after 2019. Clean later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# kaggle tutorial method: https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words\n",
    "letters_only = re.sub(\"[^a-zA-Z]\", \" \", article)\n",
    "\n",
    "# Insight Jupyter notebook method\n",
    "# article.decode('utf-8').lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "## convert to lower case and split into individual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "432"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# kaggle method\n",
    "lower_case = letters_only.lower()\n",
    "words = lower_case.split()\n",
    "\n",
    "# Insight Jupyter notebook method (lens of results are the same)\n",
    "re_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "article_tokens = re_tokenizer.tokenize(lower_case)\n",
    "\n",
    "len(article_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words\n",
    "\n",
    "#### can maybe add more stop words by looking at collection frequency. Currently using stopwords dictionary.\n",
    "\n",
    "#### 'u' means python represents each word as unicode string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers', u'herself', u'it', u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'which', u'who', u'whom', u'this', u'that', u'these', u'those', u'am', u'is', u'are', u'was', u'were', u'be', u'been', u'being', u'have', u'has', u'had', u'having', u'do', u'does', u'did', u'doing', u'a', u'an', u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until', u'while', u'of', u'at', u'by', u'for', u'with', u'about', u'against', u'between', u'into', u'through', u'during', u'before', u'after', u'above', u'below', u'to', u'from', u'up', u'down', u'in', u'out', u'on', u'off', u'over', u'under', u'again', u'further', u'then', u'once', u'here', u'there', u'when', u'where', u'why', u'how', u'all', u'any', u'both', u'each', u'few', u'more', u'most', u'other', u'some', u'such', u'no', u'nor', u'not', u'only', u'own', u'same', u'so', u'than', u'too', u'very', u's', u't', u'can', u'will', u'just', u'don', u'should', u'now', u'd', u'll', u'm', u'o', u're', u've', u'y', u'ain', u'aren', u'couldn', u'didn', u'doesn', u'hadn', u'hasn', u'haven', u'isn', u'ma', u'mightn', u'mustn', u'needn', u'shan', u'shouldn', u'wasn', u'weren', u'won', u'wouldn']\n"
     ]
    }
   ],
   "source": [
    "print stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Stopwords\n",
    "\n",
    "#### note that 'no' and 'not' are removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 262)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Kaggle Tutorial Method\n",
    "K_cleaned_tokens = [w for w in words if not w in stopwords.words(\"english\")]\n",
    "# len(words)\n",
    "\n",
    "# Insight Jupyter notebook method\n",
    "I_cleaned_tokens = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "for token in article_tokens:\n",
    "    if token not in stop_words:\n",
    "        I_cleaned_tokens.append(token)\n",
    "\n",
    "# Check if methods are the same\n",
    "len(I_cleaned_tokens) == len(K_cleaned_tokens), len(I_cleaned_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization\n",
    "\n",
    "stemming selectively removed the end of words such as to remove tense\n",
    "lemmatization accounts for variables such as part of speech, meaning, & context\n",
    "\n",
    "Lemmatization usually creates more tokens, but extra computational power later may not prove worth it\n",
    "\n",
    "Try with both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "lancaster = LancasterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stemmed_tokens = []\n",
    "lemmatized_tokens = []\n",
    "\n",
    "for token in I_cleaned_tokens:\n",
    "    stemmed_tokens.append(snowball.stem(token))\n",
    "    lemmatized_tokens.append(lemmatizer.lemmatize(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(262, 262)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lemmatized_tokens), len(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorization - bag of words (uses token count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train the bag of words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  5,  5,  1,  1,  2,  1,  1,  1,  1,  1,  2,  1,  3,  1,  1,  1,\n",
       "         1,  8,  1,  2,  1,  6,  1,  1,  2, 16,  2,  1,  1,  6,  4,  2,  3,\n",
       "         1,  1,  1,  1,  1,  4,  1,  1,  1,  1,  1,  1,  2,  2,  1,  1,  5,\n",
       "         1,  1,  1,  3,  1,  1,  1,  2,  2,  1,  1,  1,  1,  3,  6,  5,  2,\n",
       "         4,  1,  1,  9,  1,  2,  1,  8,  3,  1,  1,  2,  1,  1,  1,  4,  1,\n",
       "         3,  7,  1,  1,  1, 12,  1,  1,  6,  1,  5,  1,  1,  3,  3,  2,  3,\n",
       "         1,  1,  2,  1,  1,  2,  1]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can do feature(token) reduction later for too rare and too common words\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# stem article\n",
    "stemmed_article = ' '.join(wd for wd in stemmed_tokens)\n",
    "# perform count-based vectorization\n",
    "article_vect = vectorizer.fit_transform([stemmed_article])\n",
    "\n",
    "# Kaggle method \n",
    "train_data_features = article_vect.toarray()\n",
    "train_data_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ad\n",
      "5 analyst\n",
      "5 announc\n",
      "1 approach\n",
      "1 april\n",
      "2 averag\n",
      "1 back\n",
      "1 base\n",
      "1 beat\n",
      "1 beaten\n",
      "1 call\n",
      "2 came\n",
      "1 cap\n",
      "3 chang\n",
      "1 climb\n",
      "1 close\n",
      "1 compani\n",
      "1 compar\n",
      "8 consensus\n",
      "1 cover\n",
      "2 data\n",
      "1 date\n",
      "6 day\n",
      "1 declar\n",
      "1 declin\n",
      "2 drop\n",
      "16 earn\n",
      "2 end\n",
      "1 ep\n",
      "1 equiti\n",
      "6 estim\n",
      "4 expect\n",
      "2 facebook\n",
      "3 fb\n",
      "1 februari\n",
      "1 fell\n",
      "1 financi\n",
      "1 firm\n",
      "1 fiscal\n",
      "4 follow\n",
      "1 four\n",
      "1 fourth\n",
      "1 gain\n",
      "1 given\n",
      "1 go\n",
      "1 higher\n",
      "2 histori\n",
      "2 inc\n",
      "1 januari\n",
      "1 juli\n",
      "5 last\n",
      "1 level\n",
      "1 low\n",
      "1 made\n",
      "3 market\n",
      "1 met\n",
      "1 month\n",
      "1 move\n",
      "2 nasdaq\n",
      "2 near\n",
      "1 next\n",
      "1 novemb\n",
      "1 occas\n",
      "1 offici\n",
      "3 past\n",
      "6 per\n",
      "5 percent\n",
      "2 period\n",
      "4 posit\n",
      "1 post\n",
      "1 present\n",
      "9 price\n",
      "1 probabl\n",
      "2 project\n",
      "1 public\n",
      "8 quarter\n",
      "3 rang\n",
      "1 reaction\n",
      "1 rebound\n",
      "2 recent\n",
      "1 record\n",
      "1 releas\n",
      "1 relev\n",
      "4 report\n",
      "1 respond\n",
      "3 result\n",
      "7 revenu\n",
      "1 right\n",
      "1 send\n",
      "1 session\n",
      "12 share\n",
      "1 short\n",
      "1 sinc\n",
      "6 stock\n",
      "1 street\n",
      "5 surpris\n",
      "1 target\n",
      "1 tell\n",
      "3 th\n",
      "3 time\n",
      "2 top\n",
      "3 trade\n",
      "1 trader\n",
      "1 us\n",
      "2 versus\n",
      "1 volum\n",
      "1 wall\n",
      "2 week\n",
      "1 year\n"
     ]
    }
   ],
   "source": [
    "# Look at the vocabulary\n",
    "vocab = vectorizer.get_feature_names()\n",
    "\n",
    "# Look at the counts for each word\n",
    "dist = np.sum(train_data_features, axis = 0)\n",
    "for tag, count in zip(vocab, dist):\n",
    "    print count, tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'earn', 16), (u'share', 12), (u'price', 9), (u'consensus', 8), (u'quarter', 8), (u'revenu', 7), (u'stock', 6), (u'day', 6), (u'per', 6), (u'estim', 6)]\n"
     ]
    }
   ],
   "source": [
    "# just look at 10 most frequent words for this article\n",
    "freqs = [(word, article_vect.getcol(idx).sum()) for word, idx in vectorizer.vocabulary_.items()]\n",
    "print sorted (freqs, key = lambda x: -x[1])[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency - Inverse Document Frequency (tf-idf) Vectorization\n",
    "\n",
    "if a word occurs frequently in 1 document, it is important, but if it occurs accross many documents, it is less informative and differentiating\n",
    "\n",
    "Insight tutorial uses NYTimes corpus: https://open.blogs.nytimes.com/2008/10/14/announcing-the-new-york-times-campaign-finance-api/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together\n",
    "1) put all functions together \n",
    "2) changed list to set for stopwords because search is faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first preprocess a bunch of articles\n",
    "\n",
    "def preprocess_article_content(text_df):\n",
    "    \"\"\"\n",
    "    Simple preprocessing pipeline which uses RegExp, sets basic token requirements, and removes stop words.\n",
    "    Set up to work with df files created from JSONs\n",
    "    \"\"\"\n",
    "    print 'preprocessing article text...'\n",
    "\n",
    "    # tokenizer, stops, and stemmer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    stop_words = set(stopwords.words('english'))  # can add more stop words to this set\n",
    "    stemmer = SnowballStemmer('english')\n",
    "\n",
    "    # process articles\n",
    "    article_list = []\n",
    "    for row, article in enumerate(text_df['full_text']):\n",
    "        cleaned_tokens = []\n",
    "\n",
    "        letters_only = re.sub(\"[^a-zA-Z]\", \" \", article)\n",
    "        lower_case = letters_only.lower()\n",
    "        tokens = re_tokenizer.tokenize(lower_case)\n",
    "\n",
    "    #         tokens = tokenizer.tokenize(article.decode('utf-8').lower())\n",
    "        for token in tokens:\n",
    "            if token not in stop_words:\n",
    "                if len(token) > 0 and len(token) < 20: # removes non words\n",
    "                    if not token[0].isdigit() and not token[-1].isdigit(): # removes numbers\n",
    "                        stemmed_tokens = stemmer.stem(token)\n",
    "                        cleaned_tokens.append(stemmed_tokens)\n",
    "        # add process article\n",
    "        article_list.append(' '.join(wd for wd in cleaned_tokens))\n",
    "\n",
    "    # echo results and return\n",
    "    print 'preprocessed content for %d articles' % len(article_list)\n",
    "    return article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>body</th>\n",
       "      <td>Late 2016, I wrote how to catch the Facebook I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <td>Facebook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <td>2 Bullish Trades to Juice Your Facebook Inc (F...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          0\n",
       "body      Late 2016, I wrote how to catch the Facebook I...\n",
       "category                                           Facebook\n",
       "title     2 Bullish Trades to Juice Your Facebook Inc (F..."
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a df of a new file\n",
    "filename2 = '2_Bullish_Trades_to_Juice_Your_Facebook_Inc_FB_Stock_Position.json'\n",
    "comparison_df = read_Google_articles(filename2)\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Late 2016, I wrote how to catch the Facebook Inc (NASDAQ: ) knife. The trade was pure profits out of thin air. Since then, Facebook stock caught a 13% rally, but it still isn\\u2019t expensive. Let\\u2019s trade it again.\\n\\nFundamentally, FB execution has been flawless. It had doubters, but management proved them all wrong. After all, it would take a major gaffe to ruin the potential of a billion users.\\n\\nTechnically, while this is not an obvious short-term entry point, it should be a good point for a long-term trade. I am not looking to profit from this trade in the next few days. Instead, I am looking out to next year.\\n\\nThe Trade: Sell the FB Jan 2018 $100 put. This is a bullish trade for which I collect $4.50 per contract to open. To be successful, I need Facebook stock to stay above my sold strike while I hold the position open. Selling naked puts is risky and I only do it if I am willing and able to own FB stock at the strike sold. My breakeven price would be the strike price less the premium I collect. In this case, breakeven is $96 per share. If FB stock falls below my strike I could get assigned the stock. This means that I would own FB at a 21% discount from current levels. I also keep the premium I collect to open the trade. If I really want to juice this bullish trade, I can add a debit call setup for an even more bullish bias.\\n\\nThe Juice (optional): Buy the FB June $130/$135 debit call spread. This is a bullish trade for which I pay $2 per contract to open. I stand to double my money if FB rallies past my spread. Since we are in earnings season, I could delay entry in the debit call spread or keep it a small bet. Short-term reactions to earnings add a temporary gambling element. We never know how will traders react to the earnings regardless of how good they are. Usually those reactions, if unwarranted, are short-lived and the stock should resume its pre-earnings trajectory.\\n\\nThe Hedge (optional): Usually I like to sell credit call spreads to hedge my bet. In this case I am opting out. Instead of hedging, I can be less bullish by taking only one of the two trades suggested. Selling credit call spreads in this \\u201canimal spirit\\u201d market seems counterintuitive. The bulls are in complete control for now.\\n\\nI am not required to hold options trades open until expiration. I can close them at any time for partial gains or losses.\\n\\nNicolas Chahine is the managing director of SellSpreads.com. As of this writing, he did not hold a position in any of the aforementioned securities. You can follow him on Twitter at @racernic\\xa0and stocktwits at\\xa0@racernic.'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_df.iloc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Facebook, Inc. (NASDAQ:FB) is projected to dec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Late 2016, I wrote how to catch the Facebook I...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           full_text\n",
       "0  Facebook, Inc. (NASDAQ:FB) is projected to dec...\n",
       "1  Late 2016, I wrote how to catch the Facebook I..."
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create new df of articles\n",
    "combined_articles_df = pd.DataFrame([mydf.iloc[0][0], comparison_df.iloc[0][0]])\n",
    "combined_articles_df.columns = ['full_text']\n",
    "combined_articles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing article text...\n",
      "preprocessed content for 2 articles\n",
      "(2, 238)\n"
     ]
    }
   ],
   "source": [
    "# process articles\n",
    "processed_article_list = preprocess_article_content(combined_articles_df)\n",
    "\n",
    "# vectorize the articles and compute count matrix\n",
    "\n",
    "tf_vectorizer = TfidfVectorizer()\n",
    "tfidf_article_matrix = tf_vectorizer.fit_transform(processed_article_list)\n",
    "\n",
    "print tfidf_article_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_article_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Ngrams\n",
    "Data is currently treated as unigrams, they may be updated to bigrams later, any larger increases computational costs drastically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-1c872ca05047>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mforest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# fit the forest to the training set, using the bag of words features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mforest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sentiment\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "FB_jan25_26 = [131.479996, 132.779999]\n",
    "\n",
    "# Try random forest on bag of words\n",
    "forest = RandomForestClassifier(n_estimators=100)\n",
    "# fit the forest to the training set, using the bag of words features\n",
    "forest = forest.fit(train_data_features, FB_jan25_26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  5,  5,  1,  1,  2,  1,  1,  1,  1,  1,  2,  1,  3,  1,  1,  1,\n",
       "         1,  8,  1,  2,  1,  6,  1,  1,  2, 16,  2,  1,  1,  6,  4,  2,  3,\n",
       "         1,  1,  1,  1,  1,  4,  1,  1,  1,  1,  1,  1,  2,  2,  1,  1,  5,\n",
       "         1,  1,  1,  3,  1,  1,  1,  2,  2,  1,  1,  1,  1,  3,  6,  5,  2,\n",
       "         4,  1,  1,  9,  1,  2,  1,  8,  3,  1,  1,  2,  1,  1,  1,  4,  1,\n",
       "         3,  7,  1,  1,  1, 12,  1,  1,  6,  1,  5,  1,  1,  3,  3,  2,  3,\n",
       "         1,  1,  2,  1,  1,  2,  1]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
