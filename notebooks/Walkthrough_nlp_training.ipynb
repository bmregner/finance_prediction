{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Predicting financial indicators is definitely a holy grail for our society at its present stage. There is a vast literature on how to do this and the general approach is a time-series one, that is, predict the future of one quantity based on that quantity's past.\n",
    "\n",
    "We are trying to see if it's possible to complement this approach with data coming from news sources, reasoning that news from the world should directly and indirectly weigh on the performance of such indicators as stocks, employment rate, or inflation.\n",
    "\n",
    "Please keep in mind that we do not expect to make any significant improvement over state-of-the-art financial analyses (which involve much more complex and refined models). Rather, we are interested in building a scalable and dynamic pipeline that in the future might supplement those already-existing models or give interesting insights.\n",
    "\n",
    "### This notebook\n",
    "\n",
    "This is a walkthrough illustrating the typical usage of our package. We will try to predict future S&P500 closing values based on past S&P500 values along with NLP features extracted from the daily-updted GDELT 1.0 (http://www.gdeltproject.org/) event database.\n",
    "\n",
    "In particular, to scope down the analysis to a minimally viable scalable pipeline, I extract features from the source urls contained in the database (one associated to each event).\n",
    "\n",
    "For each day, all urls get parsed, tokenized, and stemmed, and then conflated together into a single bag of words. This will constitute one document. After that I may apply a tf-idf or word2vec vectorization (this latter being much favored).\n",
    "\n",
    "I use the extracted features (plus the same day's closing S&P500) to try and fit various regression models to predict the next day's S&P500 and compare them to a benchmark model (a simple naive model predicting the same for tomorrow as today, plus the average increase or decrease over the last few days).\n",
    "\n",
    "I also try to predict if tomorrow's index value will rise or fall, given today's news.\n",
    "\n",
    "For both tasks random forest regressors/classifiers seem promising approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import sys\n",
    "import os\n",
    "sourcedir=os.getcwd()+\"/../source\"\n",
    "if sourcedir not in sys.path:\n",
    "    sys.path.append(sourcedir)\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'model_training' from '/Users/Maxos/Desktop/Insight_stuff/bigsnippyrepo/maqro/notebooks/../source/model_training.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing our nlp proprocessing module, the reload command is for development\n",
    "import nlp_preprocessing as nlpp\n",
    "importlib.reload(nlpp)\n",
    "#importing our model training module, the reload command is for development\n",
    "import model_training as mdlt\n",
    "importlib.reload(mdlt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The nlp-preprocessing module\n",
    "\n",
    "The module has two classes for now: one deals with the nlp preprocessing of Google News articles, which are talked about in much more depth in another notebook; the other is the analog for GDELT url data.\n",
    "\n",
    "Let's explore these classes and their contents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The CorpusGoogleNews class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#del datagnews\n",
    "datagnews=nlpp.CorpusGoogleNews() # nlpp.CorpusGoogleNews('some/data/directory') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the attributes of the initialized class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datagnews.raw_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datagnews.datadirectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one public method for now: it loads files from the data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Inc\n",
      "Apple Inc 1-26-17\n",
      "Apple Inc 1-27-17\n",
      "Apple Inc 1-30-17\n",
      "Apple Inc 1-31-17\n",
      "Apple Inc 2-1-17\n"
     ]
    }
   ],
   "source": [
    "datagnews.data_directory_crawl('AAPL',verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which populates datagnews.raw_articles with dataframes like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The first day of public trading with President...</td>\n",
       "      <td>Apple Inc</td>\n",
       "      <td>3 Stocks to Watch on Tuesday: Apple Inc. (AAPL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The first day of public trading with President...</td>\n",
       "      <td>Apple Inc</td>\n",
       "      <td>3 Stocks to Watch on Tuesday: Apple Inc. (AAPL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The smart home market continues to heat up, an...</td>\n",
       "      <td>Apple Inc</td>\n",
       "      <td>Alphabet Inc (GOOGL) Steals AI Expert Back Fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Reportedly, Apple Inc.’s AAPL management is sc...</td>\n",
       "      <td>Apple Inc</td>\n",
       "      <td>Apple (AAPL) Set to Meet Government Officials ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Apple Inc. (AAPL) executives were in India tod...</td>\n",
       "      <td>Apple Inc</td>\n",
       "      <td>Apple Close to Signing Deal With Indian Govern...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body   category  \\\n",
       "0  The first day of public trading with President...  Apple Inc   \n",
       "1  The first day of public trading with President...  Apple Inc   \n",
       "2  The smart home market continues to heat up, an...  Apple Inc   \n",
       "3  Reportedly, Apple Inc.’s AAPL management is sc...  Apple Inc   \n",
       "4  Apple Inc. (AAPL) executives were in India tod...  Apple Inc   \n",
       "\n",
       "                                               title  \n",
       "0  3 Stocks to Watch on Tuesday: Apple Inc. (AAPL...  \n",
       "1  3 Stocks to Watch on Tuesday: Apple Inc. (AAPL...  \n",
       "2  Alphabet Inc (GOOGL) Steals AI Expert Back Fro...  \n",
       "3  Apple (AAPL) Set to Meet Government Officials ...  \n",
       "4  Apple Close to Signing Deal With Indian Govern...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datagnews.raw_articles['Apple Inc 1-30-17'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The CorpusGDELT class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#del datagdelt\n",
    "datagdelt=nlpp.CorpusGDELT(min_ment=800) # min_ment defaults to 1 and cuts off events that have a low number of mentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the several attributes that the class contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum number of mentions: 800\n",
      "Current directory: ../data/GDELT_1.0/\n",
      "Dates loaded so far: []\n",
      "Corpus of raw urls []\n",
      "Corpus of tfidf-vectorized docs:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#minimum number of mentions for one event to be used\n",
    "print('Minimum number of mentions:',datagdelt.minimum_ment)\n",
    "print('Current directory:',datagdelt.currentdir) # current directory\n",
    "print('Dates loaded so far:',datagdelt.dates) # dates for which data has been loaded so far\n",
    "print('Corpus of raw urls',datagdelt.url_corpus)\n",
    "print('Corpus of tfidf-vectorized docs:')\n",
    "print(datagdelt.vect_corpus_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vowels: ['a', 'e', 'i', 'o', 'u', 'y']\n",
      "Consonants: ['b', 'c', 'd', 'f', 'g', 'h', 'j', 'k', 'l', 'm', 'n', 'p', 'q', 'r', 's', 't', 'v', 'w', 'x', 'z'] \n",
      "Stemmer: <PorterStemmer>\n",
      "Punctuation: re.compile('[-.?!,\":;()|0-9]')\n",
      "Tokenizer: RegexpTokenizer(pattern='\\\\w+', gaps=False, discard_empty=True, flags=56)\n",
      "Filter for spurious url beginnings: re.compile('idind.|idus.|iduk.')\n",
      "Filter for stop words: {'', 'had', 'what', 'these', 'why', 'y', 'over', 'shouldn', 'is', 'himself', 'him', 'won', 'll', 'itself', 'should', 'now', 'herself', 'am', 'too', 'while', 'ourselves', 'here', 'that', 'there', 'some', 'mustn', 'being', 'yourself', 'been', 'by', 'themselves', 'how', 'weren', 'nor', 'each', 'aren', 'between', 'isn', 'yourselves', 'me', 'i', 'once', 'doesn', 'a', 'with', 'your', 'does', 'up', 'or', 'has', 're', 'whom', 'couldn', 'you', 'yours', 'mightn', 'and', 'myself', 'd', 'from', 'more', 'through', 'again', 'if', 'then', 'its', 'other', 'ma', 'for', 'but', 'so', 'hadn', 'do', 'such', 'only', 'when', 'did', 'own', 'o', 'above', 'all', 'be', 'to', 'she', 'of', 'our', 'my', 'further', 's', 'no', 'during', 'wasn', 'as', 'where', 'hers', 'those', 'any', 'same', 'was', 'will', 'have', 'in', 'against', 'after', 'doing', 'very', 'having', 'her', 'both', 'theirs', 'their', 'out', 'shan', 'ain', 'who', 'them', 'didn', 'don', 'wouldn', 've', 'not', 'before', 'they', 'm', 'few', 'can', 'because', 'are', 'until', 'about', 'were', 'into', 'he', 'which', 'off', 'haven', 'this', 'hasn', 'ours', 'just', 'needn', 'it', 'an', 'most', 'we', 'on', 'his', 'under', 'below', 'down', 'than', 't', 'at', 'the'}\n"
     ]
    }
   ],
   "source": [
    "#vowels and consonants\n",
    "print('Vowels:',datagdelt.vowels)\n",
    "print('Consonants:',datagdelt.consonants,end=' ')\n",
    "print()\n",
    "print('Stemmer:',datagdelt.porter) #stemmer of choice\n",
    "print('Punctuation:',datagdelt.punctuation) #punctuation regular expression\n",
    "print('Tokenizer:',datagdelt.re_tokenizer) \n",
    "print('Filter for spurious url beginnings:',datagdelt.spurious_beginnings)\n",
    "print('Filter for stop words:',datagdelt.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GLOBALEVENTID', 'SQLDATE', 'MonthYear', 'Year', 'FractionDate', 'Actor1Code', 'Actor1Name', 'Actor1CountryCode', 'Actor1KnownGroupCode', 'Actor1EthnicCode', 'Actor1Religion1Code', 'Actor1Religion2Code', 'Actor1Type1Code', 'Actor1Type2Code', 'Actor1Type3Code', 'Actor2Code', 'Actor2Name', 'Actor2CountryCode', 'Actor2KnownGroupCode', 'Actor2EthnicCode', 'Actor2Religion1Code', 'Actor2Religion2Code', 'Actor2Type1Code', 'Actor2Type2Code', 'Actor2Type3Code', 'IsRootEvent', 'EventCode', 'EventBaseCode', 'EventRootCode', 'QuadClass', 'GoldsteinScale', 'NumMentions', 'NumSources', 'NumArticles', 'AvgTone', 'Actor1Geo_Type', 'Actor1Geo_FullName', 'Actor1Geo_CountryCode', 'Actor1Geo_ADM1Code', 'Actor1Geo_Lat', 'Actor1Geo_Long', 'Actor1Geo_FeatureID', 'Actor2Geo_Type', 'Actor2Geo_FullName', 'Actor2Geo_CountryCode', 'Actor2Geo_ADM1Code', 'Actor2Geo_Lat', 'Actor2Geo_Long', 'Actor2Geo_FeatureID', 'ActionGeo_Type', 'ActionGeo_FullName', 'ActionGeo_CountryCode', 'ActionGeo_ADM1Code', 'ActionGeo_Lat', 'ActionGeo_Long', 'ActionGeo_FeatureID', 'DATEADDED', 'SOURCEURL'] "
     ]
    }
   ],
   "source": [
    "print(datagdelt.header,end=' ') #GDELT csv files header, notice the last field has the urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what methods are available and what the pipeline is like.\n",
    "\n",
    "First we load the urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Done!"
     ]
    }
   ],
   "source": [
    "datagdelt.load_urls('20161001','20170217') #the earliest available date is April 1st 2013 = 20130401"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at what the url_corpus attribute looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 140 elements in it, because we loaded 140 days!\n",
      "The loaded day n. 5 had 308 events in it that were mentioned more than 800 times:\n",
      " [[1972, 'http://www.philippinetimes.com/index.php/sid/248243461'], [1115, 'http://www.capradio.org/news/npr/story?storyid=496552413'], [970, 'http://thecabin.net/news/2016-10-04/dazzle-daze-raffle-tickets-sale'], [1951, 'http://www.whio.com/news/national-govt--politics/clinton-reaches-out-women-while-trump-defends-taxes/xnmN5QugmLzeGkEBR64y9I/'], [1965, 'http://wgno.com/2016/10/04/this-robot-is-so-realistic-that-it-helps-train-first-responders/'], [1012, 'https://www.stgeorgeutah.com/news/archive/2016/10/04/bureau-of-indian-affairs-discussion-kicks-off-free-brown-bag-lecture-series-full-schedule/'], [925, 'https://www.stgeorgeutah.com/news/archive/2016/10/04/yesco-co-owner-featured-on-undercover-boss-speaks-at-chamber-luncheon/'], [1544, 'http://www.nbcnews.com/politics/2016-election/lid-live-blogging-vice-presidential-debate-n659606?cid=public-rss_20161004'], [1400, 'http://www.floridatoday.com/story/money/2016/10/04/murray-gives-state-port-canaveral-address/91529110/'], [1161, 'http://nbc4i.com/2016/10/04/gop-declares-mike-pence-clear-winner-of-vp-debate-before-it-started/']] \n",
      " etc...\n",
      "The first event was mentioned 1972 times, the second 1115 times, etc...\n"
     ]
    }
   ],
   "source": [
    "day=5 #select one day\n",
    "print('There are',len(datagdelt.url_corpus),'elements in it, because we loaded',len(datagdelt.dates),'days!')\n",
    "print('The loaded day n.',day,'had',len(datagdelt.url_corpus[day-1]) ,'events in it that were mentioned more than',datagdelt.minimum_ment,'times:\\n', datagdelt.url_corpus[day-1][:10],'\\n etc...')\n",
    "print('The first event was mentioned',datagdelt.url_corpus[day-1][0][0],'times, the second',datagdelt.url_corpus[day-1][1][0],'times, etc...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that many of those urls contain wordings that can be very informative on what's happening in the world and therefore might tell us something about the near future of the markets!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's process these messy raw urls! Let's use word2vec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using word2vec vectorization procedure\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-19 11:44:04,442 : INFO : collecting all words and their counts\n",
      "2017-02-19 11:44:04,443 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-02-19 11:44:04,490 : INFO : collected 15000 word types from a corpus of 236610 raw words and 140 sentences\n",
      "2017-02-19 11:44:04,490 : INFO : Loading a fresh vocabulary\n",
      "2017-02-19 11:44:04,549 : INFO : min_count=1 retains 15000 unique words (100% of original 15000, drops 0)\n",
      "2017-02-19 11:44:04,550 : INFO : min_count=1 leaves 236610 word corpus (100% of original 236610, drops 0)\n",
      "2017-02-19 11:44:04,624 : INFO : deleting the raw counts dictionary of 15000 items\n",
      "2017-02-19 11:44:04,625 : INFO : sample=0.001 downsamples 26 most-common words\n",
      "2017-02-19 11:44:04,626 : INFO : downsampling leaves estimated 223514 word corpus (94.5% of prior 236610)\n",
      "2017-02-19 11:44:04,626 : INFO : estimated required memory for 15000 words and 48 dimensions: 13260000 bytes\n",
      "2017-02-19 11:44:04,683 : INFO : resetting layer weights\n",
      "2017-02-19 11:44:04,959 : INFO : training model with 3 workers on 15000 vocabulary and 48 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-02-19 11:44:04,959 : INFO : expecting 140 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-02-19 11:44:05,690 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-02-19 11:44:05,693 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-02-19 11:44:05,703 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-02-19 11:44:05,704 : INFO : training on 1183050 raw words (1117428 effective words) took 0.7s, 1504632 effective words/s\n"
     ]
    }
   ],
   "source": [
    "datagdelt.gdelt_preprocess(vectrz='word2vec',size_w2v=48)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which gives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w2v_1</th>\n",
       "      <th>w2v_10</th>\n",
       "      <th>w2v_11</th>\n",
       "      <th>w2v_12</th>\n",
       "      <th>w2v_13</th>\n",
       "      <th>w2v_14</th>\n",
       "      <th>w2v_15</th>\n",
       "      <th>w2v_16</th>\n",
       "      <th>w2v_17</th>\n",
       "      <th>w2v_18</th>\n",
       "      <th>...</th>\n",
       "      <th>w2v_44</th>\n",
       "      <th>w2v_45</th>\n",
       "      <th>w2v_46</th>\n",
       "      <th>w2v_47</th>\n",
       "      <th>w2v_48</th>\n",
       "      <th>w2v_5</th>\n",
       "      <th>w2v_6</th>\n",
       "      <th>w2v_7</th>\n",
       "      <th>w2v_8</th>\n",
       "      <th>w2v_9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news_date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20161001</th>\n",
       "      <td>-0.106196</td>\n",
       "      <td>0.107095</td>\n",
       "      <td>0.107668</td>\n",
       "      <td>-0.033903</td>\n",
       "      <td>-0.024161</td>\n",
       "      <td>0.115558</td>\n",
       "      <td>-0.060396</td>\n",
       "      <td>0.135370</td>\n",
       "      <td>-0.011432</td>\n",
       "      <td>0.078900</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261782</td>\n",
       "      <td>0.156517</td>\n",
       "      <td>-0.246079</td>\n",
       "      <td>0.161685</td>\n",
       "      <td>-0.111064</td>\n",
       "      <td>0.155389</td>\n",
       "      <td>-0.168112</td>\n",
       "      <td>0.115281</td>\n",
       "      <td>0.019051</td>\n",
       "      <td>-0.047699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20161002</th>\n",
       "      <td>-0.113771</td>\n",
       "      <td>0.100264</td>\n",
       "      <td>0.105713</td>\n",
       "      <td>-0.024083</td>\n",
       "      <td>-0.024964</td>\n",
       "      <td>0.131052</td>\n",
       "      <td>-0.062632</td>\n",
       "      <td>0.127284</td>\n",
       "      <td>-0.023864</td>\n",
       "      <td>0.073769</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.260302</td>\n",
       "      <td>0.149023</td>\n",
       "      <td>-0.250064</td>\n",
       "      <td>0.147783</td>\n",
       "      <td>-0.113185</td>\n",
       "      <td>0.157270</td>\n",
       "      <td>-0.172353</td>\n",
       "      <td>0.103838</td>\n",
       "      <td>0.018536</td>\n",
       "      <td>-0.048057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20161003</th>\n",
       "      <td>-0.110540</td>\n",
       "      <td>0.100292</td>\n",
       "      <td>0.114567</td>\n",
       "      <td>-0.018727</td>\n",
       "      <td>-0.024265</td>\n",
       "      <td>0.122546</td>\n",
       "      <td>-0.057136</td>\n",
       "      <td>0.129817</td>\n",
       "      <td>-0.025815</td>\n",
       "      <td>0.086951</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.255653</td>\n",
       "      <td>0.155750</td>\n",
       "      <td>-0.249969</td>\n",
       "      <td>0.144981</td>\n",
       "      <td>-0.115265</td>\n",
       "      <td>0.157248</td>\n",
       "      <td>-0.168569</td>\n",
       "      <td>0.105200</td>\n",
       "      <td>0.029849</td>\n",
       "      <td>-0.042455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20161004</th>\n",
       "      <td>-0.115050</td>\n",
       "      <td>0.094899</td>\n",
       "      <td>0.108515</td>\n",
       "      <td>-0.012831</td>\n",
       "      <td>-0.017209</td>\n",
       "      <td>0.125177</td>\n",
       "      <td>-0.057109</td>\n",
       "      <td>0.124576</td>\n",
       "      <td>-0.023780</td>\n",
       "      <td>0.073699</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.259164</td>\n",
       "      <td>0.146644</td>\n",
       "      <td>-0.257893</td>\n",
       "      <td>0.140503</td>\n",
       "      <td>-0.113885</td>\n",
       "      <td>0.158432</td>\n",
       "      <td>-0.172649</td>\n",
       "      <td>0.108129</td>\n",
       "      <td>0.024723</td>\n",
       "      <td>-0.047769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20161005</th>\n",
       "      <td>-0.108108</td>\n",
       "      <td>0.099156</td>\n",
       "      <td>0.115526</td>\n",
       "      <td>-0.013976</td>\n",
       "      <td>-0.012861</td>\n",
       "      <td>0.104003</td>\n",
       "      <td>-0.046854</td>\n",
       "      <td>0.130387</td>\n",
       "      <td>-0.009976</td>\n",
       "      <td>0.076836</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.267575</td>\n",
       "      <td>0.144755</td>\n",
       "      <td>-0.258512</td>\n",
       "      <td>0.158576</td>\n",
       "      <td>-0.107521</td>\n",
       "      <td>0.154613</td>\n",
       "      <td>-0.162068</td>\n",
       "      <td>0.125842</td>\n",
       "      <td>0.022172</td>\n",
       "      <td>-0.051734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20161006</th>\n",
       "      <td>-0.112574</td>\n",
       "      <td>0.104572</td>\n",
       "      <td>0.097706</td>\n",
       "      <td>-0.036413</td>\n",
       "      <td>-0.039222</td>\n",
       "      <td>0.141862</td>\n",
       "      <td>-0.073521</td>\n",
       "      <td>0.124061</td>\n",
       "      <td>-0.025292</td>\n",
       "      <td>0.072915</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.249048</td>\n",
       "      <td>0.158949</td>\n",
       "      <td>-0.245712</td>\n",
       "      <td>0.142401</td>\n",
       "      <td>-0.117084</td>\n",
       "      <td>0.156357</td>\n",
       "      <td>-0.183324</td>\n",
       "      <td>0.093742</td>\n",
       "      <td>0.020404</td>\n",
       "      <td>-0.044839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20161007</th>\n",
       "      <td>-0.108720</td>\n",
       "      <td>0.099964</td>\n",
       "      <td>0.110017</td>\n",
       "      <td>-0.021059</td>\n",
       "      <td>-0.025717</td>\n",
       "      <td>0.119847</td>\n",
       "      <td>-0.057550</td>\n",
       "      <td>0.123723</td>\n",
       "      <td>-0.017548</td>\n",
       "      <td>0.076016</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.259163</td>\n",
       "      <td>0.149232</td>\n",
       "      <td>-0.255364</td>\n",
       "      <td>0.146495</td>\n",
       "      <td>-0.111227</td>\n",
       "      <td>0.156205</td>\n",
       "      <td>-0.172690</td>\n",
       "      <td>0.110451</td>\n",
       "      <td>0.024106</td>\n",
       "      <td>-0.048494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20161008</th>\n",
       "      <td>-0.132082</td>\n",
       "      <td>0.089863</td>\n",
       "      <td>0.082826</td>\n",
       "      <td>-0.019553</td>\n",
       "      <td>-0.003057</td>\n",
       "      <td>0.159752</td>\n",
       "      <td>-0.072429</td>\n",
       "      <td>0.121460</td>\n",
       "      <td>-0.032843</td>\n",
       "      <td>0.040385</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.251844</td>\n",
       "      <td>0.141894</td>\n",
       "      <td>-0.255263</td>\n",
       "      <td>0.121864</td>\n",
       "      <td>-0.115007</td>\n",
       "      <td>0.155791</td>\n",
       "      <td>-0.190899</td>\n",
       "      <td>0.097681</td>\n",
       "      <td>0.011721</td>\n",
       "      <td>-0.046504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20161009</th>\n",
       "      <td>-0.096938</td>\n",
       "      <td>0.110626</td>\n",
       "      <td>0.128477</td>\n",
       "      <td>-0.029676</td>\n",
       "      <td>-0.035534</td>\n",
       "      <td>0.100564</td>\n",
       "      <td>-0.051367</td>\n",
       "      <td>0.138368</td>\n",
       "      <td>-0.018160</td>\n",
       "      <td>0.103708</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.256257</td>\n",
       "      <td>0.159699</td>\n",
       "      <td>-0.238598</td>\n",
       "      <td>0.160712</td>\n",
       "      <td>-0.112247</td>\n",
       "      <td>0.154521</td>\n",
       "      <td>-0.158433</td>\n",
       "      <td>0.111544</td>\n",
       "      <td>0.035158</td>\n",
       "      <td>-0.038087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20161010</th>\n",
       "      <td>-0.137194</td>\n",
       "      <td>0.081542</td>\n",
       "      <td>0.075596</td>\n",
       "      <td>-0.013007</td>\n",
       "      <td>0.013085</td>\n",
       "      <td>0.170506</td>\n",
       "      <td>-0.074553</td>\n",
       "      <td>0.115688</td>\n",
       "      <td>-0.036748</td>\n",
       "      <td>0.022193</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.252187</td>\n",
       "      <td>0.130410</td>\n",
       "      <td>-0.256965</td>\n",
       "      <td>0.108889</td>\n",
       "      <td>-0.111273</td>\n",
       "      <td>0.153195</td>\n",
       "      <td>-0.194263</td>\n",
       "      <td>0.097542</td>\n",
       "      <td>0.003480</td>\n",
       "      <td>-0.049160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              w2v_1    w2v_10    w2v_11    w2v_12    w2v_13    w2v_14  \\\n",
       "news_date                                                               \n",
       "20161001  -0.106196  0.107095  0.107668 -0.033903 -0.024161  0.115558   \n",
       "20161002  -0.113771  0.100264  0.105713 -0.024083 -0.024964  0.131052   \n",
       "20161003  -0.110540  0.100292  0.114567 -0.018727 -0.024265  0.122546   \n",
       "20161004  -0.115050  0.094899  0.108515 -0.012831 -0.017209  0.125177   \n",
       "20161005  -0.108108  0.099156  0.115526 -0.013976 -0.012861  0.104003   \n",
       "20161006  -0.112574  0.104572  0.097706 -0.036413 -0.039222  0.141862   \n",
       "20161007  -0.108720  0.099964  0.110017 -0.021059 -0.025717  0.119847   \n",
       "20161008  -0.132082  0.089863  0.082826 -0.019553 -0.003057  0.159752   \n",
       "20161009  -0.096938  0.110626  0.128477 -0.029676 -0.035534  0.100564   \n",
       "20161010  -0.137194  0.081542  0.075596 -0.013007  0.013085  0.170506   \n",
       "\n",
       "             w2v_15    w2v_16    w2v_17    w2v_18    ...       w2v_44  \\\n",
       "news_date                                            ...                \n",
       "20161001  -0.060396  0.135370 -0.011432  0.078900    ...    -0.261782   \n",
       "20161002  -0.062632  0.127284 -0.023864  0.073769    ...    -0.260302   \n",
       "20161003  -0.057136  0.129817 -0.025815  0.086951    ...    -0.255653   \n",
       "20161004  -0.057109  0.124576 -0.023780  0.073699    ...    -0.259164   \n",
       "20161005  -0.046854  0.130387 -0.009976  0.076836    ...    -0.267575   \n",
       "20161006  -0.073521  0.124061 -0.025292  0.072915    ...    -0.249048   \n",
       "20161007  -0.057550  0.123723 -0.017548  0.076016    ...    -0.259163   \n",
       "20161008  -0.072429  0.121460 -0.032843  0.040385    ...    -0.251844   \n",
       "20161009  -0.051367  0.138368 -0.018160  0.103708    ...    -0.256257   \n",
       "20161010  -0.074553  0.115688 -0.036748  0.022193    ...    -0.252187   \n",
       "\n",
       "             w2v_45    w2v_46    w2v_47    w2v_48     w2v_5     w2v_6  \\\n",
       "news_date                                                               \n",
       "20161001   0.156517 -0.246079  0.161685 -0.111064  0.155389 -0.168112   \n",
       "20161002   0.149023 -0.250064  0.147783 -0.113185  0.157270 -0.172353   \n",
       "20161003   0.155750 -0.249969  0.144981 -0.115265  0.157248 -0.168569   \n",
       "20161004   0.146644 -0.257893  0.140503 -0.113885  0.158432 -0.172649   \n",
       "20161005   0.144755 -0.258512  0.158576 -0.107521  0.154613 -0.162068   \n",
       "20161006   0.158949 -0.245712  0.142401 -0.117084  0.156357 -0.183324   \n",
       "20161007   0.149232 -0.255364  0.146495 -0.111227  0.156205 -0.172690   \n",
       "20161008   0.141894 -0.255263  0.121864 -0.115007  0.155791 -0.190899   \n",
       "20161009   0.159699 -0.238598  0.160712 -0.112247  0.154521 -0.158433   \n",
       "20161010   0.130410 -0.256965  0.108889 -0.111273  0.153195 -0.194263   \n",
       "\n",
       "              w2v_7     w2v_8     w2v_9  \n",
       "news_date                                \n",
       "20161001   0.115281  0.019051 -0.047699  \n",
       "20161002   0.103838  0.018536 -0.048057  \n",
       "20161003   0.105200  0.029849 -0.042455  \n",
       "20161004   0.108129  0.024723 -0.047769  \n",
       "20161005   0.125842  0.022172 -0.051734  \n",
       "20161006   0.093742  0.020404 -0.044839  \n",
       "20161007   0.110451  0.024106 -0.048494  \n",
       "20161008   0.097681  0.011721 -0.046504  \n",
       "20161009   0.111544  0.035158 -0.038087  \n",
       "20161010   0.097542  0.003480 -0.049160  \n",
       "\n",
       "[10 rows x 48 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datagdelt.word2vec_corpus.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOOM! Now we have all of our datapoints with their nlp features neatly arranged in a pandas dataframe, ready for processing. Mission accomplished!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to run this expensive preprocessing again on the same exact data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using word2vec vectorization procedure\n",
      "Nothing to be done, dataframes are up to date\n"
     ]
    }
   ],
   "source": [
    "datagdelt.gdelt_preprocess(vectrz='word2vec',size_w2v=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay for savings!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we initialize the model training class, feeding it the dataframe from the nlp processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## The model training module\n",
    "This section covers model training, validation, and testing, from our model_training module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize a class instance by loading into it two lists: one of names of your choosing and one of dataframes, which in this case is the output form the previous module above, datagdelt.vect_corpus_tfidf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import model_training as mdlt\n",
    "importlib.reload(mdlt)\n",
    "tet=mdlt.StockPrediction([['word2vec'],[datagdelt.word2vec_corpus],[datagdelt.w2vec_model]],update=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try an L1 linear regressor which is trying to predict the increase/decrease of tomorrow's S&P index over today's. We test on the last 20 days out of 50 and validate/tune, for every testing case, over the previous 10 days. As for the hyperparameters, we are letting our regularization parameter be searched for in the 0.001-3000 range and we allow for 40 iterations of the optimal parameter search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py3k/lib/python3.5/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameter choices: (72.01745061728414,)\n",
      "model_test_rmse: 12.977 benchmark_test_rmse: 15.351\n",
      "best parameter choices: (129.63061111111085,)\n",
      "model_test_rmse: 0.160 benchmark_test_rmse: 3.403\n",
      "best parameter choices: (110.4262242798356,)\n",
      "model_test_rmse: 2.406 benchmark_test_rmse: 0.653\n",
      "best parameter choices: (129.63061111111085,)\n",
      "model_test_rmse: 3.403 benchmark_test_rmse: 0.025\n",
      "best parameter choices: (129.63061111111085,)\n",
      "model_test_rmse: 18.666 benchmark_test_rmse: 15.245\n",
      "best parameter choices: (95.87970837261244,)\n",
      "model_test_rmse: 3.402 benchmark_test_rmse: 6.362\n",
      "best parameter choices: (110.4262242798356,)\n",
      "model_test_rmse: 2.292 benchmark_test_rmse: 0.909\n",
      "best parameter choices: (95.87970837261244,)\n",
      "model_test_rmse: 3.009 benchmark_test_rmse: 0.172\n",
      "best parameter choices: (33.60867695473269,)\n",
      "model_test_rmse: 13.207 benchmark_test_rmse: 11.780\n",
      "best parameter choices: (33.60867695473269,)\n",
      "model_test_rmse: 8.015 benchmark_test_rmse: 6.679\n",
      "best parameter choices: (33.60867695473269,)\n",
      "model_test_rmse: 11.746 benchmark_test_rmse: 10.525\n",
      "best parameter choices: (0.001,)\n",
      "model_test_rmse: 2.569 benchmark_test_rmse: 7.591\n",
      "best parameter choices: (0.001,)\n",
      "model_test_rmse: 9.791 benchmark_test_rmse: 9.849\n",
      "best parameter choices: (0.001,)\n",
      "model_test_rmse: 5.255 benchmark_test_rmse: 3.955\n",
      "best parameter choices: (0.001,)\n",
      "model_test_rmse: 1.556 benchmark_test_rmse: 2.056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(6.5635096275580205, 5.310320686182215),\n",
       " (6.3036606676414824, 5.1252792112507803)]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tet.auto_ts_val_test_reg('word2vec','lasso',[['alpha',[0.001,7000.,60.]]],parm_search_iter=40,n_folds_val=15,\n",
    "                         past_depth=60,n_folds_test=15,scaling=False,differential=False,notest=False,verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance is not too bad. The coefficients are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  -0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,   -0.        ,\n",
       "         -0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,   -0.        , -382.3833822 ,   -0.        ,\n",
       "        486.55262121,   -0.        ,  431.09244711,  -55.86947521,\n",
       "          0.        ,    0.        ,   -0.        ,   -0.        ,\n",
       "        129.62534241,   -0.        ,    0.        , -354.77565051,\n",
       "         -0.        ,    0.        ,   -0.        ,  395.14775662,\n",
       "        144.82587492,  500.85960297,    0.        ,   -0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "       -755.93900631,    0.        , -154.62738738,   -0.        ,\n",
       "         -0.        ,  -83.94125043,    0.        ,   -0.        ,\n",
       "          1.70947899,    0.98668189])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_imp=tet.models['word2vec'].coef_\n",
    "feat_imp\n",
    "#these are the feature importances for the lasso classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...which isn't surprising. As we said at the beginning, the most important feature should have been today's closing, and it was, entirely offuscating everything else.\n",
    "\n",
    "Let's see if classifying tomorrow's value going up or down will do us and better...\n",
    "N.B. We need to specify a decision threshold which I recommend leaving at 0.5 for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If one had gotten a reasonable result, they might want to play with feature importances to try and see which stems actually were the most significant. It can be done as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('buri', 0.999561607837677),\n",
       " ('light', 0.9994657635688782),\n",
       " ('ford', 0.9994151592254639),\n",
       " ('factori', 0.9993614554405212),\n",
       " ('let', 0.9993431568145752),\n",
       " ('holiday', 0.9993090629577637),\n",
       " ('draw', 0.9993025064468384),\n",
       " ('stock', 0.9992968440055847),\n",
       " ('seri', 0.9992875456809998),\n",
       " ('might', 0.9991116523742676)]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=tet.w2v_models['word2vec']\n",
    "model.similar_by_word('appl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fazl', 0.36830952763557434),\n",
       " ('oahu', 0.3590974807739258),\n",
       " ('tyrrel', 0.3559659421443939),\n",
       " ('barnstorm', 0.3421967625617981),\n",
       " ('predecessor', 0.3185845613479614),\n",
       " ('fleme', 0.31192827224731445),\n",
       " ('silli', 0.30939996242523193),\n",
       " ('gsiqdp', 0.30907684564590454),\n",
       " ('ruddock', 0.3047294020652771),\n",
       " ('tumult', 0.3030698299407959)]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_vector(feat_imp[:-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm... Not sure what to make of this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about we try a random forest regressor instead? We are letting our tuning select any combination among 5 values for the number of estimators, 5 for the maximum number of features used for splitting, and we allow a maximum depth from 5 to 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_test_rmse: 7.928 benchmark_test_rmse: 7.466\n",
      "model_test_rmse: 13.311 benchmark_test_rmse: 13.611\n",
      "model_test_rmse: 20.861 benchmark_test_rmse: 16.867\n",
      "model_test_rmse: 4.195 benchmark_test_rmse: 3.337\n",
      "model_test_rmse: 0.348 benchmark_test_rmse: 3.595\n",
      "model_test_rmse: 12.230 benchmark_test_rmse: 15.351\n",
      "model_test_rmse: 1.193 benchmark_test_rmse: 3.403\n",
      "model_test_rmse: 4.395 benchmark_test_rmse: 0.653\n",
      "model_test_rmse: 2.959 benchmark_test_rmse: 0.025\n",
      "model_test_rmse: 18.708 benchmark_test_rmse: 15.245\n",
      "model_test_rmse: 8.365 benchmark_test_rmse: 6.362\n",
      "model_test_rmse: 3.341 benchmark_test_rmse: 0.909\n",
      "model_test_rmse: 0.266 benchmark_test_rmse: 0.172\n",
      "model_test_rmse: 11.297 benchmark_test_rmse: 11.780\n",
      "model_test_rmse: 3.998 benchmark_test_rmse: 6.679\n",
      "model_test_rmse: 10.709 benchmark_test_rmse: 10.525\n",
      "model_test_rmse: 8.110 benchmark_test_rmse: 7.591\n",
      "model_test_rmse: 5.400 benchmark_test_rmse: 9.849\n",
      "model_test_rmse: 9.583 benchmark_test_rmse: 3.955\n",
      "model_test_rmse: 0.906 benchmark_test_rmse: 2.056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(7.4050806437339549, 5.71111290802365),\n",
       " (6.9715473175452187, 5.3271630939674264)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tet.auto_ts_val_test_reg('word2vec','rfreg',[['n_estim',{5,6,7}],['max_feat',{26,27,35,37,40,45,48}],\n",
    "                                             ['max_depth',{4,5,6,7,8,9}]],parm_search_iter=1,n_folds_val=15,\n",
    "                         past_depth=6,n_folds_test=20,scaling=True,differential=True,verbose=False,notest=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we want to get a prediction for today, we toggle the 'notest' attribute to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can't predict for tomorrow, because the stock market will be closed\n"
     ]
    }
   ],
   "source": [
    "tet.auto_ts_val_test_reg('word2vec','rfreg',[['n_estim',{5,6,7}],['max_feat',{26,27,35,37,40,45,48}],\n",
    "                                             ['max_depth',{4,5,6,7,8,9}]],parm_search_iter=1,n_folds_val=15,\n",
    "                         past_depth=6,n_folds_test=20,scaling=True,differential=True,verbose=False,notest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.86560304e-02,   6.47340680e-02,   5.04366059e-03,\n",
       "         4.82489746e-02,   2.60912845e-02,   6.21046423e-03,\n",
       "         1.83707279e-03,   1.73020912e-02,   5.95717567e-02,\n",
       "         2.61237858e-02,   7.94298900e-04,   5.88512680e-02,\n",
       "         2.66449214e-02,   3.90272604e-02,   5.99184367e-02,\n",
       "         3.76074582e-05,   5.71304649e-03,   2.77368604e-02,\n",
       "         4.56659497e-03,   2.76483038e-02,   6.08056934e-03,\n",
       "         9.11762961e-03,   4.30781150e-02,   1.25883151e-04,\n",
       "         3.42150078e-03,   1.04673323e-03,   7.51815745e-03,\n",
       "         1.65745414e-02,   1.89179524e-02,   8.84814693e-03,\n",
       "         4.26913387e-03,   1.41589770e-02,   3.70210519e-03,\n",
       "         7.26372796e-03,   5.41131341e-03,   2.14004773e-02,\n",
       "         1.06000923e-02,   4.32772542e-03,   4.30099701e-03,\n",
       "         2.96269348e-02,   2.25382335e-02,   2.06082933e-03,\n",
       "         8.70850127e-03,   3.85983915e-02,   0.00000000e+00,\n",
       "         6.05505838e-04,   1.66542643e-02,   9.59456167e-03,\n",
       "         3.41739488e-04,   1.56349472e-01])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_imp=tet.models['word2vec'].feature_importances_\n",
    "feat_imp\n",
    "#these are the feature importances for a random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can't predict for tomorrow, because the stock market will be closed\n"
     ]
    }
   ],
   "source": [
    "tet.auto_ts_val_test_reg('word2vec','rfreg',[['n_estim',{1,2,3,5,7}],['max_feat',{21,22,23,24}],['max_depth',{5,6,7}]],\n",
    "                         parm_search_iter=1,n_folds_val=6,n_folds_test=20,scaling=True,differential=True,verbose=False,\n",
    "                         notest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_test_rmse: 3.019775 flat_test_rmse: 7.40724752174\n",
      "model_test_rmse: 8.410034 flat_test_rmse: 1.71873020833\n",
      "model_test_rmse: 6.6198125 flat_test_rmse: 0.580156\n",
      "model_test_rmse: 17.930257 flat_test_rmse: 11.0525086923\n",
      "model_test_rmse: 11.505005 flat_test_rmse: 5.67294251852\n",
      "model_test_rmse: 8.880004 flat_test_rmse: 9.39025842857\n",
      "model_test_rmse: 3.10017866667 flat_test_rmse: 6.24663241379\n",
      "model_test_rmse: 0.929932 flat_test_rmse: 8.37825533333\n",
      "model_test_rmse: 12.530029 flat_test_rmse: 5.59196196774\n",
      "model_test_rmse: 3.84002725 flat_test_rmse: 0.55275684375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(7.6765054416666727, 4.993188530339169),\n",
       " (5.6591449928088533, 3.4814802333665931)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tet.auto_ts_val_test_reg('word2vec','knnreg',[['numb_nn',{1,2,3,4}]],parm_search_iter=4,n_folds_val=6,n_folds_test=10,\n",
    "                         scaling=True,differential=True,notest=False,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can't predict for tomorrow, because the stock market will be closed\n"
     ]
    }
   ],
   "source": [
    "tet.auto_ts_val_test_reg('word2vec','knnreg',[['numb_nn',{1,2,3,4}]],parm_search_iter=4,n_folds_val=6,n_folds_test=10,\n",
    "                         scaling=True,differential=True,notest=True,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_rec,prec,F1: [1.0, 1.0, 1.0] benchmark_rec,prec,F1: [1.0, 1.0, 1.0]\n",
      "test_rec,prec,F1: [0.0, 1.0, 0.0] benchmark_rec,prec,F1: [0.0, 1.0, 0.0]\n",
      "test_rec,prec,F1: [0.0, 1.0, 0.0] benchmark_rec,prec,F1: [0.0, 1.0, 0.0]\n",
      "test_rec,prec,F1: [0.0, 1.0, 0.0] benchmark_rec,prec,F1: [1.0, 1.0, 1.0]\n",
      "test_rec,prec,F1: [0.0, 1.0, 0.0] benchmark_rec,prec,F1: [1.0, 1.0, 1.0]\n",
      "test_rec,prec,F1: [0.0, 1.0, 0.0] benchmark_rec,prec,F1: [1.0, 1.0, 1.0]\n",
      "test_rec,prec,F1: [0.0, 1.0, 0.0] benchmark_rec,prec,F1: [1.0, 1.0, 1.0]\n",
      "test_rec,prec,F1: [0.0, 1.0, 0.0] benchmark_rec,prec,F1: [1.0, 1.0, 1.0]\n",
      "test_rec,prec,F1: [1.0, 1.0, 1.0] benchmark_rec,prec,F1: [1.0, 0.0, 0.0]\n",
      "test_rec,prec,F1: [0.0, 1.0, 0.0] benchmark_rec,prec,F1: [1.0, 1.0, 1.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(array([[ 0.2,  1. ,  0.2]]), array([[ 0.4,  0. ,  0.4]])),\n",
       " (array([[ 0.8,  0.9,  0.7]]),\n",
       "  array([[ 0.4       ,  0.3       ,  0.45825757]]))]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tet.auto_ts_val_test_class('word2vec','logreg',[['l1orl2?',{'l1',}],\n",
    "                                                ['C',[0.0000000000001,0.001,0.000001]]],\n",
    "                           parm_search_iter=30,n_folds_val=10,n_folds_test=10,past_depth=15,scaling=False,notest=False,\n",
    "                           verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 8, 4]\n",
      "test_rec,prec,F1: [1.0, 1.0, 1.0] benchmark_rec,prec,F1: [1.0, 1.0, 1.0]\n",
      "[1, 8, 4]\n",
      "test_rec,prec,F1: [1.0, 1.0, 1.0] benchmark_rec,prec,F1: [0.0, 1.0, 0.0]\n",
      "[1, 8, 4]\n",
      "test_rec,prec,F1: [0.0, 1.0, 0.0] benchmark_rec,prec,F1: [0.0, 1.0, 0.0]\n",
      "[1, 8, 4]\n",
      "test_rec,prec,F1: [1.0, 1.0, 1.0] benchmark_rec,prec,F1: [1.0, 0.0, 0.0]\n",
      "[1, 8, 4]\n",
      "test_rec,prec,F1: [1.0, 1.0, 1.0] benchmark_rec,prec,F1: [1.0, 1.0, 1.0]\n",
      "[1, 8, 4]\n",
      "test_rec,prec,F1: [1.0, 0.0, 0.0] benchmark_rec,prec,F1: [1.0, 1.0, 1.0]\n",
      "[1, 8, 4]\n",
      "test_rec,prec,F1: [1.0, 1.0, 1.0] benchmark_rec,prec,F1: [1.0, 1.0, 1.0]\n",
      "[1, 8, 4]\n",
      "test_rec,prec,F1: [1.0, 1.0, 1.0] benchmark_rec,prec,F1: [0.0, 1.0, 0.0]\n",
      "[1, 8, 4]\n",
      "test_rec,prec,F1: [1.0, 1.0, 1.0] benchmark_rec,prec,F1: [0.0, 1.0, 0.0]\n",
      "[1, 8, 4]\n",
      "test_rec,prec,F1: [0.0, 1.0, 0.0] benchmark_rec,prec,F1: [0.0, 1.0, 0.0]\n",
      "[1, 8, 4]\n",
      "test_rec,prec,F1: [1.0, 0.0, 0.0] benchmark_rec,prec,F1: [1.0, 1.0, 1.0]\n",
      "[1, 8, 4]\n",
      "test_rec,prec,F1: [1.0, 1.0, 1.0] benchmark_rec,prec,F1: [0.0, 1.0, 0.0]\n",
      "[1, 8, 4]\n",
      "test_rec,prec,F1: [1.0, 1.0, 1.0] benchmark_rec,prec,F1: [0.0, 1.0, 0.0]\n",
      "[1, 8, 4]\n",
      "test_rec,prec,F1: [0.0, 1.0, 0.0] benchmark_rec,prec,F1: [1.0, 1.0, 1.0]\n",
      "[1, 8, 4]\n",
      "test_rec,prec,F1: [1.0, 1.0, 1.0] benchmark_rec,prec,F1: [1.0, 1.0, 1.0]\n",
      "[1, 8, 4]\n",
      "test_rec,prec,F1: [1.0, 1.0, 1.0] benchmark_rec,prec,F1: [1.0, 1.0, 1.0]\n",
      "[1, 8, 4]\n",
      "test_rec,prec,F1: [0.0, 1.0, 0.0] benchmark_rec,prec,F1: [1.0, 1.0, 1.0]\n",
      "[1, 8, 4]\n",
      "test_rec,prec,F1: [0.0, 1.0, 0.0] benchmark_rec,prec,F1: [1.0, 1.0, 1.0]\n",
      "[1, 8, 4]\n",
      "test_rec,prec,F1: [1.0, 1.0, 1.0] benchmark_rec,prec,F1: [1.0, 0.0, 0.0]\n",
      "[1, 8, 4]\n",
      "test_rec,prec,F1: [0.0, 1.0, 0.0] benchmark_rec,prec,F1: [1.0, 1.0, 1.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(array([[ 0.7,  0.9,  0.6]]),\n",
       "  array([[ 0.45825757,  0.3       ,  0.48989795]])),\n",
       " (array([[ 0.65,  0.9 ,  0.55]]),\n",
       "  array([[ 0.4769696 ,  0.3       ,  0.49749372]]))]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tet.auto_ts_val_test_class('word2vec','rfclass',[['n_estim',{1,2,3,4,5,6,7}],['max_feat',{8,10,12,14,18,22,23,24}],\n",
    "                                                 ['max_depth',{4,5,6}]],parm_search_iter=1,n_folds_val=10,past_depth=15,\n",
    "                           n_folds_test=20,scaling=True,notest=False,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_rec,prec,F1: [1.0, 0.0, 0.0] benchmark_rec,prec,F1: [1.0, 0.0, 0.0]\n",
      "test_rec,prec,F1: [1.0, 1.0, 1.0] benchmark_rec,prec,F1: [0.0, 1.0, 0.0]\n",
      "test_rec,prec,F1: [0.0, 1.0, 0.0] benchmark_rec,prec,F1: [1.0, 1.0, 1.0]\n",
      "test_rec,prec,F1: [1.0, 0.0, 0.0] benchmark_rec,prec,F1: [1.0, 0.0, 0.0]\n",
      "test_rec,prec,F1: [1.0, 1.0, 1.0] benchmark_rec,prec,F1: [1.0, 0.0, 0.0]\n",
      "test_rec,prec,F1: [1.0, 0.0, 0.0] benchmark_rec,prec,F1: [1.0, 1.0, 1.0]\n",
      "test_rec,prec,F1: [1.0, 1.0, 1.0] benchmark_rec,prec,F1: [1.0, 1.0, 1.0]\n",
      "test_rec,prec,F1: [0.0, 1.0, 0.0] benchmark_rec,prec,F1: [0.0, 1.0, 0.0]\n",
      "test_rec,prec,F1: [1.0, 1.0, 1.0] benchmark_rec,prec,F1: [0.0, 1.0, 0.0]\n",
      "test_rec,prec,F1: [1.0, 1.0, 1.0] benchmark_rec,prec,F1: [0.0, 1.0, 0.0]\n",
      "test_rec,prec,F1: [1.0, 1.0, 1.0] benchmark_rec,prec,F1: [1.0, 0.0, 0.0]\n",
      "test_rec,prec,F1: [1.0, 1.0, 1.0] benchmark_rec,prec,F1: [0.0, 1.0, 0.0]\n",
      "test_rec,prec,F1: [1.0, 1.0, 1.0] benchmark_rec,prec,F1: [1.0, 1.0, 1.0]\n",
      "test_rec,prec,F1: [1.0, 1.0, 1.0] benchmark_rec,prec,F1: [1.0, 1.0, 1.0]\n",
      "test_rec,prec,F1: [1.0, 1.0, 1.0] benchmark_rec,prec,F1: [1.0, 1.0, 1.0]\n",
      "test_rec,prec,F1: [1.0, 1.0, 1.0] benchmark_rec,prec,F1: [1.0, 1.0, 1.0]\n",
      "test_rec,prec,F1: [1.0, 1.0, 1.0] benchmark_rec,prec,F1: [1.0, 1.0, 1.0]\n",
      "test_rec,prec,F1: [1.0, 1.0, 1.0] benchmark_rec,prec,F1: [1.0, 1.0, 1.0]\n",
      "test_rec,prec,F1: [1.0, 0.0, 0.0] benchmark_rec,prec,F1: [1.0, 0.0, 0.0]\n",
      "test_rec,prec,F1: [1.0, 1.0, 1.0] benchmark_rec,prec,F1: [1.0, 1.0, 1.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(array([[ 0.9,  0.8,  0.7]]),\n",
       "  array([[ 0.3       ,  0.4       ,  0.45825757]])),\n",
       " (array([[ 0.75,  0.75,  0.5 ]]),\n",
       "  array([[ 0.4330127,  0.4330127,  0.5      ]]))]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tet.auto_ts_val_test_class('word2vec','rfclass',[['n_estim',{1,2,3,4,5,6,7}],['max_feat',{8,10,12,14,18,22,23,24}],\n",
    "                                                 ['max_depth',{4,5,6}]],parm_search_iter=1,n_folds_val=10,\n",
    "                           n_folds_test=20,scaling=True,notest=False,verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good! Our model consistently overperform the benchmark as for accuracy (=F1 in this case): 0.7 vs 0.5.\n",
    "\n",
    "Now let's predict what today's closing is going to be!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tet.auto_ts_val_test_class('word2vec','rfclass',[['n_estim',{1,2,3,4,5,6,7}],['max_feat',{8,12}],['max_depth',{4,6}]],\n",
    "                           parm_search_iter=1,n_folds_val=10,n_folds_test=25,scaling=False,one_shot=True,notest=True,\n",
    "                           verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RandomForestClassifier' object has no attribute 'coef_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-bc8831b4fd77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeat_imp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word2vec'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfeat_imp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#remember: only the first n-2 features are nlp, the (n-1)-th is being after a weekend and the n-th is today's closing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RandomForestClassifier' object has no attribute 'coef_'"
     ]
    }
   ],
   "source": [
    "feat_imp=tet.models['word2vec'].coef_\n",
    "feat_imp\n",
    "#remember: only the first n-2 features are nlp, the (n-1)-th is being after a weekend and the n-th is today's closing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will go up, apparently!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tet.auto_ts_val_test_class('word2vec','svmclass',[['C',[0.000001,1.,0.001]],['kernel',{'poly',}]],\n",
    "                           parm_search_iter=15,n_folds_val=10,n_folds_test=15,scaling=True,notest=False,\n",
    "                           verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['some_name_you_choose']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictorgdelt.dataset_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, for the real deal: k-fold training and validation!\n",
    "The following method performs that in a very general manner. It lets you decide what regression model to choose, as well as the values of the hyperparameters (please see the module documentation in model_training.py for details on how to pass the hyperparameters), also you need to supply the number of folds you want your data split into, and a seed, for reproducibility. There is also an option to scale and normalize the features but it doesn't quite perform well in general.\n",
    "\n",
    "The method returns the model average performance over the k training iterations. In short, tuning will consist of choosing the value for the hyperparameters that optimizes avg_validation_rmse (that is minimize the average root mean squared on the validation datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By chance, in this one case we outperform the benchmark model with a lower rmse, but this procedure should be performed a couple of time and an average final performance should be quoted instead.\n",
    "\n",
    "Out of curiosity, let's see what the most important features were."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the method returns again average validation performances which are now measured in terms of recall, precision, and F1 score. In lack of a specific metric we want to optimize, we are going to use the F1 score for tuning.\n",
    "\n",
    "The performance plateaus and is optimal for alpha ~1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bingo! Our model predicts all 1's. Not much gained...\n",
    "\n",
    "Incidentally anyway, that's how you pull the predictions vector for a specific dataset.\n",
    "In the future I'll give the option to save a specific model run instead of overwriting. Good for free exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch from now on, please ignore!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out different regressors on the data, no luck so far :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=0.8, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='poly',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 896,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model=LogisticRegression(penalty='l2', C=1.) #, dual=False, tol=0.0001,, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='liblinear', max_iter=100, multi_class='ovr', verbose=0, warm_start=False, n_jobs=1)\n",
    "#model=RandomForestClassifier(n_estimators=10,max_features=4000,max_depth=5)#,max_features=7000)\n",
    "#model=AdaBoostClassifier(n_estimators=20)\n",
    "#model=MLPClassifier(activation='logistic',hidden_layer_sizes=(100,10,5))\n",
    "#model=KNeighborsClassifier(n_neighbors=15)\n",
    "model=SVC(C=.8,kernel='poly')\n",
    "model.fit(x_tfidf_class_trainval,y_tfidf_class_trainval)\n",
    "#model.fit(x_bow_class_trainval,y_bow_class_trainval)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py3k]",
   "language": "python",
   "name": "Python [py3k]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
