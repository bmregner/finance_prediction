{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Predicting financial indicators is definitely a holy grail for our society at its present stage. There is a vast literature on how to do this and the general approach is a time-series one, that is, predict the future of one quantity based on that quantity's past.\n",
    "\n",
    "We are trying to see if it's possible to complement this approach with data coming from news sources, reasoning that news from the world should directly and indirectly weigh on the performance of stocks, employment rates, or inflation.\n",
    "\n",
    "Please keep in mind that we do not expect to make any significant improvement over state-of-the-art financial analyses (which involve much more complex and refined models). Rather, we are interested in building a scalable and dynamic pipeline that in the future might supplement those already-existing models or give interesting insights.\n",
    "\n",
    "### This notebook\n",
    "\n",
    "This notebook is trying to predict future S&P500 closing values based on past S&P500 values along with NLP features extracted from the daily-updted GDELT 1.0 (http://www.gdeltproject.org/) event database.\n",
    "\n",
    "In particular, to scope down the analysis to a minimally viable scalable pipeline, I extract features from the urls contained in the database (one associated to each event).\n",
    "\n",
    "For each day, all urls get parsed, tokenized, and stemmed and conflated together into a single bag of words, weighted depending on the number of mentions of the event related to each specific url. This will constitute one document. After that I may or may not apply a tdf-idf vectorization or stick with bag of words.\n",
    "\n",
    "I use the extracted features (plus the same day's closing S&P500) to try and fit various regression models to predict the next day's S&P500 and compare them to the flat model, i.e. predicting the same for tomorrow as today. Unfortunately there is no clear benefit so far.\n",
    "\n",
    "I also try to predict if tomorrow's index value will rise or fall, given today's news. This approach seems more promising but as of now a successful example of this is not yet included in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import sys\n",
    "import os\n",
    "sourcedir=os.path.split(os.getcwd())[0]+\"/source\"\n",
    "if sourcedir not in sys.path:\n",
    "    sys.path.append(sourcedir)\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'model_training' from '/Users/Maxos/Desktop/Insight_stuff/bigsnippyrepo/maqro/source/model_training.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing our nlp proprocessing module, the reload command is for development\n",
    "import nlp_preprocessing as nlpp\n",
    "importlib.reload(nlpp)\n",
    "#importing our model training module, the reload command is for development\n",
    "import model_training as mdlt\n",
    "importlib.reload(mdlt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The nlp-preprocessing module\n",
    "\n",
    "The module has two classes for now: one deals with the nlp preprocessing of Google News articles, which are talked about in much more depth in another notebook; the other is the analog for GDELT url data.\n",
    "\n",
    "Let's explore these classes and their contents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The CorpusGoogleNews class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#del datagnews\n",
    "datagnews=nlpp.CorpusGoogleNews() # nlpp.CorpusGoogleNews('some/data/directory') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the attributes of the initialized class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datagnews.raw_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datagnews.datadirectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one public method for now: it loads files from the data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Inc\n",
      "Apple Inc 1-26-17\n",
      "Apple Inc 1-27-17\n",
      "Apple Inc 1-30-17\n",
      "Apple Inc 1-31-17\n",
      "Apple Inc 2-1-17\n"
     ]
    }
   ],
   "source": [
    "datagnews.data_directory_crawl('AAPL',verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which populates datagnews.raw_articles with dataframes like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apple Inc</td>\n",
       "      <td>3 Stocks to Watch on Tuesday: Apple Inc. (AAPL...</td>\n",
       "      <td>The first day of public trading with President...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Apple Inc</td>\n",
       "      <td>3 Stocks to Watch on Tuesday: Apple Inc. (AAPL...</td>\n",
       "      <td>The first day of public trading with President...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apple Inc</td>\n",
       "      <td>Alphabet Inc (GOOGL) Steals AI Expert Back Fro...</td>\n",
       "      <td>The smart home market continues to heat up, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apple Inc</td>\n",
       "      <td>Apple (AAPL) Set to Meet Government Officials ...</td>\n",
       "      <td>Reportedly, Apple Inc.’s AAPL management is sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Apple Inc</td>\n",
       "      <td>Apple Close to Signing Deal With Indian Govern...</td>\n",
       "      <td>Apple Inc. (AAPL) executives were in India tod...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    category                                              title  \\\n",
       "0  Apple Inc  3 Stocks to Watch on Tuesday: Apple Inc. (AAPL...   \n",
       "1  Apple Inc  3 Stocks to Watch on Tuesday: Apple Inc. (AAPL...   \n",
       "2  Apple Inc  Alphabet Inc (GOOGL) Steals AI Expert Back Fro...   \n",
       "3  Apple Inc  Apple (AAPL) Set to Meet Government Officials ...   \n",
       "4  Apple Inc  Apple Close to Signing Deal With Indian Govern...   \n",
       "\n",
       "                                                body  \n",
       "0  The first day of public trading with President...  \n",
       "1  The first day of public trading with President...  \n",
       "2  The smart home market continues to heat up, an...  \n",
       "3  Reportedly, Apple Inc.’s AAPL management is sc...  \n",
       "4  Apple Inc. (AAPL) executives were in India tod...  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datagnews.raw_articles['Apple Inc 1-30-17'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The CorpusGDELT class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#del datagdelt\n",
    "datagdelt=nlpp.CorpusGDELT(min_ment=400) # min_ment defaults to 1 and cuts off events that have a \n",
    "#low number of mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum numbe rof mentions: 400\n",
      "Current directory: ../data/GDELT_1.0/\n",
      "Dates loaded so far: []\n",
      "Corpus of raw urls []\n",
      "Corpus of tfidf-vectorized docs:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#minimum number of mentions for one event to be used\n",
    "print('Minimum numbe rof mentions:',datagdelt.minimum_ment)\n",
    "print('Current directory:',datagdelt.currentdir) # current directory\n",
    "print('Dates loaded so far:',datagdelt.dates) # dates for which data has been loaded so far\n",
    "print('Corpus of raw urls',datagdelt.url_corpus)\n",
    "print('Corpus of tfidf-vectorized docs:')\n",
    "print(datagdelt.vect_corpus_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vowels: ['a', 'e', 'i', 'o', 'u', 'y']\n",
      "Consonants: ['b', 'c', 'd', 'f', 'g', 'h', 'j', 'k', 'l', 'm', 'n', 'p', 'q', 'r', 's', 't', 'v', 'w', 'x', 'z'] \n",
      "Stemmer: <PorterStemmer>\n",
      "Punctuation: re.compile('[-.?!,\":;()|0-9]')\n",
      "Tokenizer: RegexpTokenizer(pattern='\\\\w+', gaps=False, discard_empty=True, flags=56)\n",
      "Filter for spurious url beginnings: re.compile('idind.|idus.|iduk.')\n",
      "Filter for stop words: {'', 'my', 'do', 'the', 'am', 'ourselves', 'were', 'shouldn', 'ain', 'all', 'himself', 'during', 'needn', 'didn', 'itself', 're', 'd', 'about', 'their', 'once', 'while', 'both', 'herself', 'over', 'no', 'aren', 'this', 'own', 'any', 'by', 'ma', 'should', 'where', 'same', 'out', 'ours', 'he', 'why', 'nor', 'its', 'y', 'because', 'as', 'under', 'her', 'between', 'too', 'can', 'or', 'had', 'me', 'we', 'has', 'to', 'hadn', 'myself', 'll', 'through', 'hers', 'in', 'are', 'm', 'which', 'haven', 'those', 'what', 'only', 'few', 'up', 'if', 'most', 'his', 'don', 'have', 'into', 'is', 'when', 'your', 'further', 'won', 'now', 'after', 'she', 'very', 'does', 'an', 'with', 'again', 'them', 'how', 'will', 'been', 'be', 've', 'it', 'they', 'of', 'our', 'having', 'o', 'you', 'yours', 'for', 'down', 'yourselves', 'theirs', 'him', 'these', 'then', 'that', 'mightn', 'a', 's', 'from', 'on', 'who', 'at', 'i', 'such', 'just', 'each', 'shan', 'doing', 'was', 'here', 'some', 'than', 'weren', 'below', 'there', 't', 'isn', 'did', 'not', 'themselves', 'being', 'but', 'off', 'more', 'wouldn', 'against', 'so', 'hasn', 'and', 'doesn', 'other', 'wasn', 'yourself', 'before', 'mustn', 'couldn', 'whom', 'above', 'until'}\n"
     ]
    }
   ],
   "source": [
    "#vowels and consonants\n",
    "print('Vowels:',datagdelt.vowels)\n",
    "print('Consonants:',datagdelt.consonants,end=' ')\n",
    "print()\n",
    "print('Stemmer:',datagdelt.porter) #stemmer of choice\n",
    "print('Punctuation:',datagdelt.punctuation) #punctuation regular expression\n",
    "print('Tokenizer:',datagdelt.re_tokenizer) \n",
    "print('Filter for spurious url beginnings:',datagdelt.spurious_beginnings)\n",
    "print('Filter for stop words:',datagdelt.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GLOBALEVENTID', 'SQLDATE', 'MonthYear', 'Year', 'FractionDate', 'Actor1Code', 'Actor1Name', 'Actor1CountryCode', 'Actor1KnownGroupCode', 'Actor1EthnicCode', 'Actor1Religion1Code', 'Actor1Religion2Code', 'Actor1Type1Code', 'Actor1Type2Code', 'Actor1Type3Code', 'Actor2Code', 'Actor2Name', 'Actor2CountryCode', 'Actor2KnownGroupCode', 'Actor2EthnicCode', 'Actor2Religion1Code', 'Actor2Religion2Code', 'Actor2Type1Code', 'Actor2Type2Code', 'Actor2Type3Code', 'IsRootEvent', 'EventCode', 'EventBaseCode', 'EventRootCode', 'QuadClass', 'GoldsteinScale', 'NumMentions', 'NumSources', 'NumArticles', 'AvgTone', 'Actor1Geo_Type', 'Actor1Geo_FullName', 'Actor1Geo_CountryCode', 'Actor1Geo_ADM1Code', 'Actor1Geo_Lat', 'Actor1Geo_Long', 'Actor1Geo_FeatureID', 'Actor2Geo_Type', 'Actor2Geo_FullName', 'Actor2Geo_CountryCode', 'Actor2Geo_ADM1Code', 'Actor2Geo_Lat', 'Actor2Geo_Long', 'Actor2Geo_FeatureID', 'ActionGeo_Type', 'ActionGeo_FullName', 'ActionGeo_CountryCode', 'ActionGeo_ADM1Code', 'ActionGeo_Lat', 'ActionGeo_Long', 'ActionGeo_FeatureID', 'DATEADDED', 'SOURCEURL'] "
     ]
    }
   ],
   "source": [
    "print(datagdelt.header,end=' ') #GDELT csv files header, notice the last field has the urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what methods are available and what the pipeline is like.\n",
    "\n",
    "First we load the urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading news for 20130901\n",
      "loading news for 20130902\n",
      "loading news for 20130903\n",
      "loading news for 20130904\n",
      "loading news for 20130905\n",
      "loading news for 20130906\n",
      "loading news for 20130907\n",
      "loading news for 20130908\n",
      "loading news for 20130909\n",
      "loading news for 20130910\n",
      "loading news for 20130911\n",
      "loading news for 20130912\n",
      "loading news for 20130913\n",
      "loading news for 20130914\n",
      "loading news for 20130915\n",
      "loading news for 20130916\n",
      "loading news for 20130917\n",
      "loading news for 20130918\n",
      "loading news for 20130919\n",
      "loading news for 20130920\n",
      "loading news for 20130921\n",
      "loading news for 20130922\n",
      "loading news for 20130923\n",
      "loading news for 20130924\n",
      "loading news for 20130925\n",
      "loading news for 20130926\n",
      "loading news for 20130927\n",
      "loading news for 20130928\n",
      "loading news for 20130929\n",
      "loading news for 20130930\n",
      "loading news for 20131001\n",
      "loading news for 20131002\n",
      "loading news for 20131003\n",
      "loading news for 20131004\n",
      "loading news for 20131005\n",
      "loading news for 20131006\n",
      "loading news for 20131007\n",
      "loading news for 20131008\n",
      "loading news for 20131009\n",
      "loading news for 20131010\n",
      "loading news for 20131011\n",
      "loading news for 20131012\n",
      "loading news for 20131013\n",
      "loading news for 20131014\n",
      "loading news for 20131015\n",
      "loading news for 20131016\n",
      "loading news for 20131017\n",
      "loading news for 20131018\n",
      "loading news for 20131019\n",
      "loading news for 20131020\n",
      "loading news for 20131021\n",
      "loading news for 20131022\n",
      "loading news for 20131023\n",
      "loading news for 20131024\n",
      "loading news for 20131025\n",
      "loading news for 20131026\n",
      "loading news for 20131027\n",
      "loading news for 20131028\n",
      "loading news for 20131029\n",
      "loading news for 20131030\n",
      "loading news for 20131031\n"
     ]
    }
   ],
   "source": [
    "datagdelt.load_urls('20130901','20131031') #the earliest available date is April 1st 2013 = 20130401"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at what the url_corpus attribute looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 60 elements in it, because we loaded 60 days!\n",
      "The loaded day n. 1 had 28 events in it that were mentioned more than 400 times: [[441, 'http://economictimes.indiatimes.com/news/news-by-industry/cons-products/food/creambell-to-attain-rs-500-crore-sales-by-end-2014-devyani-food/articleshow/22201890.cms'], [572, 'http://economictimes.indiatimes.com/news/international-business/zain-saudi-appoints-telecoms-veteran-hassan-kabbani-as-ceo/articleshow/22210051.cms'], [556, 'http://www.10news.com/money/consumer/ford-recalls-370000-cars-09012013'], [617, 'http://www.10news.com/money/consumer/ford-recalls-370000-cars-09012013'], [406, 'http://www.firstpost.com/world/radiation-readings-spike-at-water-tank-at-japans-ruined-nuclear-plant-1077395.html'], [496, 'http://romenews-tribune.com/bookmark/23500185'], [496, 'http://romenews-tribune.com/bookmark/23500185'], [410, 'http://www.todayonline.com/world/americas/kerry-us-has-evidence-sarin-gas-was-used-syria'], [474, 'http://www.therecord.com/news-story/4058673-mandela-discharged-from-hospital-condition-still-critical-to-get-intensive-care-at-home/'], [472, 'http://www.nagalandpost.com/ChannelNews/International/InternationalNews.aspx?news=TkVXUzEwMDA0NDQ1Mw%3D%3D-3temH8V%2Broc%3D']] \n",
      " etc...\n",
      "The first event was mentioned 441 times, the second 572 times, etc...\n"
     ]
    }
   ],
   "source": [
    "day=1 #please use 3 or 4 here\n",
    "print('There are',len(datagdelt.url_corpus),'elements in it, because we loaded',len(datagdelt.dates),'days!')\n",
    "print('The loaded day n.',day,'had',len(datagdelt.url_corpus[day-1]) ,'events in it that were mentioned more than',datagdelt.minimum_ment,'times:', datagdelt.url_corpus[day-1][:10],'\\n etc...')\n",
    "print('The first event was mentioned',datagdelt.url_corpus[day-1][0][0],'times, the second',datagdelt.url_corpus[day-1][1][0],'times, etc...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that at least one of those urls contains wordings that can be very informative on what's happening in the world and therefore might tell us something about the near future of the markets!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's process these messy raw urls!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datagdelt.gdelt_preprocess(tfidf=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's see what happened to the vect_corpus_tfidf attribute..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aab</th>\n",
       "      <th>aacbec</th>\n",
       "      <th>aacd</th>\n",
       "      <th>aaff</th>\n",
       "      <th>aaron</th>\n",
       "      <th>ab</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abc</th>\n",
       "      <th>abcf</th>\n",
       "      <th>abcfa</th>\n",
       "      <th>...</th>\n",
       "      <th>your</th>\n",
       "      <th>youth</th>\n",
       "      <th>yulia</th>\n",
       "      <th>yyg</th>\n",
       "      <th>zagoklj</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zeidan</th>\n",
       "      <th>zidan</th>\n",
       "      <th>zient</th>\n",
       "      <th>zoo</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news_date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20130901</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20130902</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.310006</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20130903</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135148</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20130904</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036081</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20130905</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.054006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20130906</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025471</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20130907</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20130908</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20130909</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20130910</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20130911</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059124</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.027632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20130912</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20130913</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20130914</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043423</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.202851</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20130915</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20130916</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20130917</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060392</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.198808</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20130918</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20130919</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20130920</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 2591 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           aab  aacbec  aacd  aaff     aaron        ab   abandon  abc  \\\n",
       "news_date                                                               \n",
       "20130901   0.0     0.0   0.0   0.0  0.000000  0.000000  0.000000  0.0   \n",
       "20130902   0.0     0.0   0.0   0.0  0.000000  0.000000  0.000000  0.0   \n",
       "20130903   0.0     0.0   0.0   0.0  0.000000  0.000000  0.000000  0.0   \n",
       "20130904   0.0     0.0   0.0   0.0  0.000000  0.000000  0.000000  0.0   \n",
       "20130905   0.0     0.0   0.0   0.0  0.000000  0.000000  0.000000  0.0   \n",
       "20130906   0.0     0.0   0.0   0.0  0.000000  0.000000  0.025471  0.0   \n",
       "20130907   0.0     0.0   0.0   0.0  0.000000  0.000000  0.000000  0.0   \n",
       "20130908   0.0     0.0   0.0   0.0  0.000000  0.000000  0.000000  0.0   \n",
       "20130909   0.0     0.0   0.0   0.0  0.000000  0.000000  0.000000  0.0   \n",
       "20130910   0.0     0.0   0.0   0.0  0.000000  0.000000  0.000000  0.0   \n",
       "20130911   0.0     0.0   0.0   0.0  0.000000  0.059124  0.000000  0.0   \n",
       "20130912   0.0     0.0   0.0   0.0  0.000000  0.000000  0.000000  0.0   \n",
       "20130913   0.0     0.0   0.0   0.0  0.000000  0.000000  0.000000  0.0   \n",
       "20130914   0.0     0.0   0.0   0.0  0.000000  0.000000  0.000000  0.0   \n",
       "20130915   0.0     0.0   0.0   0.0  0.000000  0.000000  0.000000  0.0   \n",
       "20130916   0.0     0.0   0.0   0.0  0.000000  0.000000  0.000000  0.0   \n",
       "20130917   0.0     0.0   0.0   0.0  0.060392  0.000000  0.000000  0.0   \n",
       "20130918   0.0     0.0   0.0   0.0  0.000000  0.000000  0.000000  0.0   \n",
       "20130919   0.0     0.0   0.0   0.0  0.000000  0.000000  0.000000  0.0   \n",
       "20130920   0.0     0.0   0.0   0.0  0.000000  0.000000  0.000000  0.0   \n",
       "\n",
       "               abcf     abcfa    ...     your  youth  yulia  yyg   zagoklj  \\\n",
       "news_date                        ...                                         \n",
       "20130901   0.000000  0.000000    ...      0.0    0.0    0.0  0.0  0.000000   \n",
       "20130902   0.310006  0.000000    ...      0.0    0.0    0.0  0.0  0.000000   \n",
       "20130903   0.000000  0.135148    ...      0.0    0.0    0.0  0.0  0.000000   \n",
       "20130904   0.000000  0.036081    ...      0.0    0.0    0.0  0.0  0.000000   \n",
       "20130905   0.000000  0.000000    ...      0.0    0.0    0.0  0.0  0.000000   \n",
       "20130906   0.000000  0.000000    ...      0.0    0.0    0.0  0.0  0.000000   \n",
       "20130907   0.000000  0.000000    ...      0.0    0.0    0.0  0.0  0.000000   \n",
       "20130908   0.000000  0.000000    ...      0.0    0.0    0.0  0.0  0.000000   \n",
       "20130909   0.000000  0.000000    ...      0.0    0.0    0.0  0.0  0.000000   \n",
       "20130910   0.000000  0.000000    ...      0.0    0.0    0.0  0.0  0.000000   \n",
       "20130911   0.027632  0.000000    ...      0.0    0.0    0.0  0.0  0.000000   \n",
       "20130912   0.000000  0.000000    ...      0.0    0.0    0.0  0.0  0.000000   \n",
       "20130913   0.000000  0.000000    ...      0.0    0.0    0.0  0.0  0.000000   \n",
       "20130914   0.000000  0.043423    ...      0.0    0.0    0.0  0.0  0.202851   \n",
       "20130915   0.000000  0.000000    ...      0.0    0.0    0.0  0.0  0.000000   \n",
       "20130916   0.000000  0.000000    ...      0.0    0.0    0.0  0.0  0.000000   \n",
       "20130917   0.198808  0.000000    ...      0.0    0.0    0.0  0.0  0.000000   \n",
       "20130918   0.000000  0.000000    ...      0.0    0.0    0.0  0.0  0.000000   \n",
       "20130919   0.000000  0.000000    ...      0.0    0.0    0.0  0.0  0.000000   \n",
       "20130920   0.000000  0.000000    ...      0.0    0.0    0.0  0.0  0.000000   \n",
       "\n",
       "           zealand  zeidan  zidan  zient       zoo  \n",
       "news_date                                           \n",
       "20130901       0.0     0.0    0.0    0.0  0.000000  \n",
       "20130902       0.0     0.0    0.0    0.0  0.000000  \n",
       "20130903       0.0     0.0    0.0    0.0  0.000000  \n",
       "20130904       0.0     0.0    0.0    0.0  0.000000  \n",
       "20130905       0.0     0.0    0.0    0.0  0.054006  \n",
       "20130906       0.0     0.0    0.0    0.0  0.000000  \n",
       "20130907       0.0     0.0    0.0    0.0  0.000000  \n",
       "20130908       0.0     0.0    0.0    0.0  0.000000  \n",
       "20130909       0.0     0.0    0.0    0.0  0.000000  \n",
       "20130910       0.0     0.0    0.0    0.0  0.000000  \n",
       "20130911       0.0     0.0    0.0    0.0  0.000000  \n",
       "20130912       0.0     0.0    0.0    0.0  0.000000  \n",
       "20130913       0.0     0.0    0.0    0.0  0.000000  \n",
       "20130914       0.0     0.0    0.0    0.0  0.000000  \n",
       "20130915       0.0     0.0    0.0    0.0  0.000000  \n",
       "20130916       0.0     0.0    0.0    0.0  0.000000  \n",
       "20130917       0.0     0.0    0.0    0.0  0.000000  \n",
       "20130918       0.0     0.0    0.0    0.0  0.000000  \n",
       "20130919       0.0     0.0    0.0    0.0  0.000000  \n",
       "20130920       0.0     0.0    0.0    0.0  0.000000  \n",
       "\n",
       "[20 rows x 2591 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datagdelt.vect_corpus_tfidf.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOOM! Now we have all of our datapoints with their nlp features neatly arranged in a pandas dataframe. this is ready for processing. Mission accomplished!\n",
    "\n",
    "Notice that the dataframe is extremely sparse, which essentially means our dataset is going to be not informative at all. But again this is just a proof of principle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to run this expensive preprocessing again on the same exact data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nothing to be done, dataframes are up to date\n"
     ]
    }
   ],
   "source": [
    "datagdelt.gdelt_preprocess(tfidf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Btw, if you're curious, the extracted words (stems, actually, the endings are absent) are: ['aab', 'aacbec', 'aacd', 'aaff', 'aaron', 'ab', 'abandon', 'abc', 'abcf', 'abcfa', 'abduct', 'abe', 'abid', 'abil', 'aboard', 'abort', 'abrio', 'abroad', 'ac', 'aca', 'acabd', 'acapulco', 'acbecefb', 'acceler', 'access', 'accid', 'accolad', 'accord', 'account', 'accredit'] ...\n"
     ]
    }
   ],
   "source": [
    "print(\"Btw, if you're curious, the extracted words (stems, actually, the endings are absent) are:\", list(datagdelt.vect_corpus_tfidf.columns)[:30],'...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## The model training module (work in progress, please be patient)\n",
    "This section covers model training, validation, and testing, from our model_training module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize a class instance by loading into it two lists: one of names of your choosing and one of dataframes, which in this case is the output form the previous module above, datagdelt.vect_corpus_tfidf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#del predictorgdelt\n",
    "predictorgdelt=mdlt.StockPrediction([['some_name_you_choose'],[datagdelt.vect_corpus_tfidf]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aab</th>\n",
       "      <th>aacbec</th>\n",
       "      <th>aacd</th>\n",
       "      <th>aaff</th>\n",
       "      <th>aaron</th>\n",
       "      <th>ab</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abc</th>\n",
       "      <th>abcf</th>\n",
       "      <th>abcfa</th>\n",
       "      <th>...</th>\n",
       "      <th>your</th>\n",
       "      <th>youth</th>\n",
       "      <th>yulia</th>\n",
       "      <th>yyg</th>\n",
       "      <th>zagoklj</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zeidan</th>\n",
       "      <th>zidan</th>\n",
       "      <th>zient</th>\n",
       "      <th>zoo</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news_date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20130901</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20130902</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.310006</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20130903</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135148</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20130904</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036081</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20130905</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.054006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2591 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           aab  aacbec  aacd  aaff  aaron   ab  abandon  abc      abcf  \\\n",
       "news_date                                                                \n",
       "20130901   0.0     0.0   0.0   0.0    0.0  0.0      0.0  0.0  0.000000   \n",
       "20130902   0.0     0.0   0.0   0.0    0.0  0.0      0.0  0.0  0.310006   \n",
       "20130903   0.0     0.0   0.0   0.0    0.0  0.0      0.0  0.0  0.000000   \n",
       "20130904   0.0     0.0   0.0   0.0    0.0  0.0      0.0  0.0  0.000000   \n",
       "20130905   0.0     0.0   0.0   0.0    0.0  0.0      0.0  0.0  0.000000   \n",
       "\n",
       "              abcfa    ...     your  youth  yulia  yyg  zagoklj  zealand  \\\n",
       "news_date              ...                                                 \n",
       "20130901   0.000000    ...      0.0    0.0    0.0  0.0      0.0      0.0   \n",
       "20130902   0.000000    ...      0.0    0.0    0.0  0.0      0.0      0.0   \n",
       "20130903   0.135148    ...      0.0    0.0    0.0  0.0      0.0      0.0   \n",
       "20130904   0.036081    ...      0.0    0.0    0.0  0.0      0.0      0.0   \n",
       "20130905   0.000000    ...      0.0    0.0    0.0  0.0      0.0      0.0   \n",
       "\n",
       "           zeidan  zidan  zient       zoo  \n",
       "news_date                                  \n",
       "20130901      0.0    0.0    0.0  0.000000  \n",
       "20130902      0.0    0.0    0.0  0.000000  \n",
       "20130903      0.0    0.0    0.0  0.000000  \n",
       "20130904      0.0    0.0    0.0  0.000000  \n",
       "20130905      0.0    0.0    0.0  0.054006  \n",
       "\n",
       "[5 rows x 2591 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictorgdelt.dataset_list[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['some_name_you_choose']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictorgdelt.dataset_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we convert these dataframes into numpy arrays well formatted to feed into scikit learn. Notice that you will now use the name you chose before to process that specific dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictorgdelt.prepare_data('some_name_you_choose')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, as you see below, we have now populated dictionaries (the key is the dataset name you chose) containing x datasets..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'some_name_you_choose': array([[  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "           0.00000000e+00,   1.00000000e+00,   1.63296997e+03],\n",
       "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "           0.00000000e+00,   0.00000000e+00,   1.63977002e+03],\n",
       "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "           0.00000000e+00,   0.00000000e+00,   1.65307996e+03],\n",
       "        ..., \n",
       "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "           0.00000000e+00,   0.00000000e+00,   1.76210999e+03],\n",
       "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "           0.00000000e+00,   0.00000000e+00,   1.77194995e+03],\n",
       "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "           0.00000000e+00,   0.00000000e+00,   1.76331006e+03]])}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictorgdelt.xdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and three different prediction options for the y set: tomorrow's S&P, tomorrow up or down?, tomorrow-today, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'some_name_you_choose': array([[  1.63977002e+03,   1.00000000e+00,   6.80004900e+00],\n",
       "        [  1.65307996e+03,   1.00000000e+00,   1.33099360e+01],\n",
       "        [  1.65507996e+03,   1.00000000e+00,   2.00000000e+00],\n",
       "        [  1.65517004e+03,   1.00000000e+00,   9.00880000e-02],\n",
       "        [  1.67170996e+03,   1.00000000e+00,   1.65399170e+01],\n",
       "        [  1.68398999e+03,   1.00000000e+00,   1.22800290e+01],\n",
       "        [  1.68913000e+03,   1.00000000e+00,   5.14001500e+00],\n",
       "        [  1.68342004e+03,   0.00000000e+00,  -5.70996100e+00],\n",
       "        [  1.68798999e+03,   1.00000000e+00,   4.56994600e+00],\n",
       "        [  1.69759998e+03,   1.00000000e+00,   9.60998600e+00],\n",
       "        [  1.70476001e+03,   1.00000000e+00,   7.16003400e+00],\n",
       "        [  1.72552002e+03,   1.00000000e+00,   2.07600100e+01],\n",
       "        [  1.72233997e+03,   0.00000000e+00,  -3.18005400e+00],\n",
       "        [  1.70991003e+03,   0.00000000e+00,  -1.24299320e+01],\n",
       "        [  1.70183997e+03,   0.00000000e+00,  -8.07006800e+00],\n",
       "        [  1.69742004e+03,   0.00000000e+00,  -4.41992200e+00],\n",
       "        [  1.69277002e+03,   0.00000000e+00,  -4.65002400e+00],\n",
       "        [  1.69867004e+03,   1.00000000e+00,   5.90002400e+00],\n",
       "        [  1.69175000e+03,   0.00000000e+00,  -6.92004400e+00],\n",
       "        [  1.68155005e+03,   0.00000000e+00,  -1.01999510e+01],\n",
       "        [  1.69500000e+03,   1.00000000e+00,   1.34499510e+01],\n",
       "        [  1.69387000e+03,   0.00000000e+00,  -1.13000500e+00],\n",
       "        [  1.67866003e+03,   0.00000000e+00,  -1.52099610e+01],\n",
       "        [  1.69050000e+03,   1.00000000e+00,   1.18399660e+01],\n",
       "        [  1.67612000e+03,   0.00000000e+00,  -1.43800050e+01],\n",
       "        [  1.65544995e+03,   0.00000000e+00,  -2.06700440e+01],\n",
       "        [  1.65640002e+03,   1.00000000e+00,   9.50073000e-01],\n",
       "        [  1.69256006e+03,   1.00000000e+00,   3.61600350e+01],\n",
       "        [  1.70319995e+03,   1.00000000e+00,   1.06398920e+01],\n",
       "        [  1.71014001e+03,   1.00000000e+00,   6.94006400e+00],\n",
       "        [  1.69806006e+03,   0.00000000e+00,  -1.20799560e+01],\n",
       "        [  1.72154004e+03,   1.00000000e+00,   2.34799800e+01],\n",
       "        [  1.73315002e+03,   1.00000000e+00,   1.16099850e+01],\n",
       "        [  1.74450000e+03,   1.00000000e+00,   1.13499760e+01],\n",
       "        [  1.74466003e+03,   1.00000000e+00,   1.60034000e-01],\n",
       "        [  1.75467004e+03,   1.00000000e+00,   1.00100100e+01],\n",
       "        [  1.74638000e+03,   0.00000000e+00,  -8.29003900e+00],\n",
       "        [  1.75206995e+03,   1.00000000e+00,   5.68994100e+00],\n",
       "        [  1.75977002e+03,   1.00000000e+00,   7.70007400e+00],\n",
       "        [  1.76210999e+03,   1.00000000e+00,   2.33996500e+00],\n",
       "        [  1.77194995e+03,   1.00000000e+00,   9.83996600e+00],\n",
       "        [  1.76331006e+03,   0.00000000e+00,  -8.63989200e+00],\n",
       "        [  1.75654004e+03,   0.00000000e+00,  -6.77002000e+00]])}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictorgdelt.ydata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, how about we split the dataset into train+validation and test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictorgdelt.trval_test_split('some_name_you_choose',0.2) #0.2 here is the fraction #testset/#total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ha! Now the x/ydata dictionary entry has been split into a 2-tuple of (training+validation,test) datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "           0.00000000e+00,   0.00000000e+00,   1.74466003e+03],\n",
       "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "           0.00000000e+00,   0.00000000e+00,   1.67170996e+03],\n",
       "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "           0.00000000e+00,   0.00000000e+00,   1.67866003e+03],\n",
       "        ..., \n",
       "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "           0.00000000e+00,   0.00000000e+00,   1.68913000e+03],\n",
       "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "           0.00000000e+00,   1.00000000e+00,   1.74450000e+03],\n",
       "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "           5.40056182e-02,   0.00000000e+00,   1.65507996e+03]]),\n",
       " array([[  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "           0.00000000e+00,   0.00000000e+00,   1.72154004e+03],\n",
       "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "           0.00000000e+00,   0.00000000e+00,   1.70183997e+03],\n",
       "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "           0.00000000e+00,   0.00000000e+00,   1.71014001e+03],\n",
       "        ..., \n",
       "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "           0.00000000e+00,   0.00000000e+00,   1.76331006e+03],\n",
       "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "           0.00000000e+00,   1.00000000e+00,   1.63296997e+03],\n",
       "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "           0.00000000e+00,   0.00000000e+00,   1.72552002e+03]]))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictorgdelt.xdata['some_name_you_choose']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[  1.75467004e+03,   1.00000000e+00,   1.00100100e+01],\n",
       "        [  1.68398999e+03,   1.00000000e+00,   1.22800290e+01],\n",
       "        [  1.69050000e+03,   1.00000000e+00,   1.18399660e+01],\n",
       "        [  1.77194995e+03,   1.00000000e+00,   9.83996600e+00],\n",
       "        [  1.67612000e+03,   0.00000000e+00,  -1.43800050e+01],\n",
       "        [  1.68913000e+03,   1.00000000e+00,   5.14001500e+00],\n",
       "        [  1.69867004e+03,   1.00000000e+00,   5.90002400e+00],\n",
       "        [  1.69175000e+03,   0.00000000e+00,  -6.92004400e+00],\n",
       "        [  1.70476001e+03,   1.00000000e+00,   7.16003400e+00],\n",
       "        [  1.76210999e+03,   1.00000000e+00,   2.33996500e+00],\n",
       "        [  1.68155005e+03,   0.00000000e+00,  -1.01999510e+01],\n",
       "        [  1.70319995e+03,   1.00000000e+00,   1.06398920e+01],\n",
       "        [  1.68798999e+03,   1.00000000e+00,   4.56994600e+00],\n",
       "        [  1.67170996e+03,   1.00000000e+00,   1.65399170e+01],\n",
       "        [  1.65507996e+03,   1.00000000e+00,   2.00000000e+00],\n",
       "        [  1.65544995e+03,   0.00000000e+00,  -2.06700440e+01],\n",
       "        [  1.75977002e+03,   1.00000000e+00,   7.70007400e+00],\n",
       "        [  1.72154004e+03,   1.00000000e+00,   2.34799800e+01],\n",
       "        [  1.69387000e+03,   0.00000000e+00,  -1.13000500e+00],\n",
       "        [  1.69277002e+03,   0.00000000e+00,  -4.65002400e+00],\n",
       "        [  1.69500000e+03,   1.00000000e+00,   1.34499510e+01],\n",
       "        [  1.74638000e+03,   0.00000000e+00,  -8.29003900e+00],\n",
       "        [  1.67866003e+03,   0.00000000e+00,  -1.52099610e+01],\n",
       "        [  1.72552002e+03,   1.00000000e+00,   2.07600100e+01],\n",
       "        [  1.69256006e+03,   1.00000000e+00,   3.61600350e+01],\n",
       "        [  1.71014001e+03,   1.00000000e+00,   6.94006400e+00],\n",
       "        [  1.70183997e+03,   0.00000000e+00,  -8.07006800e+00],\n",
       "        [  1.76331006e+03,   0.00000000e+00,  -8.63989200e+00],\n",
       "        [  1.65307996e+03,   1.00000000e+00,   1.33099360e+01],\n",
       "        [  1.65640002e+03,   1.00000000e+00,   9.50073000e-01],\n",
       "        [  1.70991003e+03,   0.00000000e+00,  -1.24299320e+01],\n",
       "        [  1.68342004e+03,   0.00000000e+00,  -5.70996100e+00],\n",
       "        [  1.74466003e+03,   1.00000000e+00,   1.60034000e-01],\n",
       "        [  1.65517004e+03,   1.00000000e+00,   9.00880000e-02]]),\n",
       " array([[  1.73315002e+03,   1.00000000e+00,   1.16099850e+01],\n",
       "        [  1.69742004e+03,   0.00000000e+00,  -4.41992200e+00],\n",
       "        [  1.69806006e+03,   0.00000000e+00,  -1.20799560e+01],\n",
       "        [  1.75206995e+03,   1.00000000e+00,   5.68994100e+00],\n",
       "        [  1.74450000e+03,   1.00000000e+00,   1.13499760e+01],\n",
       "        [  1.69759998e+03,   1.00000000e+00,   9.60998600e+00],\n",
       "        [  1.75654004e+03,   0.00000000e+00,  -6.77002000e+00],\n",
       "        [  1.63977002e+03,   1.00000000e+00,   6.80004900e+00],\n",
       "        [  1.72233997e+03,   0.00000000e+00,  -3.18005400e+00]]))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictorgdelt.ydata['some_name_you_choose']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, for the real deal: k-fold training and validation!\n",
    "The following method performs that in a very general manner. It lets you decide what regression model to choose, as well as the values of the hyperparameters (please see the module documentation in model_training.py for details on how to pass the hyperparameters), also you need to supply the number of folds you want your data split into, and a seed, for reproducibility. There is also an option to scale and normalize the features but it doesn't quite perform well in general.\n",
    "\n",
    "The method returns the model average performance over the k training iterations. In short, tuning will consist of choosing the value for the hyperparameters that optimizes avg_validation_rmse (that is minimize the average root mean squared on the validation datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha = 11.2\n",
      "avg_train_rmse: 11.6470100241 avg_validation_rmse: 11.8295047334\n",
      "alpha = 11.3\n",
      "avg_train_rmse: 11.6470935177 avg_validation_rmse: 11.8295020586\n",
      "alpha = 11.4\n",
      "avg_train_rmse: 11.6471777528 avg_validation_rmse: 11.8295001005\n",
      "alpha = 11.5\n",
      "avg_train_rmse: 11.6472627295 avg_validation_rmse: 11.8294988592\n",
      "alpha = 11.6\n",
      "avg_train_rmse: 11.6473484476 avg_validation_rmse: 11.8294983351\n",
      "alpha = 11.7\n",
      "avg_train_rmse: 11.6474349073 avg_validation_rmse: 11.8294985283\n",
      "alpha = 11.8\n",
      "avg_train_rmse: 11.6475221085 avg_validation_rmse: 11.829499439\n",
      "alpha = 11.9\n",
      "avg_train_rmse: 11.6476100511 avg_validation_rmse: 11.8295010676\n",
      "alpha = 12.0\n",
      "avg_train_rmse: 11.6476987351 avg_validation_rmse: 11.8295034142\n",
      "alpha = 12.1\n",
      "avg_train_rmse: 11.6477881607 avg_validation_rmse: 11.8295064791\n",
      "alpha = 12.2\n",
      "avg_train_rmse: 11.6478783276 avg_validation_rmse: 11.8295102625\n",
      "alpha = 12.3\n",
      "avg_train_rmse: 11.6479692359 avg_validation_rmse: 11.8295147646\n",
      "alpha = 12.4\n",
      "avg_train_rmse: 11.6480608856 avg_validation_rmse: 11.8295199856\n",
      "alpha = 12.5\n",
      "avg_train_rmse: 11.6481532766 avg_validation_rmse: 11.8295259259\n",
      "alpha = 12.6\n",
      "avg_train_rmse: 11.648246409 avg_validation_rmse: 11.8295325856\n",
      "alpha = 12.7\n",
      "avg_train_rmse: 11.6483402828 avg_validation_rmse: 11.8295399649\n"
     ]
    }
   ],
   "source": [
    "#10-fold validated lasso linear regression with sliding hyperparameter alpha, seed=100, no scaling, \n",
    "#for dataset 'some_name_you_choose'.\n",
    "for alpha in [12.+0.1*i for i in range(-8,8)]:\n",
    "    print('alpha =',alpha)\n",
    "    predictorgdelt.kfold_val_reg(10,'some_name_you_choose','lasso',alpha,100,scaling=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that the minimum is reached for alpha = 11.6 (you'll probably get different values). So now we go into testing and use this parameter.\n",
    "\n",
    "The following method, very similar to the previous one, retrains the model on the full train+validation dataset with the desired hyperparameters. If the model defines feature importances, these are returned by the method.\n",
    "\n",
    "Importantly, the method also prints out the performance of a benchmark model (just a trivial flat prediction from today to tomorrow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_test_rmse: 8.20685552719 flat_test_rmse: 8.53742476458\n"
     ]
    }
   ],
   "source": [
    "feature_importances=predictorgdelt.kfold_test_reg('some_name_you_choose','lasso',11.6,scaling=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By chance, in this one case we outperform the benchmark model with a lower rmse, but this procedure should be performed a couple of time and an average final performance should be quoted instead.\n",
    "\n",
    "Out of curiosity, let's see what the most important features were."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['*yesterdayS&P', 0.91139353353349029], ['errand', -0.0], ['everybodi', 0.0], ['everi', -0.0], ['ever', -0.0], ['evacu', 0.0], ['europ', 0.0], ['eurasian', 0.0], ['eu', 0.0], ['etx', 0.0]] ...\n"
     ]
    }
   ],
   "source": [
    "key_cols=list(predictorgdelt.dataset_list[0].columns)+['*weekend?','*yesterdayS&P']\n",
    "print([[key_cols[i],feature_importances[i]] for i in np.argsort(abs(feature_importances))[::-1]][:10],'...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...which, isn't surprising. As we said at the beginning, the most important feature should have been today's closing, and it was, entirely offuscating everything else.\n",
    "\n",
    "Let's see if classifying tomorrow's value going up or down will do us and better...\n",
    "N.B. We need to specify a decision threshold which I recommend leaving at 0.5 for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha = 0.5\n",
      "avg_train_rec,prec,F1: [1.0, 0.6470967741935485, 0.78559676178163562] avg_validation_rec,prec,F1: [1.0, 0.65000000000000002, 0.77476190476190465]\n",
      "alpha = 0.6\n",
      "avg_train_rec,prec,F1: [1.0, 0.6470967741935485, 0.78559676178163562] avg_validation_rec,prec,F1: [1.0, 0.65000000000000002, 0.77476190476190465]\n",
      "alpha = 0.7\n",
      "avg_train_rec,prec,F1: [1.0, 0.6470967741935485, 0.78559676178163562] avg_validation_rec,prec,F1: [1.0, 0.65000000000000002, 0.77476190476190465]\n",
      "alpha = 0.8\n",
      "avg_train_rec,prec,F1: [1.0, 0.6470967741935485, 0.78559676178163562] avg_validation_rec,prec,F1: [1.0, 0.65000000000000002, 0.77476190476190465]\n",
      "alpha = 0.9\n",
      "avg_train_rec,prec,F1: [1.0, 0.6470967741935485, 0.78559676178163562] avg_validation_rec,prec,F1: [1.0, 0.65000000000000002, 0.77476190476190465]\n",
      "alpha = 1.0\n",
      "avg_train_rec,prec,F1: [1.0, 0.6470967741935485, 0.78559676178163562] avg_validation_rec,prec,F1: [1.0, 0.65000000000000002, 0.77476190476190465]\n",
      "alpha = 1.1\n",
      "avg_train_rec,prec,F1: [1.0, 0.6470967741935485, 0.78559676178163562] avg_validation_rec,prec,F1: [1.0, 0.65000000000000002, 0.77476190476190465]\n",
      "alpha = 1.2\n",
      "avg_train_rec,prec,F1: [1.0, 0.6470967741935485, 0.78559676178163562] avg_validation_rec,prec,F1: [1.0, 0.65000000000000002, 0.77476190476190465]\n",
      "alpha = 1.3\n",
      "avg_train_rec,prec,F1: [1.0, 0.6470967741935485, 0.78559676178163562] avg_validation_rec,prec,F1: [1.0, 0.65000000000000002, 0.77476190476190465]\n",
      "alpha = 1.4\n",
      "avg_train_rec,prec,F1: [1.0, 0.6470967741935485, 0.78559676178163562] avg_validation_rec,prec,F1: [1.0, 0.65000000000000002, 0.77476190476190465]\n"
     ]
    }
   ],
   "source": [
    "#10-fold validated lasso logistic regression with sliding hyperparameter alpha, seed=100, no scaling, \n",
    "#for dataset 'some_name_you_choose'. Scaling not yet supported\n",
    "for alpha in [1.+0.1*i for i in range(-5,5)]:\n",
    "    print('alpha =',alpha)\n",
    "    predictorgdelt.kfold_val_class(10,'some_name_you_choose','logreg',['l1',alpha],100,thres=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the method returns again average validation performances which are now measured in terms of recall, precision, and F1 score. In lack of a specific metric we want to optimize, we are going to use the F1 score for tuning.\n",
    "\n",
    "The performance plateaus and is optimal for alpha ~1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_rec,prec,F1: [1.0, 0.55555555555555558, 0.7142857142857143]\n"
     ]
    }
   ],
   "source": [
    "feature_importances_class=predictorgdelt.kfold_test_class('some_name_you_choose','logreg',['l1',1.0],thres=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I haven't yet implemented a benchmark model for classification. This is the test data though:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the training+validation data labels were [ 1.  1.  1.  1.  0.  1.  1.  0.  1.  1.  0.  1.  1.  1.  1.  0.  1.  1.\n",
      "  0.  0.  1.  0.  0.  1.  1.  1.  0.  0.  1.  1.  0.  0.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "print('the training+validation data labels were',predictorgdelt.ydata['some_name_you_choose'][0][:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so I'm gonna compare my model to one that predicts only 1's (majority class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the benchmark model's recall, precision, and F1 scores are (1.0, 0.5555555555555556, 0.7142857142857143)\n"
     ]
    }
   ],
   "source": [
    "print(\"the benchmark model's recall, precision, and F1 scores are\", mdlt.scores(predictorgdelt.ydata['some_name_you_choose'][1][:,1],np.ones(9),[0.5])[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "oh bummer, this model seems identical to mine... which is confirmed by feature importances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['*yesterdayS&P', 0.00035189982423960872], ['errand', 0.0], ['everybodi', 0.0], ['everi', 0.0], ['ever', 0.0], ['evacu', 0.0], ['europ', 0.0], ['eurasian', 0.0], ['eu', 0.0], ['etx', 0.0]] ...\n"
     ]
    }
   ],
   "source": [
    "key_cols=list(predictorgdelt.dataset_list[0].columns)+['*weekend?','*yesterdayS&P']\n",
    "print([[key_cols[i],feature_importances_class[0][i]] for i in np.argsort(abs(feature_importances_class[0]))[::-1]][:10],'...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... this model is exclusively bias... and by direct visual inspection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and the predicted test labels are [ 1.  1.  1.  1.  1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "print('and the predicted test labels are',predictorgdelt.yhat_class['some_name_you_choose'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bingo! Our model predicts all 1's. Not much gained...\n",
    "\n",
    "Incidentally anyway, that's how you pull the predictions vector for a specific dataset.\n",
    "In the future I'll give the option to save a specific model run instead of overwriting. Good for free exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch from now on, please ignore!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_rmse: 9.20135417438 avg_validation_rmse: 23.8508310037\n"
     ]
    }
   ],
   "source": [
    "predictorgdelt.kfold_val_reg(10,'apriltodectfidf','lasso',1.3,10,scaling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_rmse: 5.91587243572 avg_validation_rmse: 24.4121821352\n"
     ]
    }
   ],
   "source": [
    "predictorgdelt.kfold_val_reg(10,'apriltodectfidf','rfreg',[50,4500,10],10,scaling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_rmse: 11.1816716619 avg_validation_rmse: 15.3459786376\n"
     ]
    }
   ],
   "source": [
    "predictorgdelt.kfold_val_reg(10,'apriltodectfidf','adabreg',15,10,scaling=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_rmse: 10.342872738 avg_validation_rmse: 11.4892361364\n"
     ]
    }
   ],
   "source": [
    "predictorgdelt.kfold_val_reg(10,'apriltodectfidf','knnreg',7,10,scaling=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_test_rmse: 16.0651087792 flat_test_rmse: 14.4209762157\n"
     ]
    }
   ],
   "source": [
    "aa=predictorgdelt.kfold_test_reg('apriltodectfidf','knnreg',7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_rec,prec,F1: [0.81837529044943147, 0.70479197132136429, 0.75720674859733283] avg_validation_rec,prec,F1: [0.76335497835497834, 0.64537684537684537, 0.6926744610887835]\n"
     ]
    }
   ],
   "source": [
    "predictorgdelt.kfold_val_class(10,'apriltodectfidf','knnclass',7,10,[0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_rec,prec,F1: [0.5, 0.41666666666666669, 0.45454545454545453]\n"
     ]
    }
   ],
   "source": [
    "predictorgdelt.kfold_test_class('apriltodectfidf','knnclass',7,[0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_rec,prec,F1: [1.0, 0.71820616787952551, 0.83593883914424061] avg_validation_rec,prec,F1: [1.0, 0.59458333333333324, 0.73921100638491943]\n"
     ]
    }
   ],
   "source": [
    "predictorgdelt.kfold_val_class(10,'apriltodectfidf','rfclass',[40,5,10],10,[0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_rec,prec,F1: [1.0, 0.51282051282051277, 0.67796610169491522]\n"
     ]
    }
   ],
   "source": [
    "predictorgdelt.kfold_test_class('apriltodectfidf','rfclass',[40,5,10],[0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51282051282051277"
      ]
     },
     "execution_count": 744,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(predictorgdelt.ydata['apriltodectfidf'][1][:,1])/len(predictorgdelt.ydata['apriltodectfidf'][1][:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.5128205128205128, 0.6779661016949152, 0.5128205128205128)"
      ]
     },
     "execution_count": 778,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdlt.scores(predictorgdelt.ydata['apriltodectfidf'][1][:,1],np.ones(len(predictorgdelt.ydata['apriltodectfidf'][1])),[0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_rec,prec,F1: [1.0, 0.59213718334048937, 0.74374608177131407] avg_validation_rec,prec,F1: [1.0, 0.59458333333333324, 0.73921100638491943]\n"
     ]
    }
   ],
   "source": [
    "predictorgdelt.kfold_val_class(10,'apriltodectfidf','logreg',['l1',1.5],10,[0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_rec,prec,F1: [1.0, 0.51282051282051277, 0.67796610169491522]\n"
     ]
    }
   ],
   "source": [
    "aa=predictorgdelt.kfold_test_class('apriltodectfidf','logreg',['l1',2.5],[0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_rec,prec,F1: [0.76262732475581552, 0.77500920878424662, 0.7669871458718045] avg_validation_rec,prec,F1: [0.66152958152958141, 0.61717171717171726, 0.62368359527432093]\n"
     ]
    }
   ],
   "source": [
    "predictorgdelt.kfold_val_class(10,'apriltodectfidf','svmclass',[1.,'poly'],10,[0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_rec,prec,F1: [0.65000000000000002, 0.59090909090909094, 0.61904761904761907]\n"
     ]
    }
   ],
   "source": [
    "predictorgdelt.kfold_test_class('apriltodectfidf','svmclass',[1.,'poly'],[0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apriltodectfidf': array([ 0.,  1.,  1.,  1.,  1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  1.,\n",
       "         0.,  1.,  1.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  0.,  1.,  0.,\n",
       "         0.,  1.,  1.,  0.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  0.,  1.])}"
      ]
     },
     "execution_count": 775,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictorgdelt.yhat_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,\n",
       "        1.,  0.,  1.,  0.,  0.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,  0.,\n",
       "        1.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  1.,  0.,  1.])"
      ]
     },
     "execution_count": 772,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictorgdelt.ydata['apriltodectfidf'][1][:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-660-12c529f1bcd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictorgdelt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkfold_val_reg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'apriltodectfidf'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'svmreg'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'poly'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscaling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/Maxos/Desktop/Insight_stuff/bigsnippyrepo/maqro/model_training.py\u001b[0m in \u001b[0;36mkfold_val_reg\u001b[0;34m(self, n_folds_val, dataset_id, regressor, parm, seed, differential, scaling)\u001b[0m\n\u001b[1;32m    198\u001b[0m                                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'houston, we have an unknown model problem'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m                         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m                         \u001b[0mavg_rms_mod_val\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                         \u001b[0mavg_rms_mod_train\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py3k/lib/python3.5/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m         \u001b[0;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py3k/lib/python3.5/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    254\u001b[0m                 \u001b[0mcache_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m                 max_iter=self.max_iter, random_seed=random_seed)\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "predictorgdelt.kfold_val_reg(10,'apriltodectfidf','svmreg',[15,'poly'],10,scaling=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_rmse: 12.6222333907 avg_validation_rmse: 12.1568874732\n"
     ]
    }
   ],
   "source": [
    "predictorgdelt.kfold_val_reg(10,'apriltodectfidf','mlpreg',['relu',(100,)],10,scaling=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_test_rmse: 14.8924855598 flat_test_rmse: 14.4209762157\n"
     ]
    }
   ],
   "source": [
    "aa=predictorgdelt.kfold_test_reg('apriltodectfidf','mlpreg',['relu',(100,)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apriltodectfidf': array([ 1744.52278741,  1803.29915885,  1694.4451712 ,  1693.34940803,\n",
       "         1837.50763223,  1661.97637678,  1576.88280783,  1647.17413376,\n",
       "         1653.98888935,  1660.05401055,  1630.59372504,  1760.84365842,\n",
       "         1594.12637618,  1693.15485608,  1657.5242832 ,  1559.35083745,\n",
       "         1693.66655617,  1770.69578534,  1678.18873354,  1618.16557548,\n",
       "         1798.57971612,  1595.75929349,  1790.80377837,  1762.44887651,\n",
       "         1569.11649056,  1588.40738375,  1769.48474365,  1773.99257709,\n",
       "         1789.57343515,  1688.86454837,  1637.86982931,  1793.05291322,\n",
       "         1655.27695931,  1687.8802146 ,  1631.17041088,  1657.8406513 ,\n",
       "         1655.79591841,  1562.46506112,  1711.34272805])}"
      ]
     },
     "execution_count": 561,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictorgdelt.yhat_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1754.670044,  1800.900024,  1703.199951,  1689.469971,\n",
       "        1841.069946,  1650.469971,  1553.689941,  1642.810059,\n",
       "        1667.469971,  1630.47998 ,  1612.52002 ,  1767.930054,\n",
       "        1592.430054,  1685.72998 ,  1655.079956,  1541.609985,\n",
       "        1681.550049,  1767.689941,  1655.449951,  1606.280029,\n",
       "        1795.150024,  1573.089966,  1785.030029,  1756.540039,\n",
       "        1570.25    ,  1593.609985,  1747.150024,  1786.540039,\n",
       "        1787.869995,  1689.130005,  1633.77002 ,  1792.810059,\n",
       "        1628.930054,  1706.869995,  1639.040039,  1652.619995,\n",
       "        1642.800049,  1562.5     ,  1698.060059])"
      ]
     },
     "execution_count": 562,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictorgdelt.ydata['apriltodectfidf'][1][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_rmse: 6.33518759404 avg_validation_rmse: 24.5460444404\n"
     ]
    }
   ],
   "source": [
    "predictorgdelt.kfold_val_reg(10,'apriltodectfidf','ridge',0.001,10,scaling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_test_rmse: 14.2964522937 flat_test_rmse: 13.5413782105\n"
     ]
    }
   ],
   "source": [
    "aa=predictorgdelt.kfold_test_reg('7daystfidf','lasso',37.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_rmse: 11.1986058145 avg_validation_rmse: 10.800359651\n"
     ]
    }
   ],
   "source": [
    "predictorgdelt.kfold_val_reg(15,'7daystfidf','ridge',8400.,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_test_rmse: 14.3814887421 flat_test_rmse: 13.5413782105\n"
     ]
    }
   ],
   "source": [
    "aa=predictorgdelt.kfold_test_reg('7daystfidf','ridge',8400.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_rmse: 11.0071954777 avg_validation_rmse: 12.3585904621\n"
     ]
    }
   ],
   "source": [
    "predictorgdelt.kfold_val_reg(10,'7daystfidf','svreg',[0.01,'poly'],10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_test_rmse: 10.3370215097 flat_test_rmse: 10.8353511909\n"
     ]
    }
   ],
   "source": [
    "predictorgdelt.kfold_test_reg('7daystfidf','svreg',[0.01,'poly'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "key_cols=list(datagdelt.vect_corpus_tfidf.columns)+['*weekend?','*yesterdayS&P']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['north', 2.1730974001773933],\n",
       " ['train', 1.5123459726524862],\n",
       " ['snowden', 1.264379084303753],\n",
       " ['crew', -1.130072887205146],\n",
       " ['big', 1.0974619202530456],\n",
       " ['ae', -0.84521265295771053],\n",
       " ['milit', -0.26812945924067383],\n",
       " ['*yesterdayS&P', 1.8144839374748277e-05],\n",
       " ['juli', 0.0],\n",
       " ['exam', 0.0],\n",
       " ['everi', 0.0],\n",
       " ['everybodi', 0.0],\n",
       " ['evict', 0.0],\n",
       " ['evid', 0.0],\n",
       " ['evolut', 0.0],\n",
       " ['ex', 0.0],\n",
       " ['exactli', 0.0],\n",
       " ['examin', 0.0],\n",
       " ['egyptian', 0.0],\n",
       " ['exce', 0.0],\n",
       " ['except', 0.0],\n",
       " ['exception', 0.0],\n",
       " ['exchang', 0.0],\n",
       " ['exclus', 0.0],\n",
       " ['exec', 0.0],\n",
       " ['execut', 0.0],\n",
       " ['juror', 0.0],\n",
       " ['ever', 0.0],\n",
       " ['eventu', 0.0],\n",
       " ['event', 0.0],\n",
       " ['etern', 0.0],\n",
       " ['ethanol', 0.0],\n",
       " ['ethnic', 0.0],\n",
       " ['etx', 0.0],\n",
       " ['eu', 0.0],\n",
       " ['eurasian', 0.0],\n",
       " ['eurobank', 0.0],\n",
       " ['europ', 0.0],\n",
       " ['european', 0.0],\n",
       " ['eurozon', 0.0],\n",
       " ['evacu', 0.0],\n",
       " ['evad', 0.0],\n",
       " ['evalu', 0.0],\n",
       " ['eve', 0.0],\n",
       " ['even', 0.0],\n",
       " ['exelon', 0.0],\n",
       " ['exercis', 0.0],\n",
       " ['exhaust', 0.0],\n",
       " ['express', 0.0],\n",
       " ['extens', 0.0],\n",
       " ['extra', 0.0],\n",
       " ['extradit', 0.0],\n",
       " ['extrem', 0.0],\n",
       " ['exxon', 0.0],\n",
       " ['eye', 0.0],\n",
       " ['fa', 0.0],\n",
       " ['faa', 0.0],\n",
       " ['faaaa', 0.0],\n",
       " ['faacac', 0.0],\n",
       " ['faacd', 0.0],\n",
       " ['faae', 0.0],\n",
       " ['fabc', 0.0],\n",
       " ['fabulist', 0.0],\n",
       " ['facaedceab', 0.0],\n",
       " ['extend', 0.0],\n",
       " ['export', 0.0],\n",
       " ['exhibit', 0.0],\n",
       " ['explos', 0.0],\n",
       " ['exit', 0.0],\n",
       " ['expand', 0.0],\n",
       " ['expans', 0.0],\n",
       " ['expect', 0.0],\n",
       " ['expeditor', 0.0],\n",
       " ['expel', 0.0],\n",
       " ['expens', 0.0],\n",
       " ['experi', 0.0],\n",
       " ['expert', 0.0],\n",
       " ['expir', 0.0],\n",
       " ['explain', 0.0],\n",
       " ['explan', 0.0],\n",
       " ['explod', 0.0],\n",
       " ['exploit', 0.0],\n",
       " ['explor', 0.0],\n",
       " ['estim', 0.0],\n",
       " ['estat', 0.0],\n",
       " ['establish', 0.0],\n",
       " ['elizabeth', 0.0],\n",
       " ['ellesmer', 0.0],\n",
       " ['elsewher', 0.0],\n",
       " ['elus', 0.0],\n",
       " ['email', 0.0],\n",
       " ['embassi', 0.0],\n",
       " ['embezzl', 0.0],\n",
       " ['embrac', 0.0],\n",
       " ['embroil', 0.0],\n",
       " ['emerg', 0.0],\n",
       " ['emiss', 0.0],\n",
       " ['emit', 0.0],\n",
       " ['emmi', 0.0],\n",
       " ['empir', 0.0],\n",
       " ['employ', 0.0],\n",
       " ['employe', 0.0],\n",
       " ['elkin', 0.0],\n",
       " ['elit', 0.0],\n",
       " ['enclosur', 0.0],\n",
       " ['elig', 0.0],\n",
       " ['eheptgihnqo', 0.0],\n",
       " ['eight', 0.0],\n",
       " ['eject', 0.0],\n",
       " ['el', 0.0],\n",
       " ['elan', 0.0],\n",
       " ['elbaradei', 0.0],\n",
       " ['elderli', 0.0],\n",
       " ['elect', 0.0],\n",
       " ['elector', 0.0],\n",
       " ['electr', 0.0],\n",
       " ['electron', 0.0],\n",
       " ['elementari', 0.0],\n",
       " ['eleph', 0.0],\n",
       " ['elev', 0.0],\n",
       " ['eleven', 0.0],\n",
       " ['en', 0.0],\n",
       " ['encount', 0.0],\n",
       " ['est', 0.0],\n",
       " ['envoy', 0.0],\n",
       " ['eri', 0.0],\n",
       " ['erian', 0.0],\n",
       " ['eric', 0.0],\n",
       " ['errand', 0.0],\n",
       " ['ert', 0.0],\n",
       " ['erupt', 0.0],\n",
       " ['escap', 0.0],\n",
       " ['escape', 0.0],\n",
       " ['escondido', 0.0],\n",
       " ['escort', 0.0],\n",
       " ['eshenbaugh', 0.0],\n",
       " ['eskom', 0.0],\n",
       " ['especi', 0.0],\n",
       " ['espionag', 0.0],\n",
       " ['essam', 0.0],\n",
       " ['epa', 0.0],\n",
       " ['environment', 0.0],\n",
       " ['end', 0.0],\n",
       " ['environ', 0.0],\n",
       " ['endlessli', 0.0],\n",
       " ['endors', 0.0],\n",
       " ['enemi', 0.0],\n",
       " ['energi', 0.0],\n",
       " ['enforc', 0.0],\n",
       " ['engin', 0.0],\n",
       " ['england', 0.0],\n",
       " ['engulf', 0.0],\n",
       " ['enough', 0.0],\n",
       " ['enrich', 0.0],\n",
       " ['enrol', 0.0],\n",
       " ['ensign', 0.0],\n",
       " ['enter', 0.0],\n",
       " ['enthusiast', 0.0],\n",
       " ['entranc', 0.0],\n",
       " ['face', 0.0],\n",
       " ['facebook', 0.0],\n",
       " ['facial', 0.0],\n",
       " ['fightback', 0.0],\n",
       " ['figur', 0.0],\n",
       " ['file', 0.0],\n",
       " ['filibust', 0.0],\n",
       " ['filipina', 0.0],\n",
       " ['filipino', 0.0],\n",
       " ['fill', 0.0],\n",
       " ['fin', 0.0],\n",
       " ['final', 0.0],\n",
       " ['financ', 0.0],\n",
       " ['financi', 0.0],\n",
       " ['find', 0.0],\n",
       " ['fine', 0.0],\n",
       " ['finger', 0.0],\n",
       " ['finish', 0.0],\n",
       " ['finnish', 0.0],\n",
       " ['fighter', 0.0],\n",
       " ['fight', 0.0],\n",
       " ['firefight', 0.0],\n",
       " ['fiercer', 0.0],\n",
       " ['felon', 0.0],\n",
       " ['feloni', 0.0],\n",
       " ['felt', 0.0],\n",
       " ['femal', 0.0],\n",
       " ['fervor', 0.0],\n",
       " ['feud', 0.0],\n",
       " ['ffa', 0.0],\n",
       " ['ffbfe', 0.0],\n",
       " ['ffca', 0.0],\n",
       " ['ffe', 0.0],\n",
       " ['ffecfa', 0.0],\n",
       " ['fi', 0.0],\n",
       " ['fiance', 0.0],\n",
       " ['field', 0.0],\n",
       " ['fierc', 0.0],\n",
       " ['fire', 0.0],\n",
       " ['firework', 0.0],\n",
       " ['feinstein', 0.0],\n",
       " ['flew', 0.0],\n",
       " ['flight', 0.0],\n",
       " ['flock', 0.0],\n",
       " ['flood', 0.0],\n",
       " ['floodwat', 0.0],\n",
       " ['floor', 0.0],\n",
       " ['florida', 0.0],\n",
       " ['flossi', 0.0],\n",
       " ['flower', 0.0],\n",
       " ['flu', 0.0],\n",
       " ['fluke', 0.0],\n",
       " ['fma', 0.0],\n",
       " ['fncynjr', 0.0],\n",
       " ['fndirev', 0.0],\n",
       " ['fnhifq', 0.0],\n",
       " ['fniism', 0.0],\n",
       " ['fli', 0.0],\n",
       " ['flee', 0.0],\n",
       " ['firm', 0.0],\n",
       " ['flaw', 0.0],\n",
       " ['first', 0.0],\n",
       " ['fisa', 0.0],\n",
       " ['fiscal', 0.0],\n",
       " ['fish', 0.0],\n",
       " ['fisher', 0.0],\n",
       " ['fit', 0.0],\n",
       " ['fitchburg', 0.0],\n",
       " ['fitow', 0.0],\n",
       " ['five', 0.0],\n",
       " ['fix', 0.0],\n",
       " ['fla', 0.0],\n",
       " ['flag', 0.0],\n",
       " ['flap', 0.0],\n",
       " ['flare', 0.0],\n",
       " ['flash', 0.0],\n",
       " ['fellow', 0.0],\n",
       " ['fefdecf', 0.0],\n",
       " ['facil', 0.0],\n",
       " ['fall', 0.0],\n",
       " ['fals', 0.0],\n",
       " ['famili', 0.0],\n",
       " ['famou', 0.0],\n",
       " ['fanni', 0.0],\n",
       " ['far', 0.0],\n",
       " ['farc', 0.0],\n",
       " ['farewel', 0.0],\n",
       " ['farlam', 0.0],\n",
       " ['farm', 0.0],\n",
       " ['farmer', 0.0],\n",
       " ['fashion', 0.0],\n",
       " ['fast', 0.0],\n",
       " ['faster', 0.0],\n",
       " ['fat', 0.0],\n",
       " ['fatal', 0.0],\n",
       " ['fallen', 0.0],\n",
       " ['fake', 0.0],\n",
       " ['fateh', 0.0],\n",
       " ['faith', 0.0],\n",
       " ['fact', 0.0],\n",
       " ['factbox', 0.0],\n",
       " ['factori', 0.0],\n",
       " ['fade', 0.0],\n",
       " ['fae', 0.0],\n",
       " ['faebec', 0.0],\n",
       " ['faedc', 0.0],\n",
       " ['fafda', 0.0],\n",
       " ['faffcd', 0.0],\n",
       " ['fager', 0.0],\n",
       " ['fail', 0.0],\n",
       " ['failur', 0.0],\n",
       " ['fair', 0.0],\n",
       " ['fairholm', 0.0],\n",
       " ['fairmont', 0.0],\n",
       " ['fate', 0.0],\n",
       " ['father', 0.0],\n",
       " ['fefa', 0.0],\n",
       " ['fe', 0.0],\n",
       " ['feadeadc', 0.0],\n",
       " ['fear', 0.0],\n",
       " ['feather', 0.0],\n",
       " ['featur', 0.0],\n",
       " ['feb', 0.0],\n",
       " ['fec', 0.0],\n",
       " ['feceb', 0.0],\n",
       " ['fed', 0.0],\n",
       " ['feder', 0.0],\n",
       " ['fedex', 0.0],\n",
       " ['fee', 0.0],\n",
       " ['feec', 0.0],\n",
       " ['feed', 0.0],\n",
       " ['feefcfa', 0.0],\n",
       " ['feel', 0.0],\n",
       " ['fea', 0.0],\n",
       " ['fdeb', 0.0],\n",
       " ['fault', 0.0],\n",
       " ['fda', 0.0],\n",
       " ['favela', 0.0],\n",
       " ['favor', 0.0],\n",
       " ['favorit', 0.0],\n",
       " ['fba', 0.0],\n",
       " ['fbaab', 0.0],\n",
       " ['fbebfa', 0.0],\n",
       " ['fbfadef', 0.0],\n",
       " ['fbffbe', 0.0],\n",
       " ['fbi', 0.0],\n",
       " ['fca', 0.0],\n",
       " ['fcaa', 0.0],\n",
       " ['fcab', 0.0],\n",
       " ['fcdabb', 0.0],\n",
       " ['fcdfa', 0.0],\n",
       " ['fcef', 0.0],\n",
       " ['egyptislamist', 0.0],\n",
       " ['egypt', 0.0],\n",
       " ['fniithn', 0.0],\n",
       " ['director', 0.0],\n",
       " ['dine', 0.0],\n",
       " ['dinner', 0.0],\n",
       " ['dio', 0.0],\n",
       " ['dip', 0.0],\n",
       " ['diplomaci', 0.0],\n",
       " ['diplomat', 0.0],\n",
       " ['direct', 0.0],\n",
       " ['dirti', 0.0],\n",
       " ['egg', 0.0],\n",
       " ['disabl', 0.0],\n",
       " ['disagre', 0.0],\n",
       " ['disappear', 0.0],\n",
       " ['disappoint', 0.0],\n",
       " ['disarma', 0.0],\n",
       " ['disarray', 0.0],\n",
       " ['disast', 0.0],\n",
       " ['dime', 0.0],\n",
       " ['dilemma', 0.0],\n",
       " ['digit', 0.0],\n",
       " ['dig', 0.0],\n",
       " ['dfa', 0.0],\n",
       " ['dfaefaa', 0.0],\n",
       " ['dfe', 0.0],\n",
       " ['dfea', 0.0],\n",
       " ['dfee', 0.0],\n",
       " ['dial', 0.0],\n",
       " ['dialogu', 0.0],\n",
       " ['diamond', 0.0],\n",
       " ['diari', 0.0],\n",
       " ['dickerson', 0.0],\n",
       " ['dictat', 0.0],\n",
       " ['didnt', 0.0],\n",
       " ['die', 0.0],\n",
       " ['diego', 0.0],\n",
       " ['difficult', 0.0],\n",
       " ['discharg', 0.0],\n",
       " ['disclos', 0.0],\n",
       " ['disclosur', 0.0],\n",
       " ['disrupt', 0.0],\n",
       " ['dissent', 0.0],\n",
       " ['dissolut', 0.0],\n",
       " ['dissolv', 0.0],\n",
       " ['distant', 0.0],\n",
       " ['distast', 0.0],\n",
       " ['distract', 0.0],\n",
       " ['district', 0.0],\n",
       " ['disturb', 0.0],\n",
       " ['dither', 0.0],\n",
       " ['diver', 0.0],\n",
       " ['divers', 0.0],\n",
       " ['divert', 0.0],\n",
       " ['divid', 0.0],\n",
       " ['divis', 0.0],\n",
       " ['dkaim', 0.0],\n",
       " ['dissect', 0.0],\n",
       " ['disrepair', 0.0],\n",
       " ['discount', 0.0],\n",
       " ['disput', 0.0],\n",
       " ['discov', 0.0],\n",
       " ['discrimin', 0.0],\n",
       " ['discuss', 0.0],\n",
       " ['diseas', 0.0],\n",
       " ['disgust', 0.0],\n",
       " ['justic', 0.0],\n",
       " ['dish', 0.0],\n",
       " ['disk', 0.0],\n",
       " ['dislik', 0.0],\n",
       " ['dismal', 0.0],\n",
       " ['dismantl', 0.0],\n",
       " ['dismiss', 0.0],\n",
       " ['dispatch', 0.0],\n",
       " ['displac', 0.0],\n",
       " ['display', 0.0],\n",
       " ['devyani', 0.0],\n",
       " ['devote', 0.0],\n",
       " ['devic', 0.0],\n",
       " ['definit', 0.0],\n",
       " ['del', 0.0],\n",
       " ['delay', 0.0],\n",
       " ['delhi', 0.0],\n",
       " ['deliber', 0.0],\n",
       " ['delic', 0.0],\n",
       " ['deliv', 0.0],\n",
       " ['dell', 0.0],\n",
       " ['dem', 0.0],\n",
       " ['demand', 0.0],\n",
       " ['demilitar', 0.0],\n",
       " ['demo', 0.0],\n",
       " ['democrat', 0.0],\n",
       " ['demolish', 0.0],\n",
       " ['demolit', 0.0],\n",
       " ['demonstr', 0.0],\n",
       " ['defrock', 0.0],\n",
       " ['defin', 0.0],\n",
       " ['denial', 0.0],\n",
       " ['deficit', 0.0],\n",
       " ['deduct', 0.0],\n",
       " ['dee', 0.0],\n",
       " ['deep', 0.0],\n",
       " ['deepen', 0.0],\n",
       " ['deepli', 0.0],\n",
       " ['defac', 0.0],\n",
       " ['default', 0.0],\n",
       " ['defeat', 0.0],\n",
       " ['defect', 0.0],\n",
       " ['defenc', 0.0],\n",
       " ['defend', 0.0],\n",
       " ['defens', 0.0],\n",
       " ['defer', 0.0],\n",
       " ['defi', 0.0],\n",
       " ['defiant', 0.0],\n",
       " ['deni', 0.0],\n",
       " ['denounc', 0.0],\n",
       " ['deviant', 0.0],\n",
       " ['desk', 0.0],\n",
       " ['despit', 0.0],\n",
       " ['destin', 0.0],\n",
       " ['destroy', 0.0],\n",
       " ['destruct', 0.0],\n",
       " ['detail', 0.0],\n",
       " ['detain', 0.0],\n",
       " ['detaine', 0.0],\n",
       " ['detect', 0.0],\n",
       " ['detent', 0.0],\n",
       " ['deter', 0.0],\n",
       " ['deterior', 0.0],\n",
       " ['deterr', 0.0],\n",
       " ['detroit', 0.0],\n",
       " ['devast', 0.0],\n",
       " ['develop', 0.0],\n",
       " ['desper', 0.0],\n",
       " ['design', 0.0],\n",
       " ['densiti', 0.0],\n",
       " ['desh', 0.0],\n",
       " ['dental', 0.0],\n",
       " ['denton', 0.0],\n",
       " ['depart', 0.0],\n",
       " ['deplor', 0.0],\n",
       " ['deport', 0.0],\n",
       " ['depress', 0.0],\n",
       " ['dept', 0.0],\n",
       " ['depth', 0.0],\n",
       " ['deputi', 0.0],\n",
       " ['der', 0.0],\n",
       " ['derail', 0.0],\n",
       " ['derrick', 0.0],\n",
       " ['describ', 0.0],\n",
       " ['desert', 0.0],\n",
       " ['deserv', 0.0],\n",
       " ['dna', 0.0],\n",
       " ['doctor', 0.0],\n",
       " ['document', 0.0],\n",
       " ['ebbcf', 0.0],\n",
       " ['ebf', 0.0],\n",
       " ['ebfb', 0.0],\n",
       " ['ebrd', 0.0],\n",
       " ['ec', 0.0],\n",
       " ['ecabec', 0.0],\n",
       " ['eccbdc', 0.0],\n",
       " ['eccccab', 0.0],\n",
       " ['ecf', 0.0],\n",
       " ['ecffcb', 0.0],\n",
       " ['econom', 0.0],\n",
       " ['economi', 0.0],\n",
       " ['economist', 0.0],\n",
       " ['ecuador', 0.0],\n",
       " ['ecuadorean', 0.0],\n",
       " ['ed', 0.0],\n",
       " ['ebe', 0.0],\n",
       " ['ebbaf', 0.0],\n",
       " ['edc', 0.0],\n",
       " ['ebba', 0.0],\n",
       " ['eager', 0.0],\n",
       " ['eagl', 0.0],\n",
       " ['earli', 0.0],\n",
       " ['earlier', 0.0],\n",
       " ['earn', 0.0],\n",
       " ['earth', 0.0],\n",
       " ['earthquak', 0.0],\n",
       " ['eas', 0.0],\n",
       " ['east', 0.0],\n",
       " ['eastern', 0.0],\n",
       " ['eat', 0.0],\n",
       " ['eavesdrop', 0.0],\n",
       " ['eb', 0.0],\n",
       " ['ebay', 0.0],\n",
       " ['ebb', 0.0],\n",
       " ['edb', 0.0],\n",
       " ['edcfa', 0.0],\n",
       " ['eaf', 0.0],\n",
       " ['efb', 0.0],\n",
       " ['efbbbb', 0.0],\n",
       " ['efc', 0.0],\n",
       " ['efcceb', 0.0],\n",
       " ['efd', 0.0],\n",
       " ['efe', 0.0],\n",
       " ['effbff', 0.0],\n",
       " ['effc', 0.0],\n",
       " ['effect', 0.0],\n",
       " ['effort', 0.0],\n",
       " ['efreau', 0.0],\n",
       " ['efrfku', 0.0],\n",
       " ['efrfkui', 0.0],\n",
       " ['efrfkur', 0.0],\n",
       " ['efrfro', 0.0],\n",
       " ['egan', 0.0],\n",
       " ['efbafaf', 0.0],\n",
       " ['efaefcb', 0.0],\n",
       " ['edf', 0.0],\n",
       " ['efa', 0.0],\n",
       " ['edfdca', 0.0],\n",
       " ['edffefb', 0.0],\n",
       " ['edg', 0.0],\n",
       " ['editori', 0.0],\n",
       " ['edmundson', 0.0],\n",
       " ['edt', 0.0],\n",
       " ['educ', 0.0],\n",
       " ['edward', 0.0],\n",
       " ['eeb', 0.0],\n",
       " ['eebd', 0.0],\n",
       " ['eec', 0.0],\n",
       " ['eecfbbadfc', 0.0],\n",
       " ['eed', 0.0],\n",
       " ['eefaa', 0.0],\n",
       " ['ef', 0.0],\n",
       " ['eafc', 0.0],\n",
       " ['eaeabb', 0.0],\n",
       " ['dodger', 0.0],\n",
       " ['doomsday', 0.0],\n",
       " ['dormic', 0.0],\n",
       " ['dot', 0.0],\n",
       " ['doubl', 0.0],\n",
       " ['doubt', 0.0],\n",
       " ['dover', 0.0],\n",
       " ['downplay', 0.0],\n",
       " ['downstream', 0.0],\n",
       " ['dowri', 0.0],\n",
       " ['dozen', 0.0],\n",
       " ['draft', 0.0],\n",
       " ['drag', 0.0],\n",
       " ['drama', 0.0],\n",
       " ['draw', 0.0],\n",
       " ['drawn', 0.0],\n",
       " ['dream', 0.0],\n",
       " ['dope', 0.0],\n",
       " ['doom', 0.0],\n",
       " ['dress', 0.0],\n",
       " ['doolittl', 0.0],\n",
       " ['doe', 0.0],\n",
       " ['doesnt', 0.0],\n",
       " ['dog', 0.0],\n",
       " ['doha', 0.0],\n",
       " ['doj', 0.0],\n",
       " ['dol', 0.0],\n",
       " ['doll', 0.0],\n",
       " ['dollar', 0.0],\n",
       " ['dolphin', 0.0],\n",
       " ['domin', 0.0],\n",
       " ['donald', 0.0],\n",
       " ['donat', 0.0],\n",
       " ['done', 0.0],\n",
       " ['donor', 0.0],\n",
       " ['dont', 0.0],\n",
       " ['dreamlin', 0.0],\n",
       " ['dri', 0.0],\n",
       " ['ead', 0.0],\n",
       " ['dudzick', 0.0],\n",
       " ['dui', 0.0],\n",
       " ['duke', 0.0],\n",
       " ['dumb', 0.0],\n",
       " ['dumbarton', 0.0],\n",
       " ['dump', 0.0],\n",
       " ['duo', 0.0],\n",
       " ['durham', 0.0],\n",
       " ['dustin', 0.0],\n",
       " ['dutch', 0.0],\n",
       " ['dwindl', 0.0],\n",
       " ['dye', 0.0],\n",
       " ['dynasti', 0.0],\n",
       " ['ea', 0.0],\n",
       " ['eaa', 0.0],\n",
       " ['eaaac', 0.0],\n",
       " ['due', 0.0],\n",
       " ['dudek', 0.0],\n",
       " ['drill', 0.0],\n",
       " ['duckfat', 0.0],\n",
       " ['drink', 0.0],\n",
       " ['driscol', 0.0],\n",
       " ['drive', 0.0],\n",
       " ['driver', 0.0],\n",
       " ['drone', 0.0],\n",
       " ['drop', 0.0],\n",
       " ['drove', 0.0],\n",
       " ['drown', 0.0],\n",
       " ['drug', 0.0],\n",
       " ['drunk', 0.0],\n",
       " ['drunken', 0.0],\n",
       " ['dual', 0.0],\n",
       " ['dubai', 0.0],\n",
       " ['duchess', 0.0],\n",
       " ['duck', 0.0],\n",
       " ['fniisx', 0.0],\n",
       " ['fniivw', 0.0],\n",
       " ['juic', 0.0],\n",
       " ['icahn', 0.0],\n",
       " ['hvu', 0.0],\n",
       " ['hydraul', 0.0],\n",
       " ['hydrocodon', 0.0],\n",
       " ['hypocrisi', 0.0],\n",
       " ['iaeoklgupuphfh', 0.0],\n",
       " ['ibrahim', 0.0],\n",
       " ['icac', 0.0],\n",
       " ['icc', 0.0],\n",
       " ['henderson', 0.0],\n",
       " ['ice', 0.0],\n",
       " ['id', 0.0],\n",
       " ['idaho', 0.0],\n",
       " ['idahoan', 0.0],\n",
       " ['idd', 0.0],\n",
       " ['identifi', 0.0],\n",
       " ['idinbrebvn', 0.0],\n",
       " ['hurt', 0.0],\n",
       " ['hurrican', 0.0],\n",
       " ['hurdl', 0.0],\n",
       " ['hunter', 0.0],\n",
       " ['huawei', 0.0],\n",
       " ['hub', 0.0],\n",
       " ['huddl', 0.0],\n",
       " ['hudson', 0.0],\n",
       " ['huffington', 0.0],\n",
       " ['hug', 0.0],\n",
       " ['huge', 0.0],\n",
       " ['hulk', 0.0],\n",
       " ['human', 0.0],\n",
       " ['humanitarian', 0.0],\n",
       " ['humbl', 0.0],\n",
       " ['hummingbird', 0.0],\n",
       " ['hundr', 0.0],\n",
       " ['hungri', 0.0],\n",
       " ['hunt', 0.0],\n",
       " ['idinln', 0.0],\n",
       " ['idinlnhjq', 0.0],\n",
       " ['idinlnicv', 0.0],\n",
       " ['implic', 0.0],\n",
       " ['impot', 0.0],\n",
       " ['imprison', 0.0],\n",
       " ['impromptu', 0.0],\n",
       " ['improv', 0.0],\n",
       " ['impuls', 0.0],\n",
       " ['imxnbevapp', 0.0],\n",
       " ['in', 0.0],\n",
       " ['inadequ', 0.0],\n",
       " ['inaugur', 0.0],\n",
       " ['inc', 0.0],\n",
       " ['inch', 0.0],\n",
       " ['incid', 0.0],\n",
       " ['includ', 0.0],\n",
       " ['increas', 0.0],\n",
       " ['increasingli', 0.0],\n",
       " ['import', 0.0],\n",
       " ['impass', 0.0],\n",
       " ['idinlnkyf', 0.0],\n",
       " ['impal', 0.0],\n",
       " ['ifprdjzhatro', 0.0],\n",
       " ['ignor', 0.0],\n",
       " ['igp', 0.0],\n",
       " ['ikea', 0.0],\n",
       " ['ill', 0.0],\n",
       " ['illeg', 0.0],\n",
       " ['illinoi', 0.0],\n",
       " ['illus', 0.0],\n",
       " ['imag', 0.0],\n",
       " ['imf', 0.0],\n",
       " ['immigr', 0.0],\n",
       " ['immin', 0.0],\n",
       " ['immol', 0.0],\n",
       " ['immun', 0.0],\n",
       " ['impact', 0.0],\n",
       " ['hoyal', 0.0],\n",
       " ['hover', 0.0],\n",
       " ['houston', 0.0],\n",
       " ['hike', 0.0],\n",
       " ['hillari', 0.0],\n",
       " ['hinder', 0.0],\n",
       " ['hindu', 0.0],\n",
       " ['hint', 0.0],\n",
       " ['hip', 0.0],\n",
       " ['hippi', 0.0],\n",
       " ['hire', 0.0],\n",
       " ['histor', 0.0],\n",
       " ['histori', 0.0],\n",
       " ['hit', 0.0],\n",
       " ['hitch', 0.0],\n",
       " ['hitler', 0.0],\n",
       " ['hiv', 0.0],\n",
       " ['hizbollah', 0.0],\n",
       " ['ho', 0.0],\n",
       " ['hill', 0.0],\n",
       " ['hijack', 0.0],\n",
       " ['hoffman', 0.0],\n",
       " ['hijab', 0.0],\n",
       " ['herald', 0.0],\n",
       " ['here', 0.0],\n",
       " ['heritag', 0.0],\n",
       " ['hero', 0.0],\n",
       " ['heroin', 0.0],\n",
       " ['herrera', 0.0],\n",
       " ['hezbollah', 0.0],\n",
       " ['hidden', 0.0],\n",
       " ['hide', 0.0],\n",
       " ['hideout', 0.0],\n",
       " ['high', 0.0],\n",
       " ['higher', 0.0],\n",
       " ['highest', 0.0],\n",
       " ['highlight', 0.0],\n",
       " ['highway', 0.0],\n",
       " ['hobbl', 0.0],\n",
       " ['hogan', 0.0],\n",
       " ['housem', 0.0],\n",
       " ['honor', 0.0],\n",
       " ['hood', 0.0],\n",
       " ['hook', 0.0],\n",
       " ['hooker', 0.0],\n",
       " ['hop', 0.0],\n",
       " ['hope', 0.0],\n",
       " ['hosni', 0.0],\n",
       " ['hospit', 0.0],\n",
       " ['host', 0.0],\n",
       " ['hostag', 0.0],\n",
       " ['hostess', 0.0],\n",
       " ['hostil', 0.0],\n",
       " ['hot', 0.0],\n",
       " ['hotel', 0.0],\n",
       " ['hour', 0.0],\n",
       " ['hous', 0.0],\n",
       " ['honour', 0.0],\n",
       " ['honolulu', 0.0],\n",
       " ['hold', 0.0],\n",
       " ['hong', 0.0],\n",
       " ['holder', 0.0],\n",
       " ['holi', 0.0],\n",
       " ['holiday', 0.0],\n",
       " ['hollow', 0.0],\n",
       " ['hollywood', 0.0],\n",
       " ['holm', 0.0],\n",
       " ['home', 0.0],\n",
       " ['homebuy', 0.0],\n",
       " ['homeland', 0.0],\n",
       " ['homeless', 0.0],\n",
       " ['homemad', 0.0],\n",
       " ['homicid', 0.0],\n",
       " ['homosexu', 0.0],\n",
       " ['hondura', 0.0],\n",
       " ['honest', 0.0],\n",
       " ['ind', 0.0],\n",
       " ['independ', 0.0],\n",
       " ['index', 0.0],\n",
       " ['item', 0.0],\n",
       " ['ivori', 0.0],\n",
       " ['ix', 0.0],\n",
       " ['izhvi', 0.0],\n",
       " ['jackson', 0.0],\n",
       " ['jacksonvil', 0.0],\n",
       " ['jahi', 0.0],\n",
       " ['jail', 0.0],\n",
       " ['jal', 0.0],\n",
       " ['jamaat', 0.0],\n",
       " ['jamaica', 0.0],\n",
       " ['jame', 0.0],\n",
       " ['janet', 0.0],\n",
       " ['japan', 0.0],\n",
       " ['japanes', 0.0],\n",
       " ['jasper', 0.0],\n",
       " ['iuzbkteo', 0.0],\n",
       " ['italian', 0.0],\n",
       " ['jazeera', 0.0],\n",
       " ['itali', 0.0],\n",
       " ['iron', 0.0],\n",
       " ['ironi', 0.0],\n",
       " ['irrefut', 0.0],\n",
       " ['irrepress', 0.0],\n",
       " ['irvin', 0.0],\n",
       " ['isdktgidhotab', 0.0],\n",
       " ['isl', 0.0],\n",
       " ['islam', 0.0],\n",
       " ['islami', 0.0],\n",
       " ['islamist', 0.0],\n",
       " ['island', 0.0],\n",
       " ['ison', 0.0],\n",
       " ['isra', 0.0],\n",
       " ['israel', 0.0],\n",
       " ['issu', 0.0],\n",
       " ['jayden', 0.0],\n",
       " ['jazz', 0.0],\n",
       " ['iraqi', 0.0],\n",
       " ['joint', 0.0],\n",
       " ['joliet', 0.0],\n",
       " ['jolt', 0.0],\n",
       " ['jon', 0.0],\n",
       " ['jone', 0.0],\n",
       " ['joplin', 0.0],\n",
       " ['jordan', 0.0],\n",
       " ['joseph', 0.0],\n",
       " ['journalist', 0.0],\n",
       " ['journey', 0.0],\n",
       " ['joy', 0.0],\n",
       " ['jpha', 0.0],\n",
       " ['jpmorgan', 0.0],\n",
       " ['judg', 0.0],\n",
       " ['judici', 0.0],\n",
       " ['judith', 0.0],\n",
       " ['joke', 0.0],\n",
       " ['join', 0.0],\n",
       " ['jean', 0.0],\n",
       " ['john', 0.0],\n",
       " ['jeep', 0.0],\n",
       " ['jeff', 0.0],\n",
       " ['jeffrey', 0.0],\n",
       " ['jeopard', 0.0],\n",
       " ['jersey', 0.0],\n",
       " ['jerusalem', 0.0],\n",
       " ['jess', 0.0],\n",
       " ['jesu', 0.0],\n",
       " ['jet', 0.0],\n",
       " ['jihadi', 0.0],\n",
       " ['jo', 0.0],\n",
       " ['joan', 0.0],\n",
       " ['joaquin', 0.0],\n",
       " ['job', 0.0],\n",
       " ['jofi', 0.0],\n",
       " ['ireland', 0.0],\n",
       " ['iraq', 0.0],\n",
       " ['india', 0.0],\n",
       " ['inject', 0.0],\n",
       " ['injuri', 0.0],\n",
       " ['injustic', 0.0],\n",
       " ['ink', 0.0],\n",
       " ['inmat', 0.0],\n",
       " ['innoc', 0.0],\n",
       " ['innov', 0.0],\n",
       " ['inquiri', 0.0],\n",
       " ['insan', 0.0],\n",
       " ['insid', 0.0],\n",
       " ['insist', 0.0],\n",
       " ['inspect', 0.0],\n",
       " ['inspector', 0.0],\n",
       " ['inspir', 0.0],\n",
       " ['instal', 0.0],\n",
       " ['institut', 0.0],\n",
       " ['injur', 0.0],\n",
       " ['initiat', 0.0],\n",
       " ['insult', 0.0],\n",
       " ['initi', 0.0],\n",
       " ['indian', 0.0],\n",
       " ['indiana', 0.0],\n",
       " ['indic', 0.0],\n",
       " ['indict', 0.0],\n",
       " ['indo', 0.0],\n",
       " ['indonesia', 0.0],\n",
       " ['industri', 0.0],\n",
       " ['inequ', 0.0],\n",
       " ['inexcus', 0.0],\n",
       " ['infect', 0.0],\n",
       " ['inferno', 0.0],\n",
       " ['info', 0.0],\n",
       " ['inform', 0.0],\n",
       " ['ingredi', 0.0],\n",
       " ['inhof', 0.0],\n",
       " ['instructor', 0.0],\n",
       " ['insur', 0.0],\n",
       " ['iranian', 0.0],\n",
       " ['intox', 0.0],\n",
       " ['intrud', 0.0],\n",
       " ['jump', 0.0],\n",
       " ['intrus', 0.0],\n",
       " ['invest', 0.0],\n",
       " ['investig', 0.0],\n",
       " ['investor', 0.0],\n",
       " ['invit', 0.0],\n",
       " ['involv', 0.0],\n",
       " ['inyemen', 0.0],\n",
       " ['iowa', 0.0],\n",
       " ['ip', 0.0],\n",
       " ['ipad', 0.0],\n",
       " ['iphon', 0.0],\n",
       " ['ir', 0.0],\n",
       " ['iran', 0.0],\n",
       " ['introduc', 0.0],\n",
       " ['interview', 0.0],\n",
       " ['insurg', 0.0],\n",
       " ['intervent', 0.0],\n",
       " ['integr', 0.0],\n",
       " ['intel', 0.0],\n",
       " ['intellectu', 0.0],\n",
       " ['intellig', 0.0],\n",
       " ['intent', 0.0],\n",
       " ['intercept', 0.0],\n",
       " ['intercompani', 0.0],\n",
       " ['interest', 0.0],\n",
       " ['interfer', 0.0],\n",
       " ['interim', 0.0],\n",
       " ['intermountain', 0.0],\n",
       " ['intern', 0.0],\n",
       " ['internet', 0.0],\n",
       " ['interpol', 0.0],\n",
       " ['interrog', 0.0],\n",
       " ['henni', 0.0],\n",
       " ['hemp', 0.0],\n",
       " ['fniiyv', 0.0],\n",
       " ['garnett', 0.0],\n",
       " ['game', 0.0],\n",
       " ['gang', 0.0],\n",
       " ['gangrap', 0.0],\n",
       " ['ganja', 0.0],\n",
       " ['gap', 0.0],\n",
       " ['garcetti', 0.0],\n",
       " ['garner', 0.0],\n",
       " ['gaslin', 0.0],\n",
       " ['help', 0.0],\n",
       " ['gate', 0.0],\n",
       " ['gather', 0.0],\n",
       " ['gave', 0.0],\n",
       " ['gaxhjuni', 0.0],\n",
       " ['gay', 0.0],\n",
       " ['gaz', 0.0],\n",
       " ['gaza', 0.0],\n",
       " ['juri', 0.0],\n",
       " ['gambl', 0.0],\n",
       " ['gambit', 0.0],\n",
       " ['gallon', 0.0],\n",
       " ['full', 0.0],\n",
       " ['fulli', 0.0],\n",
       " ['fund', 0.0],\n",
       " ['fundrais', 0.0],\n",
       " ['funer', 0.0],\n",
       " ['furi', 0.0],\n",
       " ['furlough', 0.0],\n",
       " ['fuse', 0.0],\n",
       " ['futur', 0.0],\n",
       " ['ga', 0.0],\n",
       " ['gadget', 0.0],\n",
       " ['gaga', 0.0],\n",
       " ['gain', 0.0],\n",
       " ['gal', 0.0],\n",
       " ['galleri', 0.0],\n",
       " ['gazett', 0.0],\n",
       " ['ge', 0.0],\n",
       " ['gear', 0.0],\n",
       " ['giraldi', 0.0],\n",
       " ['gismweckwz', 0.0],\n",
       " ['gitmo', 0.0],\n",
       " ['give', 0.0],\n",
       " ['giveaway', 0.0],\n",
       " ['given', 0.0],\n",
       " ['glass', 0.0],\n",
       " ['glich', 0.0],\n",
       " ['glitch', 0.0],\n",
       " ['global', 0.0],\n",
       " ['globe', 0.0],\n",
       " ['gmail', 0.0],\n",
       " ['go', 0.0],\n",
       " ['goa', 0.0],\n",
       " ['goal', 0.0],\n",
       " ['goe', 0.0],\n",
       " ['girl', 0.0],\n",
       " ['gingrich', 0.0],\n",
       " ['geiss', 0.0],\n",
       " ['gilroy', 0.0],\n",
       " ['gener', 0.0],\n",
       " ['geneva', 0.0],\n",
       " ['geniu', 0.0],\n",
       " ['genocid', 0.0],\n",
       " ['geographi', 0.0],\n",
       " ['georg', 0.0],\n",
       " ['georgia', 0.0],\n",
       " ['german', 0.0],\n",
       " ['germani', 0.0],\n",
       " ['get', 0.0],\n",
       " ['gettysburg', 0.0],\n",
       " ['gi', 0.0],\n",
       " ['giant', 0.0],\n",
       " ['gibney', 0.0],\n",
       " ['gift', 0.0],\n",
       " ['fulfil', 0.0],\n",
       " ['fukushima', 0.0],\n",
       " ...]"
      ]
     },
     "execution_count": 769,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab=aa[0]#model.coef_[0]\n",
    "[[key_cols[i],ab[i]] for i in np.argsort(abs(ab))[::-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_rec,prec,F1: [1.0, 0.77187932464248254, 0.87119529697081077] avg_validation_rec,prec,F1: [0.97070707070707074, 0.62297619047619035, 0.75461295226512615]\n"
     ]
    }
   ],
   "source": [
    "predictorgdelt.kfold_val_class(10,'apriltodectfidf','logreg',['l1',0.095],10,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_rec,prec,F1: [0.93333333333333335, 0.3888888888888889, 0.5490196078431373]\n"
     ]
    }
   ],
   "source": [
    "aa=predictorgdelt.kfold_test_class('apriltodectfidf','logreg',['l1',0.095],0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 583,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictorgdelt.ydata['apriltodectfidf'][1][:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictorgdelt.yhat_class['apriltodectfidf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_rec,prec,F1: [0.99316945840049764, 0.87472075977840691, 0.93013643870901608] avg_validation_rec,prec,F1: [0.78934065934065933, 0.60476190476190483, 0.65097784615687426]\n"
     ]
    }
   ],
   "source": [
    "predictorgdelt.kfold_val_class(10,'7daystfidf','svclass',[0.01,'poly'],10,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_rec,prec,F1: [0.69230769230769229, 0.75, 0.71999999999999986]\n"
     ]
    }
   ],
   "source": [
    "predictorgdelt.kfold_test_class('7daystfidf','svclass',[0.01,'poly'],0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  1.,  1.,  0.,  1.,  1.,  0.,  1.,  0.,  0.,  1.,  1.,\n",
       "        1.,  1.,  1.,  0.,  1.,  0.,  0.])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictorgdelt.yhat_class['7daystfidf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  1.,  1.,  0.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  0.])"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictorgdelt.ydata['7daystfidf'][1][:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#downloading and unzipping, run at your own risk, contains dreadful shell commands\n",
    "for date in range(20131001,20131032):\n",
    "    os.system('wget http://data.gdeltproject.org/events/'+str(date)+'.export.CSV.zip')\n",
    "    os.system('unzip '+str(date)+'.export.CSV.zip')\n",
    "    os.system('mv '+str(date)+'.export.CSV data/GDELT_1.0')\n",
    "    os.system('rm '+str(date)+'.export.CSV.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 Maxos  staff    10M May 20  2013 data/GDELT_1.0/20130401.export.CSV\r\n"
     ]
    }
   ],
   "source": [
    "!ls -hl data/GDELT_1.0/20130401.export.CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "header_daily=pd.read_csv('data/GDELT_1.0/CSV.header.dailyupdates.txt',delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GLOBALEVENTID</th>\n",
       "      <th>SQLDATE</th>\n",
       "      <th>MonthYear</th>\n",
       "      <th>Year</th>\n",
       "      <th>FractionDate</th>\n",
       "      <th>Actor1Code</th>\n",
       "      <th>Actor1Name</th>\n",
       "      <th>Actor1CountryCode</th>\n",
       "      <th>Actor1KnownGroupCode</th>\n",
       "      <th>Actor1EthnicCode</th>\n",
       "      <th>...</th>\n",
       "      <th>Actor2Geo_FeatureID</th>\n",
       "      <th>ActionGeo_Type</th>\n",
       "      <th>ActionGeo_FullName</th>\n",
       "      <th>ActionGeo_CountryCode</th>\n",
       "      <th>ActionGeo_ADM1Code</th>\n",
       "      <th>ActionGeo_Lat</th>\n",
       "      <th>ActionGeo_Long</th>\n",
       "      <th>ActionGeo_FeatureID</th>\n",
       "      <th>DATEADDED</th>\n",
       "      <th>SOURCEURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>253461012</td>\n",
       "      <td>20030404</td>\n",
       "      <td>200304</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003.2575</td>\n",
       "      <td>AUS</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>AUS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>AS</td>\n",
       "      <td>1</td>\n",
       "      <td>Australia</td>\n",
       "      <td>AS</td>\n",
       "      <td>AS</td>\n",
       "      <td>-27.0000</td>\n",
       "      <td>133.000</td>\n",
       "      <td>AS</td>\n",
       "      <td>20130401</td>\n",
       "      <td>http://www.bangkokpost.com/breakingnews/343522...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>253461013</td>\n",
       "      <td>20030404</td>\n",
       "      <td>200304</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003.2575</td>\n",
       "      <td>BUS</td>\n",
       "      <td>SHOP OWNER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-1354145</td>\n",
       "      <td>4</td>\n",
       "      <td>Tai Hang, Hong Kong (general), Hong Kong</td>\n",
       "      <td>HK</td>\n",
       "      <td>HK00</td>\n",
       "      <td>22.4667</td>\n",
       "      <td>114.150</td>\n",
       "      <td>-1354145</td>\n",
       "      <td>20130401</td>\n",
       "      <td>http://www.bloomberg.com/news/2013-04-01/hong-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>253461014</td>\n",
       "      <td>20030404</td>\n",
       "      <td>200304</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003.2575</td>\n",
       "      <td>BUS</td>\n",
       "      <td>SHOP OWNER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-1354454</td>\n",
       "      <td>4</td>\n",
       "      <td>Tai Hang, Hong Kong (general), Hong Kong</td>\n",
       "      <td>HK</td>\n",
       "      <td>HK00</td>\n",
       "      <td>22.4667</td>\n",
       "      <td>114.150</td>\n",
       "      <td>-1354145</td>\n",
       "      <td>20130401</td>\n",
       "      <td>http://www.bloomberg.com/news/2013-04-01/hong-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>253461015</td>\n",
       "      <td>20030404</td>\n",
       "      <td>200304</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003.2575</td>\n",
       "      <td>CVL</td>\n",
       "      <td>MIGRANT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>AS</td>\n",
       "      <td>1</td>\n",
       "      <td>Australia</td>\n",
       "      <td>AS</td>\n",
       "      <td>AS</td>\n",
       "      <td>-27.0000</td>\n",
       "      <td>133.000</td>\n",
       "      <td>AS</td>\n",
       "      <td>20130401</td>\n",
       "      <td>http://www.bangkokpost.com/breakingnews/343522...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>253461016</td>\n",
       "      <td>20030404</td>\n",
       "      <td>200304</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003.2575</td>\n",
       "      <td>HLH</td>\n",
       "      <td>DOCTOR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>Nevada, United States</td>\n",
       "      <td>US</td>\n",
       "      <td>USNV</td>\n",
       "      <td>38.4199</td>\n",
       "      <td>-117.122</td>\n",
       "      <td>NV</td>\n",
       "      <td>20130401</td>\n",
       "      <td>http://www.startribune.com/nation/200818961.html</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   GLOBALEVENTID   SQLDATE  MonthYear  Year  FractionDate Actor1Code  \\\n",
       "0      253461012  20030404     200304  2003     2003.2575        AUS   \n",
       "1      253461013  20030404     200304  2003     2003.2575        BUS   \n",
       "2      253461014  20030404     200304  2003     2003.2575        BUS   \n",
       "3      253461015  20030404     200304  2003     2003.2575        CVL   \n",
       "4      253461016  20030404     200304  2003     2003.2575        HLH   \n",
       "\n",
       "   Actor1Name Actor1CountryCode Actor1KnownGroupCode Actor1EthnicCode  \\\n",
       "0   AUSTRALIA               AUS                  NaN              NaN   \n",
       "1  SHOP OWNER               NaN                  NaN              NaN   \n",
       "2  SHOP OWNER               NaN                  NaN              NaN   \n",
       "3     MIGRANT               NaN                  NaN              NaN   \n",
       "4      DOCTOR               NaN                  NaN              NaN   \n",
       "\n",
       "                         ...                         Actor2Geo_FeatureID  \\\n",
       "0                        ...                                          AS   \n",
       "1                        ...                                    -1354145   \n",
       "2                        ...                                    -1354454   \n",
       "3                        ...                                          AS   \n",
       "4                        ...                                         NaN   \n",
       "\n",
       "  ActionGeo_Type                        ActionGeo_FullName  \\\n",
       "0              1                                 Australia   \n",
       "1              4  Tai Hang, Hong Kong (general), Hong Kong   \n",
       "2              4  Tai Hang, Hong Kong (general), Hong Kong   \n",
       "3              1                                 Australia   \n",
       "4              2                     Nevada, United States   \n",
       "\n",
       "  ActionGeo_CountryCode ActionGeo_ADM1Code ActionGeo_Lat ActionGeo_Long  \\\n",
       "0                    AS                 AS      -27.0000        133.000   \n",
       "1                    HK               HK00       22.4667        114.150   \n",
       "2                    HK               HK00       22.4667        114.150   \n",
       "3                    AS                 AS      -27.0000        133.000   \n",
       "4                    US               USNV       38.4199       -117.122   \n",
       "\n",
       "  ActionGeo_FeatureID DATEADDED  \\\n",
       "0                  AS  20130401   \n",
       "1            -1354145  20130401   \n",
       "2            -1354145  20130401   \n",
       "3                  AS  20130401   \n",
       "4                  NV  20130401   \n",
       "\n",
       "                                           SOURCEURL  \n",
       "0  http://www.bangkokpost.com/breakingnews/343522...  \n",
       "1  http://www.bloomberg.com/news/2013-04-01/hong-...  \n",
       "2  http://www.bloomberg.com/news/2013-04-01/hong-...  \n",
       "3  http://www.bangkokpost.com/breakingnews/343522...  \n",
       "4   http://www.startribune.com/nation/200818961.html  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is just to show what the GDELT files look like\n",
    "sample_df=pd.read_csv('data/GDELT_1.0/20130401.export.CSV',delimiter='\\t')\n",
    "sample_df.columns=list(header_daily)\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.rte.ie/news/2013/0401/379281-india-drug-patent-novartis/'"
      ]
     },
     "execution_count": 1206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_url[0][1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from urllib.request import urlopen\n",
    "#from bs4 import BeautifulSoup\n",
    "\n",
    "#with urlopen(\"https://www.google.com\") as response:\n",
    "        #html = response.read()\n",
    "#        soup = BeautifulSoup(response)\n",
    "\n",
    "\n",
    "\n",
    "#soup = BeautifulSoup(urlopen(\"https://github.com/jbwhit/jupyter-tips-and-tricks/tree/master/deliver\"))    \n",
    "    \n",
    "#for url in corpus_url[0]:\n",
    " #   print(url[1])\n",
    "    #soup = BeautifulSoup(urlopen(url[1]))#, \"lxml\"))#\"https://www.google.com\"), \"lxml\")\n",
    "#print(soup.title.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "re_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "punctuation = re.compile(r'[-.?!,\":;()|0-9]')\n",
    "stop_words = set(stopwords.words('english')+[\"\"])\n",
    "porter = PorterStemmer()\n",
    "vowels = list(\"aeiouy\")\n",
    "consonants = list(\"bcdfghjklmnpqrstvwxz\")\n",
    "spurious_beginnings = re.compile(r'idind.|idus.|iduk.')\n",
    "\n",
    "def url_tokenizer(url):\n",
    "    c,d,e=[],[],[]\n",
    "    if url!='BBC Monitoring':\n",
    "        a=urlparse(url)[2].split('.')[0].split('/')[-1]\n",
    "        b = re_tokenizer.tokenize(a.lower())\n",
    "        for word in b:\n",
    "            c+=[punctuation.sub(\"\", word)]\n",
    "        for word in c:\n",
    "            if word not in stop_words:\n",
    "                d+=[word]\n",
    "        if len(d)<=1:\n",
    "            return []\n",
    "        for word in d:\n",
    "            stemtemp=porter.stem(word)\n",
    "            length=len(stemtemp)\n",
    "            unique=len(set(stemtemp))\n",
    "            num_vow=sum(stemtemp.count(c) for c in vowels)\n",
    "            num_cons=sum(stemtemp.count(c) for c in consonants)\n",
    "            if length<15 and (num_cons-num_vow)<7 and unique>1 and num_vow>0 and (length-unique)<5 and not spurious_beginnings.match(stemtemp) and '_' not in stemtemp:\n",
    "                e+=[stemtemp]\n",
    "    return e\n",
    "\n",
    "def wrapper_tokenizer(url_doc):\n",
    "    wordlist=[]\n",
    "    for url in url_doc:\n",
    "        for mentions in range(url[0]):\n",
    "            wordlist+=url_tokenizer(url[1])\n",
    "    return wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 996,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['bash', 'trick', 'file', 'size', 'byte', 'kilobyt', 'megabyt', 'gigabyt'],\n",
       " ['softwar', 'test', 'data', 'scienc'])"
      ]
     },
     "execution_count": 996,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_tokenizer('http://iosdevelopertips.com/bash/bash-trick-file-sizes-byte-kilobyte-megabyte-gigabyte.html'),url_tokenizer('http://alexgude.com/blog/software-testing-for-data-science')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bash',\n",
       " 'trick',\n",
       " 'file',\n",
       " 'size',\n",
       " 'byte',\n",
       " 'kilobyt',\n",
       " 'megabyt',\n",
       " 'gigabyt',\n",
       " 'bash',\n",
       " 'trick',\n",
       " 'file',\n",
       " 'size',\n",
       " 'byte',\n",
       " 'kilobyt',\n",
       " 'megabyt',\n",
       " 'gigabyt',\n",
       " 'bash',\n",
       " 'trick',\n",
       " 'file',\n",
       " 'size',\n",
       " 'byte',\n",
       " 'kilobyt',\n",
       " 'megabyt',\n",
       " 'gigabyt',\n",
       " 'softwar',\n",
       " 'test',\n",
       " 'data',\n",
       " 'scienc']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " wrapper_tokenizer([[3,'http://iosdevelopertips.com/bash/bash-trick-file-sizes-byte-kilobyte-megabyte-gigabyte.html']\n",
    "                    ,[1,'http://alexgude.com/blog/software-testing-for-data-science']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vocabularycreator(date1,date2,cutoff_numb,save=False):\n",
    "    word_corpus=set([])\n",
    "    header=list(header_daily)\n",
    "    for date in range(date1,date2):\n",
    "        df=pd.read_csv('data/GDELT_1.0/'+str(date)+'.export.CSV',delimiter='\\t')\n",
    "        df.columns=header\n",
    "        df=df.sort_values('NumMentions', ascending=False)\n",
    "        for i in range(cutoff_numb):\n",
    "            word_corpus=word_corpus.union(set(url_tokenizer(df.iloc[i,-1])))\n",
    "        del df\n",
    "    if save:\n",
    "        print(\"Sorry, I haven't implemented this feature yet\")\n",
    "    return word_corpus\n",
    "\n",
    "def corpuscreator_url(date1,date2,cutoff_numb,save=False):\n",
    "    url_corpus=[]\n",
    "    header=list(header_daily)\n",
    "    for date in range(date1,date2):\n",
    "        df=pd.read_csv('data/GDELT_1.0/'+str(date)+'.export.CSV',delimiter='\\t')\n",
    "        df.columns=header\n",
    "        df=df.sort_values('NumMentions', ascending=False)\n",
    "        url_doc=[]\n",
    "        for i in range(cutoff_numb):\n",
    "            url_doc+=[[df['NumMentions'][i],df.iloc[i,-1]]]\n",
    "        url_corpus+=[url_doc]\n",
    "    if save:\n",
    "        print(\"Sorry, I haven't implemented a saving feature yet\")\n",
    "    return url_corpus"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#the warning\n",
    "#//anaconda/envs/py3k/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2827: DtypeWarning: Columns (9,11) \n",
    "#have mixed types. Specify dtype option on import or set low_memory=False. \n",
    "#if self.run_code(code, result):\n",
    "#comes up because it takes a long time for the csv reader to guess the dtype of the different columns.\n",
    "# It would be best to specify types with a dictionary instead {'column name':int, 'other column name':float} etc.\n",
    "\n",
    "#this is not really used\n",
    "vocabulary=vocabularycreator(20130401,20130431,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py3k/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (9,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "//anaconda/envs/py3k/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (8,26,27,28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "//anaconda/envs/py3k/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (8,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "//anaconda/envs/py3k/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (10,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "//anaconda/envs/py3k/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (8,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "//anaconda/envs/py3k/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (8,9,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "//anaconda/envs/py3k/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (8,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "//anaconda/envs/py3k/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "//anaconda/envs/py3k/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (8,11,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "//anaconda/envs/py3k/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "//anaconda/envs/py3k/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (8,9,10,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "//anaconda/envs/py3k/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (9,11,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "//anaconda/envs/py3k/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "//anaconda/envs/py3k/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (9,10,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "//anaconda/envs/py3k/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (8,9,11,21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "//anaconda/envs/py3k/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "//anaconda/envs/py3k/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (10,11,14,20,21,23,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "//anaconda/envs/py3k/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (8,9,11,13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "//anaconda/envs/py3k/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (8,9,11,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "//anaconda/envs/py3k/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (8,9,10,11,18,21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "//anaconda/envs/py3k/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (8,9,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "//anaconda/envs/py3k/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (8,9,10,11,21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "//anaconda/envs/py3k/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (8,9,11,18,21,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "//anaconda/envs/py3k/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (8,9,10,11,13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "//anaconda/envs/py3k/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (8,9,11,14,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "//anaconda/envs/py3k/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (8,9,10,11,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "//anaconda/envs/py3k/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (8,9,10,11,12,13,14,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "//anaconda/envs/py3k/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (8,9,11,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "//anaconda/envs/py3k/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (8,9,10,11,13,14,18,19,20,21,23,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "//anaconda/envs/py3k/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (8,9,10,11,12,13,14,19,26,27,28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "//anaconda/envs/py3k/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (8,9,11,14,26,27,28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "//anaconda/envs/py3k/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (8,9,10,11,12,13,14,19,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "//anaconda/envs/py3k/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (8,9,10,11,14,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "//anaconda/envs/py3k/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (8,9,10,11,14,21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "//anaconda/envs/py3k/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (8,9,10,11,12,13,14,18,20,21,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "//anaconda/envs/py3k/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (8,9,10,11,14,19,21,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "//anaconda/envs/py3k/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (8,9,10,11,12,13,14,19,21,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "#creating the corpus of urls (and number of mentions) by reading over all csv files, takes a while, not very efficient\n",
    "corpus_url=corpuscreator_url(20130401,20130431,100)+corpuscreator_url(20130501,20130532,100)+corpuscreator_url(20130601,20130631,100)+corpuscreator_url(20130701,20130732,100)+corpuscreator_url(20130801,20130832,100)+corpuscreator_url(20130901,20130931,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py3k/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (8,9,10,11,12,13,14,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "//anaconda/envs/py3k/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (8,9,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "//anaconda/envs/py3k/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (9,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "//anaconda/envs/py3k/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (8,9,11,14,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "//anaconda/envs/py3k/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (8,9,10,11,13,14,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "//anaconda/envs/py3k/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (8,9,11,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "//anaconda/envs/py3k/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (8,9,10,11,12,13,14,19,26,27,28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "//anaconda/envs/py3k/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (8,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "//anaconda/envs/py3k/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (8,9,11,26,27,28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "//anaconda/envs/py3k/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (8,9,10,11,12,13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "//anaconda/envs/py3k/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (8,9,11,13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "//anaconda/envs/py3k/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (8,9,10,11,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "corpus_url+=corpuscreator_url(20131001,20131032,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#other features that I intend to use, but right now I'm just only using URLs\n",
    "feat_columns=['FractionDate','Actor1Code','Actor1Name','Actor1CountryCode','Actor1Type1Code','Actor2Code',\n",
    "              'Actor2Name','Actor2CountryCode','Actor2Type1Code','EventCode','QuadClass','GoldsteinScale',\n",
    "              'NumMentions','AvgTone']\n",
    "#out of which, categorical are\n",
    "cat_columns=['Actor1Code','Actor1Name','Actor1CountryCode','Actor1Type1Code','Actor2Code','Actor2Name',\n",
    "             'Actor2CountryCode','Actor2Type1Code','EventCode','QuadClass']\n",
    "\n",
    "\n",
    "#def preprocess(date,corp,cutoff_numb,fcol=feat_columns,ccol=cat_columns,tfidf=False):\n",
    "#    df=pd.read_csv('data/GDELT_1.0/'+str(date)+'.export.CSV',delimiter='\\t')\n",
    "#    df.columns=list(header_daily)\n",
    "#    df=(df.sort_values('NumMentions', ascending=False))[0:cutoff_numb]\n",
    "#    df_with_dummies = pd.get_dummies(df[fcol],columns=ccol)\n",
    "#    if tfidf:\n",
    "#        vectorizer = TfidfVectorizer(min_df=1,tokenizer=url_tokenizer)\n",
    "#    else:\n",
    "#        vectorizer = CountVectorizer(min_df=1,tokenizer=url_tokenizer)\n",
    "#    X = vectorizer.fit_transform(corp)\n",
    "#    Y=X.toarray()\n",
    "#    for i,col in enumerate(vectorizer.get_feature_names()):\n",
    "#        df_with_dummies[col]=pd.DataFrame(Y[:,i])\n",
    "#    return df_with_dummies\n",
    "\n",
    "#this is all about preprocessing the lists of words and vectorize them, possibly applying tfidf\n",
    "\n",
    "def preprocess_red(corp,tfidf=False):\n",
    "    if tfidf:\n",
    "        vectorizer = TfidfVectorizer(min_df=1,tokenizer=wrapper_tokenizer,lowercase=False)\n",
    "    else:\n",
    "        vectorizer = CountVectorizer(min_df=1,tokenizer=wrapper_tokenizer,lowercase=False)\n",
    "    X = vectorizer.fit_transform(corp)\n",
    "    Y=X.toarray()\n",
    "    dictionary={col:Y[:,i] for i,col in enumerate(vectorizer.get_feature_names())}\n",
    "    return pd.DataFrame(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15509"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datagdelt.url_corpus[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-af17a21ca238>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtfidf_dataset_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreprocess_red\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatagdelt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl_corpus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-9ab17baa84ca>\u001b[0m in \u001b[0;36mpreprocess_red\u001b[0;34m(corp, tfidf)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwrapper_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mY\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mdictionary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py3k/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1350\u001b[0m             \u001b[0mTf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0midf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m         \"\"\"\n\u001b[0;32m-> 1352\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py3k/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 839\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py3k/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py3k/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 241\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-620dfc30d07c>\u001b[0m in \u001b[0;36mwrapper_tokenizer\u001b[0;34m(url_doc)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0murl_doc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmentions\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mwordlist\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0murl_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwordlist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-620dfc30d07c>\u001b[0m in \u001b[0;36murl_tokenizer\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mc\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tfidf_dataset_df=preprocess_red(datagdelt.url_corpus,tfidf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#these are the feature dataframes, they contain bag of words or tf-idf vectorization of every single document\n",
    "#(e.g. one full day of news)\n",
    "bow_dataset_df=preprocess_red(corpus_url)\n",
    "tfidf_dataset_df=preprocess_red(corpus_url,tfidf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1013,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aab</th>\n",
       "      <th>aacbec</th>\n",
       "      <th>aacd</th>\n",
       "      <th>aad</th>\n",
       "      <th>aada</th>\n",
       "      <th>aadb</th>\n",
       "      <th>aae</th>\n",
       "      <th>aaf</th>\n",
       "      <th>aafeefc</th>\n",
       "      <th>aaff</th>\n",
       "      <th>...</th>\n",
       "      <th>zimvwodawr</th>\n",
       "      <th>zipwir</th>\n",
       "      <th>zmsm</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zookeep</th>\n",
       "      <th>zs</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>zuma</th>\n",
       "      <th>zzg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 6868 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aab  aacbec  aacd  aad  aada  aadb  aae  aaf  aafeefc  aaff ...   \\\n",
       "0    0       0     0    0     0     0    0    0        0     0 ...    \n",
       "1    0       0     0    0     0     0    0    0        0     0 ...    \n",
       "2    0       0     0    0     0     0    0    0        0     0 ...    \n",
       "3    0       0     0    0     0     0    0    0        0     0 ...    \n",
       "4    0       0     0    0     0     0    0    0        0     0 ...    \n",
       "\n",
       "   zimvwodawr  zipwir  zmsm  zone  zoo  zookeep  zs  zuckerberg  zuma  zzg  \n",
       "0           0       0     0     0    0        0   0           0     0    0  \n",
       "1           0       0     0     0    0        0   0           0     0    0  \n",
       "2           0       0     0     0    0        0   0           0     0    0  \n",
       "3           0       0     0     0    0        0   0           0    10    0  \n",
       "4           0       0     0     0    0        0   0           0     0    0  \n",
       "\n",
       "[5 rows x 6868 columns]"
      ]
     },
     "execution_count": 1013,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1020,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word #char #uniquechar #vowels #conson\n",
      "aab 3 2 2 1\n",
      "aacbec 6 4 3 3\n",
      "aacd 4 3 2 2\n",
      "aad 3 2 2 1\n",
      "aada 4 2 3 1\n",
      "aadb 4 3 2 2\n",
      "aae 3 2 3 0\n",
      "aaf 3 2 2 1\n",
      "aafeefc 7 4 4 3\n",
      "aaff 4 2 2 2\n",
      "aaron 5 4 3 2\n",
      "aarriv 6 4 3 3\n",
      "ab 2 2 1 1\n",
      "abandon 7 5 3 4\n",
      "abb 3 2 1 2\n",
      "abba 4 2 2 2\n",
      "abc 3 3 1 2\n",
      "abccb 5 3 1 4\n",
      "abcf 4 4 1 3\n",
      "abcfa 5 4 2 3\n",
      "abd 3 3 1 2\n",
      "abdic 5 5 2 3\n",
      "abdoul 6 6 3 3\n",
      "abduct 6 6 2 4\n",
      "abductor 8 8 3 5\n",
      "abe 3 3 2 1\n",
      "abeb 4 3 2 2\n",
      "abedf 5 5 2 3\n",
      "abf 3 3 1 2\n",
      "abfbd 5 4 1 4\n",
      "abfff 5 3 1 4\n",
      "abid 4 4 2 2\n",
      "abil 4 4 2 2\n",
      "abl 3 3 1 2\n",
      "ablyazov 8 7 4 4\n",
      "aboard 6 5 3 3\n",
      "abort 5 5 2 3\n",
      "abound 6 6 3 3\n",
      "abramson 8 7 3 5\n",
      "abrio 5 5 3 2\n",
      "abroad 6 5 3 3\n",
      "abu 3 3 2 1\n",
      "abus 4 4 2 2\n",
      "abuzz 5 4 2 3\n",
      "abyei 5 5 4 1\n",
      "ac 2 2 1 1\n",
      "aca 3 2 2 1\n",
      "acabd 5 4 2 3\n",
      "academi 7 6 4 3\n",
      "acapulco 8 6 4 4\n",
      "acb 3 3 1 2\n",
      "acbb 4 3 1 3\n",
      "acbd 4 4 1 3\n",
      "acbecefb 8 5 3 5\n",
      "acbff 5 4 1 4\n",
      "acc 3 2 1 2\n",
      "accbcdd 7 4 1 6\n",
      "acceler 7 5 3 4\n",
      "accept 6 5 2 4\n",
      "access 6 4 2 4\n",
      "accid 5 4 2 3\n",
      "accident 8 7 3 5\n",
      "accolad 7 5 3 4\n",
      "accord 6 5 2 4\n",
      "account 7 6 3 4\n",
      "accredit 8 7 3 5\n",
      "accus 5 4 2 3\n",
      "ace 3 3 2 1\n",
      "acea 4 3 3 1\n",
      "acetaminophen 13 10 6 7\n",
      "acf 3 3 1 2\n",
      "acfba 5 4 2 3\n",
      "acfcb 5 4 1 4\n",
      "ach 3 3 1 2\n",
      "acid 4 4 2 2\n",
      "acknowledg 10 10 3 7\n",
      "acquisit 8 7 4 4\n",
      "acquit 6 6 3 3\n",
      "acquitt 7 6 3 4\n",
      "across 6 5 2 4\n",
      "act 3 3 1 2\n",
      "action 6 6 3 3\n",
      "activ 5 5 2 3\n",
      "activist 8 6 3 5\n",
      "actor 5 5 2 3\n",
      "actress 7 6 2 5\n",
      "acus 4 4 2 2\n",
      "ad 2 2 1 1\n",
      "ada 3 2 2 1\n",
      "adaccf 6 4 2 4\n",
      "adafebc 7 6 3 4\n",
      "adapt 5 4 2 3\n",
      "adawiya 7 5 5 2\n",
      "adb 3 3 1 2\n",
      "adc 3 3 1 2\n",
      "adca 4 3 2 2\n",
      "add 3 2 1 2\n",
      "addc 4 3 1 3\n",
      "addec 5 4 2 3\n",
      "address 7 5 2 5\n",
      "ade 3 3 2 1\n",
      "adebolajo 9 7 5 4\n",
      "adeedcee 8 4 5 3\n",
      "adf 3 3 1 2\n",
      "adfcdac 7 4 2 5\n",
      "adjourn 7 7 3 4\n",
      "adli 4 4 2 2\n",
      "adm 3 3 1 2\n",
      "admin 5 5 2 3\n",
      "administr 9 8 3 6\n",
      "admit 5 5 2 3\n",
      "adopt 5 5 2 3\n",
      "adult 5 5 2 3\n",
      "adulteri 8 8 4 4\n",
      "advanc 6 5 2 4\n",
      "advantag 8 6 3 5\n",
      "adventur 8 8 3 5\n",
      "advert 6 6 2 4\n",
      "advertis 8 8 3 5\n",
      "advis 5 5 2 3\n",
      "advisori 8 7 4 4\n",
      "advoc 5 5 2 3\n",
      "advocaci 8 6 4 4\n",
      "ae 2 2 2 0\n",
      "aea 3 2 3 0\n",
      "aeb 3 3 2 1\n",
      "aebad 5 4 3 2\n",
      "aec 3 3 2 1\n",
      "aecb 4 4 2 2\n",
      "aecdbd 6 5 2 4\n",
      "aef 3 3 2 1\n",
      "af 2 2 1 1\n",
      "afa 3 2 2 1\n",
      "afabbac 7 4 3 4\n",
      "afb 3 3 1 2\n",
      "afc 3 3 1 2\n",
      "afd 3 3 1 2\n",
      "afdb 4 4 1 3\n",
      "afe 3 3 2 1\n",
      "afebc 5 5 2 3\n",
      "afeea 5 3 4 1\n",
      "aff 3 2 1 2\n",
      "affair 6 4 3 3\n",
      "affddbeac 9 6 3 6\n",
      "affect 6 5 2 4\n",
      "affili 6 4 3 3\n",
      "affirm 6 5 2 4\n",
      "afford 6 5 2 4\n",
      "afgan 5 4 2 3\n",
      "afghan 6 5 2 4\n",
      "afghanistan 11 8 4 7\n",
      "afp 3 3 1 2\n",
      "africa 6 5 3 3\n",
      "african 7 6 3 4\n",
      "afternoon 9 7 4 5\n",
      "aftphubbk 9 8 2 7\n",
      "ag 2 2 1 1\n",
      "age 3 3 2 1\n",
      "agenc 5 5 2 3\n",
      "agenda 6 5 3 3\n",
      "agent 5 5 2 3\n",
      "aggrav 6 4 2 4\n",
      "aggress 7 5 2 5\n",
      "agm 3 3 1 2\n",
      "ago 3 3 2 1\n",
      "agon 4 4 2 2\n",
      "agoni 5 5 3 2\n",
      "agre 4 4 2 2\n",
      "agreement 9 7 4 5\n",
      "agricultur 10 8 4 6\n",
      "agrochem 8 8 3 5\n",
      "ahead 5 4 3 2\n",
      "ahmadinejad 11 8 5 6\n",
      "aid 3 3 2 1\n",
      "aig 3 3 2 1\n",
      "ail 3 3 2 1\n",
      "aim 3 3 2 1\n",
      "air 3 3 2 1\n",
      "airbu 5 5 3 2\n",
      "aircraft 8 6 3 5\n",
      "airlin 6 5 3 3\n",
      "airplan 7 6 3 4\n",
      "airport 7 6 3 4\n",
      "airstrik 8 6 3 5\n",
      "airway 6 5 4 2\n",
      "aisl 4 4 2 2\n",
      "aj 2 2 1 1\n",
      "ak 2 2 1 1\n",
      "aka 3 2 2 1\n",
      "akbar 5 4 2 3\n",
      "akin 4 4 2 2\n",
      "al 2 2 1 1\n",
      "alaa 4 2 3 1\n",
      "alabama 7 4 4 3\n",
      "alamo 5 4 3 2\n",
      "alarm 5 4 2 3\n",
      "alaska 6 4 3 3\n",
      "alawit 6 5 3 3\n",
      "albania 7 5 4 3\n",
      "albanian 8 5 4 4\n",
      "albert 6 6 2 4\n",
      "alberta 7 6 3 4\n",
      "album 5 5 2 3\n",
      "albuquerqu 10 7 5 5\n",
      "alcatelluc 10 6 4 6\n",
      "alcohol 7 5 3 4\n",
      "alec 4 4 2 2\n",
      "aleppo 6 5 3 3\n",
      "aleqmiggmm 10 7 3 7\n",
      "alert 5 5 2 3\n",
      "aleutian 8 7 5 3\n",
      "alex 4 4 2 2\n",
      "alexand 7 6 3 4\n",
      "alexanderbrep 13 9 5 8\n",
      "alexandria 10 8 5 5\n",
      "alexi 5 5 3 2\n",
      "alexian 7 6 4 3\n",
      "algeria 7 6 4 3\n",
      "ali 3 3 2 1\n",
      "alibaba 7 4 4 3\n",
      "alibhai 7 5 4 3\n",
      "align 5 5 2 3\n",
      "aliv 4 4 2 2\n",
      "aljazeera 9 6 5 4\n",
      "allahu 6 4 3 3\n",
      "alleg 5 4 2 3\n",
      "allegedli 9 6 4 5\n",
      "allen 5 4 2 3\n",
      "allergi 7 6 3 4\n",
      "alli 4 3 2 2\n",
      "allianc 7 5 3 4\n",
      "alloc 5 4 2 3\n",
      "allow 5 4 2 3\n",
      "almanac 7 5 3 4\n",
      "almost 6 6 2 4\n",
      "alon 4 4 2 2\n",
      "alphabet 8 7 3 5\n",
      "alqaida 7 5 4 3\n",
      "alreadi 7 6 4 3\n",
      "alright 7 7 2 5\n",
      "also 4 4 2 2\n",
      "alter 5 5 2 3\n",
      "alto 4 4 2 2\n",
      "alumni 6 6 3 3\n",
      "alway 5 4 3 2\n",
      "alzheim 7 7 3 4\n",
      "amanda 6 4 3 3\n",
      "amarillo 8 6 4 4\n",
      "amazon 6 5 3 3\n",
      "amb 3 3 1 2\n",
      "ambassador 10 7 4 6\n",
      "amber 5 5 2 3\n",
      "ambit 5 5 2 3\n",
      "ambush 6 6 2 4\n",
      "amd 3 3 1 2\n",
      "amend 5 5 2 3\n",
      "america 7 6 4 3\n",
      "american 8 7 4 4\n",
      "amgen 5 5 2 3\n",
      "amid 4 4 2 2\n",
      "ammo 4 3 2 2\n",
      "ammunit 7 6 3 4\n",
      "amnesti 7 7 3 4\n",
      "amoeba 6 5 4 2\n",
      "among 5 5 2 3\n",
      "amr 3 3 1 2\n",
      "amtrak 6 5 2 4\n",
      "ana 3 2 2 1\n",
      "analysi 7 6 4 3\n",
      "analyst 7 6 3 4\n",
      "analyt 6 5 3 3\n",
      "andean 6 4 3 3\n",
      "anderson 8 7 3 5\n",
      "andi 4 4 2 2\n",
      "andrew 6 6 2 4\n",
      "android 7 6 3 4\n",
      "angel 5 5 2 3\n",
      "angelina 8 6 4 4\n",
      "anger 5 5 2 3\n",
      "angl 4 4 1 3\n",
      "angri 5 5 2 3\n",
      "angst 5 5 1 4\n",
      "anguish 7 7 3 4\n",
      "anim 4 4 2 2\n",
      "ankl 4 4 1 3\n",
      "ann 3 2 1 2\n",
      "annapoli 8 6 4 4\n",
      "anniversari 11 7 5 6\n",
      "announc 7 5 3 4\n",
      "annual 6 4 3 3\n",
      "anoth 5 5 2 3\n",
      "anp 3 3 1 2\n",
      "ansar 5 4 2 3\n",
      "answer 6 6 2 4\n",
      "antarct 7 5 2 5\n",
      "anthoni 7 6 3 4\n",
      "anti 4 4 2 2\n",
      "anticip 7 6 3 4\n",
      "antigay 7 6 4 3\n",
      "antiq 5 5 2 3\n",
      "antiqu 6 6 3 3\n",
      "antitrust 9 7 3 6\n",
      "antiu 5 5 3 2\n",
      "antoin 6 5 3 3\n",
      "anyon 5 4 3 2\n",
      "anyth 5 5 2 3\n",
      "aol 3 3 2 1\n",
      "ap 2 2 1 1\n",
      "apart 5 4 2 3\n",
      "apartheid 9 8 4 5\n",
      "apnewsbreak 11 9 4 7\n",
      "apo 3 3 2 1\n",
      "apolog 6 5 3 3\n",
      "apologis 8 7 4 4\n",
      "app 3 2 1 2\n",
      "appar 5 3 2 3\n",
      "appeal 6 4 3 3\n",
      "appear 6 4 3 3\n",
      "appl 4 3 1 3\n",
      "applaud 7 5 3 4\n",
      "applic 6 5 2 4\n",
      "appoint 7 6 3 4\n",
      "appointe 8 7 4 4\n",
      "approach 8 6 3 5\n",
      "approv 6 5 2 4\n",
      "april 5 5 2 3\n",
      "apsg 4 4 1 3\n",
      "arab 4 3 2 2\n",
      "arabia 6 4 4 2\n",
      "arabian 7 5 4 3\n",
      "arak 4 3 2 2\n",
      "aramyan 7 5 4 3\n",
      "arbitr 6 5 2 4\n",
      "arcelormitt 11 9 4 7\n",
      "arctic 6 5 2 4\n",
      "ardrossan 9 6 3 6\n",
      "area 4 3 3 1\n",
      "argentin 8 7 3 5\n",
      "argentina 9 7 4 5\n",
      "argentinian 11 7 5 6\n",
      "argo 4 4 2 2\n",
      "argu 4 4 2 2\n",
      "argument 8 8 3 5\n",
      "aria 4 3 3 1\n",
      "ariel 5 5 3 2\n",
      "ariz 4 4 2 2\n",
      "arizona 7 6 4 3\n",
      "ark 3 3 1 2\n",
      "arkansa 7 5 3 4\n",
      "arm 3 3 1 2\n",
      "armenia 7 6 4 3\n",
      "armenian 8 6 4 4\n",
      "armi 4 4 2 2\n",
      "armistic 8 7 3 5\n",
      "armstrong 9 8 2 7\n",
      "around 6 6 3 3\n",
      "arraign 7 5 3 4\n",
      "arrest 6 5 2 4\n",
      "arreste 7 5 3 4\n",
      "arriv 5 4 2 3\n",
      "arrog 5 4 2 3\n",
      "arrow 5 4 2 3\n",
      "arsen 5 5 2 3\n",
      "art 3 3 1 2\n",
      "artefact 8 6 3 5\n",
      "artesia 7 6 4 3\n",
      "articl 6 6 2 4\n",
      "artifici 8 6 4 4\n",
      "artist 6 5 2 4\n",
      "artsakh 7 6 2 5\n",
      "artwork 7 6 2 5\n",
      "asco 4 4 2 2\n",
      "asda 4 3 2 2\n",
      "asean 5 4 3 2\n",
      "ashor 5 5 2 3\n",
      "ashton 6 6 2 4\n",
      "ashwani 7 6 3 4\n",
      "asia 4 3 3 1\n",
      "asian 5 4 3 2\n",
      "asiana 6 4 4 2\n",
      "asid 4 4 2 2\n",
      "ask 3 3 1 2\n",
      "aso 3 3 2 1\n",
      "assad 5 3 2 3\n",
      "assam 5 3 2 3\n",
      "assang 6 4 2 4\n",
      "assassin 8 4 3 5\n",
      "assault 7 5 3 4\n",
      "assembl 7 6 2 5\n",
      "assess 6 3 2 4\n",
      "asset 5 4 2 3\n",
      "assisi 6 3 3 3\n",
      "associ 6 5 3 3\n",
      "asteroid 8 8 4 4\n",
      "asthma 6 5 2 4\n",
      "astonish 8 7 3 5\n",
      "astronaut 9 7 4 5\n",
      "aswani 6 5 3 3\n",
      "asylum 6 6 3 3\n",
      "atchison 8 8 3 5\n",
      "athcpart 8 6 2 6\n",
      "athen 5 5 2 3\n",
      "atlant 6 4 2 4\n",
      "atlanta 7 4 3 4\n",
      "atm 3 3 1 2\n",
      "atmospher 9 9 3 6\n",
      "atom 4 4 2 2\n",
      "atsn 4 4 1 3\n",
      "att 3 2 1 2\n",
      "attack 6 4 2 4\n",
      "attackbref 10 8 3 7\n",
      "attackbrem 10 8 3 7\n",
      "attempt 7 5 2 5\n",
      "attend 6 5 2 4\n",
      "attent 6 4 2 4\n",
      "attic 5 4 2 3\n",
      "attica 6 4 3 3\n",
      "attorney 8 7 4 4\n",
      "au 2 2 2 0\n",
      "auckland 8 7 3 5\n",
      "auction 7 7 4 3\n",
      "audit 5 5 3 2\n",
      "auditor 7 7 4 3\n",
      "aug 3 3 2 1\n",
      "august 6 5 3 3\n",
      "augusta 7 5 4 3\n",
      "aumf 4 4 2 2\n",
      "aurora 6 4 4 2\n",
      "aussi 5 4 3 2\n",
      "auster 6 6 3 3\n",
      "australia 9 7 5 4\n",
      "australian 10 8 5 5\n",
      "austria 7 6 4 3\n",
      "austrian 8 7 4 4\n",
      "author 6 6 3 3\n",
      "autism 6 6 3 3\n",
      "autist 6 5 3 3\n",
      "auto 4 4 3 1\n",
      "automak 7 6 4 3\n",
      "automat 7 5 4 3\n",
      "automatt 8 5 4 4\n",
      "autonomouscar 13 9 7 6\n",
      "autumn 6 5 3 3\n",
      "avail 5 4 3 2\n",
      "ave 3 3 2 1\n",
      "avenu 5 5 3 2\n",
      "avert 5 5 2 3\n",
      "aviat 5 4 3 2\n",
      "avoid 5 5 3 2\n",
      "avon 4 4 2 2\n",
      "aw 2 2 1 1\n",
      "await 5 4 3 2\n",
      "awak 4 3 2 2\n",
      "awar 4 3 2 2\n",
      "award 5 4 2 3\n",
      "away 4 3 3 1\n",
      "awwmypzwaz 10 6 3 7\n",
      "axe 3 3 2 1\n",
      "axfpmqjuutn 11 10 3 8\n",
      "aydogan 7 6 4 3\n",
      "ayvani 6 5 4 2\n",
      "azerbaijan 10 8 5 5\n",
      "azerbaijani 11 8 6 5\n",
      "azhar 5 4 2 3\n",
      "ba 2 2 1 1\n",
      "baa 3 2 2 1\n",
      "bab 3 2 1 2\n",
      "babi 4 3 2 2\n",
      "babysit 7 6 3 4\n",
      "bac 3 3 1 2\n",
      "back 4 4 1 3\n",
      "backdrop 8 8 2 6\n",
      "backer 6 6 2 4\n",
      "backfir 7 7 2 5\n",
      "background 10 10 3 7\n",
      "backlash 8 7 2 6\n",
      "backlog 7 7 2 5\n",
      "backpack 8 5 2 6\n",
      "bacon 5 5 2 3\n",
      "bad 3 3 1 2\n",
      "badea 5 4 3 2\n",
      "badedcbfa 9 6 3 6\n",
      "badli 5 5 2 3\n",
      "bae 3 3 2 1\n",
      "baeddca 7 5 3 4\n",
      "baf 3 3 1 2\n",
      "bafaa 5 3 3 2\n",
      "bafc 4 4 1 3\n",
      "baff 4 3 1 3\n",
      "bag 3 3 1 2\n",
      "baghdad 7 5 2 5\n",
      "bahrain 7 6 3 4\n",
      "bail 4 4 2 2\n",
      "bailout 7 7 4 3\n",
      "baincapit 9 7 4 5\n",
      "bake 4 4 2 2\n",
      "bakeri 6 6 3 3\n",
      "bakken 6 5 2 4\n",
      "bako 4 4 2 2\n",
      "balanc 6 5 2 4\n",
      "bale 4 4 2 2\n",
      "bali 4 4 2 2\n",
      "ball 4 3 1 3\n",
      "balloon 7 5 3 4\n",
      "ballot 6 5 2 4\n",
      "balochistan 11 10 4 7\n",
      "baltimor 8 8 3 5\n",
      "ban 3 3 1 2\n",
      "band 4 4 1 3\n",
      "banerje 7 6 3 4\n",
      "bangkok 7 6 2 5\n",
      "bangladesh 10 9 3 7\n",
      "bangladeshi 11 10 4 7\n",
      "bangsamoro 10 8 4 6\n",
      "bank 4 4 1 3\n",
      "banker 6 6 2 4\n",
      "bankia 6 5 3 3\n",
      "bankofamerica 13 11 6 7\n",
      "bankrupt 8 8 2 6\n",
      "bankruptci 10 10 3 7\n",
      "banksi 6 6 2 4\n",
      "banner 6 5 2 4\n",
      "bansal 6 5 2 4\n",
      "baptism 7 7 2 5\n",
      "baquba 6 4 3 3\n",
      "bar 3 3 1 2\n",
      "barack 6 5 2 4\n",
      "barajuen 8 7 4 4\n",
      "barangay 8 6 4 4\n",
      "barclay 7 6 3 4\n",
      "barcpart 8 6 2 6\n",
      "barg 4 4 1 3\n",
      "barletta 8 6 3 5\n",
      "barley 6 6 3 3\n",
      "barnett 7 6 2 5\n",
      "barney 6 6 3 3\n",
      "barnier 7 6 3 4\n",
      "barrel 6 5 2 4\n",
      "barrera 7 4 3 4\n",
      "barricad 8 6 3 5\n",
      "barrick 7 6 2 5\n",
      "barroso 7 5 3 4\n",
      "basalt 6 5 2 4\n",
      "base 4 4 2 2\n",
      "basement 8 7 3 5\n",
      "bashir 6 6 2 4\n",
      "basi 4 4 2 2\n",
      "bastion 7 7 3 4\n",
      "bathroom 8 7 3 5\n",
      "baton 5 5 2 3\n",
      "batter 6 5 2 4\n",
      "batteri 7 6 3 4\n",
      "battl 5 4 1 4\n",
      "battlefield 11 8 4 7\n",
      "baucu 5 4 3 2\n",
      "bay 3 3 2 1\n",
      "bba 3 2 1 2\n",
      "bbade 5 4 2 3\n",
      "bbae 4 3 2 2\n",
      "bbca 4 3 1 3\n",
      "bbcaec 6 4 2 4\n",
      "bbdaecdec 9 5 3 6\n",
      "bbe 3 2 1 2\n",
      "bbea 4 3 2 2\n",
      "bbeafd 6 5 2 4\n",
      "bbfa 4 3 1 3\n",
      "bbfadbf 7 4 1 6\n",
      "bbfe 4 3 1 3\n",
      "bbva 4 3 1 3\n",
      "bca 3 3 1 2\n",
      "bcaebfb 7 5 2 5\n",
      "bcbec 5 3 1 4\n",
      "bcde 4 4 1 3\n",
      "bce 3 3 1 2\n",
      "bcfe 4 4 1 3\n",
      "bda 3 3 1 2\n",
      "bdadb 5 3 1 4\n",
      "bdbbfa 6 4 1 5\n",
      "bddcaba 7 4 2 5\n",
      "bde 3 3 1 2\n",
      "bdeaba 6 4 3 3\n",
      "bdeaf 5 5 2 3\n",
      "bdebf 5 4 1 4\n",
      "bdec 4 4 1 3\n",
      "bea 3 3 2 1\n",
      "beach 5 5 2 3\n",
      "beacon 6 6 3 3\n",
      "bear 4 4 2 2\n",
      "beat 4 4 2 2\n",
      "beaten 6 5 3 3\n",
      "beau 4 4 3 1\n",
      "beauti 6 6 4 2\n",
      "beaver 6 5 3 3\n",
      "beb 3 2 1 2\n",
      "bebdda 6 4 2 4\n",
      "bebec 5 3 2 3\n",
      "beblawi 7 6 3 4\n",
      "becam 5 5 2 3\n",
      "becc 4 3 1 3\n",
      "bece 4 3 2 2\n",
      "becom 5 5 2 3\n",
      "bed 3 3 1 2\n",
      "bee 3 2 2 1\n",
      "beec 4 3 2 2\n",
      "beef 4 3 2 2\n",
      "beefb 5 3 2 3\n",
      "beer 4 3 2 2\n",
      "bef 3 3 1 2\n",
      "befabd 6 5 2 4\n",
      "befbb 5 3 1 4\n",
      "began 5 5 2 3\n",
      "begin 5 5 2 3\n",
      "behavior 8 8 4 4\n",
      "behead 6 5 3 3\n",
      "behind 6 6 2 4\n",
      "beij 4 4 2 2\n",
      "beirut 6 6 3 3\n",
      "belaru 6 6 3 3\n",
      "belen 5 4 2 3\n",
      "belgian 7 7 3 4\n",
      "belgium 7 7 3 4\n",
      "beli 4 4 2 2\n",
      "believ 6 5 3 3\n",
      "bell 4 3 1 3\n",
      "belong 6 6 2 4\n",
      "beltagi 7 7 3 4\n",
      "benazir 7 7 3 4\n",
      "bench 5 5 1 4\n",
      "benefit 7 6 3 4\n",
      "benetton 8 5 3 5\n",
      "benghazi 8 8 3 5\n",
      "berkshir 8 7 2 6\n",
      "berlin 6 6 2 4\n",
      "berlusconi 10 10 4 6\n",
      "bernardino 10 8 4 6\n",
      "berri 5 4 2 3\n",
      "beset 5 4 2 3\n",
      "besieg 6 5 3 3\n",
      "best 4 4 1 3\n",
      "bestbuy 7 6 3 4\n",
      "bet 3 3 1 2\n",
      "beth 4 4 1 3\n",
      "betray 6 6 3 3\n",
      "better 6 4 2 4\n",
      "bewild 6 6 2 4\n",
      "beyonc 6 6 3 3\n",
      "beyond 6 6 3 3\n",
      "bezo 4 4 2 2\n",
      "bfa 3 3 1 2\n",
      "bfac 4 4 1 3\n",
      "bfbda 5 4 1 4\n",
      "bfeda 5 5 2 3\n",
      "bffe 4 3 1 3\n",
      "bfo 3 3 1 2\n",
      "bharti 6 6 2 4\n",
      "bhutto 6 5 2 4\n",
      "bia 3 3 2 1\n",
      "bialik 6 5 3 3\n",
      "bid 3 3 1 2\n",
      "biden 5 5 2 3\n",
      "big 3 3 1 2\n",
      "bigger 6 5 2 4\n",
      "biggest 7 6 2 5\n",
      "bike 4 4 2 2\n",
      "biker 5 5 2 3\n",
      "bill 4 3 1 3\n",
      "billion 7 5 3 4\n",
      "billionair 10 7 5 5\n",
      "bin 3 3 1 2\n",
      "bio 3 3 2 1\n",
      "biofuel 7 7 4 3\n",
      "biomass 7 6 3 4\n",
      "biopsi 6 5 3 3\n",
      "biotechnolog 12 10 5 7\n",
      "bipartisan 10 8 4 6\n",
      "bird 4 4 1 3\n",
      "birdflu 7 7 2 5\n",
      "birmingham 10 8 3 7\n",
      "birth 5 5 1 4\n",
      "birthday 8 8 3 5\n",
      "bishop 6 6 2 4\n",
      "biskup 6 6 2 4\n",
      "bison 5 5 2 3\n",
      "bite 4 4 2 2\n",
      "bitter 6 5 2 4\n",
      "bittersweet 11 7 4 7\n",
      "biz 3 3 1 2\n",
      "bizarr 6 5 2 4\n",
      "bjwnpsgea 9 9 2 7\n",
      "black 5 5 1 4\n",
      "blackberri 10 8 3 7\n",
      "blackfac 8 6 2 6\n",
      "blacklist 9 8 2 7\n",
      "blackston 9 9 2 7\n",
      "blackwat 8 7 2 6\n",
      "blake 5 5 2 3\n",
      "blakey 6 6 3 3\n",
      "blame 5 5 2 3\n",
      "blasio 6 6 3 3\n",
      "blasphemi 9 9 3 6\n",
      "blast 5 5 1 4\n",
      "blaze 5 5 2 3\n",
      "bleak 5 5 2 3\n",
      "bleed 5 4 2 3\n",
      "blend 5 5 1 4\n",
      "bless 5 4 1 4\n",
      "blind 5 5 1 4\n",
      "blink 5 5 1 4\n",
      "block 5 5 1 4\n",
      "blockupi 8 8 3 5\n",
      "blog 4 4 1 3\n",
      "blogger 7 6 2 5\n",
      "blond 5 5 1 4\n",
      "blood 5 4 2 3\n",
      "bloodbath 9 7 3 6\n",
      "bloodhound 10 7 4 6\n",
      "bloodi 6 5 3 3\n",
      "bloodsh 7 6 2 5\n",
      "bloomberg 9 7 3 6\n",
      "blow 4 4 1 3\n",
      "blowout 7 6 3 4\n",
      "blueberri 9 6 4 5\n",
      "blur 4 4 1 3\n",
      "bnqsqtu 7 6 1 6\n",
      "bnull 5 4 1 4\n",
      "bo 2 2 1 1\n",
      "board 5 5 2 3\n",
      "boardwalk 9 8 3 6\n",
      "boast 5 5 2 3\n",
      "boat 4 4 2 2\n",
      "bob 3 2 1 2\n",
      "boca 4 4 2 2\n",
      "bodi 4 4 2 2\n",
      "boe 3 3 2 1\n",
      "boehner 7 6 3 4\n",
      "boevski 7 7 3 4\n",
      "bofa 4 4 2 2\n",
      "bogu 4 4 2 2\n",
      "boil 4 4 2 2\n",
      "bois 4 4 2 2\n",
      "boko 4 3 2 2\n",
      "bokoharam 9 7 4 5\n",
      "bold 4 4 1 3\n",
      "bolder 6 6 2 4\n",
      "bolivia 7 6 4 3\n",
      "bolivian 8 7 4 4\n",
      "bolster 7 7 2 5\n",
      "bomb 4 3 1 3\n",
      "bomber 6 5 2 4\n",
      "bond 4 4 1 3\n",
      "bonu 4 4 2 2\n",
      "bonus 5 5 2 3\n",
      "boobi 5 3 3 2\n",
      "book 4 3 2 2\n",
      "booker 6 5 3 3\n",
      "boom 4 3 2 2\n",
      "boon 4 3 2 2\n",
      "boost 5 4 2 3\n",
      "boot 4 3 2 2\n",
      "booz 4 3 2 2\n",
      "boozallen 9 7 4 5\n",
      "border 6 5 2 4\n",
      "bore 4 4 2 2\n",
      "borey 5 5 3 2\n",
      "born 4 4 1 3\n",
      "borough 7 6 3 4\n",
      "borrow 6 4 2 4\n",
      "bosco 5 4 2 3\n",
      "bosnia 6 6 3 3\n",
      "boson 5 4 2 3\n",
      "bosphoru 8 7 3 5\n",
      "boss 4 3 1 3\n",
      "boston 6 5 2 4\n",
      "botch 5 5 1 4\n",
      "botox 5 4 2 3\n",
      "bottl 5 4 1 4\n",
      "botul 5 5 2 3\n",
      "boubacar 8 6 4 4\n",
      "boulder 7 7 3 4\n",
      "boulevard 9 9 4 5\n",
      "bound 5 5 2 3\n",
      "bounti 6 6 3 3\n",
      "bout 4 4 2 2\n",
      "bow 3 3 1 2\n",
      "box 3 3 1 2\n",
      "boy 3 3 2 1\n",
      "boycot 6 5 3 3\n",
      "boycott 7 5 3 4\n",
      "boyfriend 9 9 4 5\n",
      "boyscout 8 7 4 4\n",
      "bpa 3 3 1 2\n",
      "bprefineri 10 7 4 6\n",
      "brace 5 5 2 3\n",
      "bracelet 8 7 3 5\n",
      "bradi 5 5 2 3\n",
      "bradley 7 7 3 4\n",
      "brain 5 5 2 3\n",
      "brake 5 5 2 3\n",
      "bramw 5 5 1 4\n",
      "branch 6 6 1 5\n",
      "brand 5 5 1 4\n",
      "brandenburg 11 8 3 8\n",
      "brandon 7 6 2 5\n",
      "branson 7 6 2 5\n",
      "brass 5 4 1 4\n",
      "braszczok 9 8 2 7\n",
      "brave 5 5 2 3\n",
      "brazen 6 6 2 4\n",
      "brazil 6 6 2 4\n",
      "brazilian 9 7 4 5\n",
      "breach 6 6 2 4\n",
      "break 5 5 2 3\n",
      "breakfast 9 8 3 6\n",
      "breakthrough 12 10 4 8\n",
      "breast 6 6 2 4\n",
      "breatharian 11 8 5 6\n",
      "brewer 6 4 2 4\n",
      "bribe 5 4 2 3\n",
      "briberi 7 4 3 4\n",
      "bridg 5 5 1 4\n",
      "brief 5 5 2 3\n",
      "briefli 7 6 3 4\n",
      "bright 6 6 1 5\n",
      "bring 5 5 1 4\n",
      "brink 5 5 1 4\n",
      "brinkley 8 8 3 5\n",
      "bristol 7 7 2 5\n",
      "britain 7 6 3 4\n",
      "british 7 6 2 5\n",
      "briton 6 6 2 4\n",
      "broad 5 5 2 3\n",
      "broadcast 9 8 3 6\n",
      "broader 7 6 3 4\n",
      "broadway 8 7 4 4\n",
      "broke 5 5 2 3\n",
      "broken 6 6 2 4\n",
      "broker 6 5 2 4\n",
      "brooklyn 8 7 3 5\n",
      "brother 7 6 2 5\n",
      "brotherhood 11 7 4 7\n",
      "brought 7 7 2 5\n",
      "broward 7 6 2 5\n",
      "browder 7 6 2 5\n",
      "brown 5 5 1 4\n",
      "brunch 6 6 1 5\n",
      "brunt 5 5 1 4\n",
      "brush 5 5 1 4\n",
      "brussel 7 6 2 5\n",
      "brutal 6 6 2 4\n",
      "brute 5 5 2 3\n",
      "bryan 5 5 2 3\n",
      "bsdyte 6 6 2 4\n",
      "bstarbuck 9 8 2 7\n",
      "bu 2 2 1 1\n",
      "bucket 6 6 2 4\n",
      "buckl 5 5 1 4\n",
      "buckwild 8 8 2 6\n",
      "buddhist 8 7 2 6\n",
      "budget 6 6 2 4\n",
      "buffett 7 5 2 5\n",
      "bug 3 3 1 2\n",
      "build 5 5 2 3\n",
      "builder 7 7 3 4\n",
      "bulacan 7 6 3 4\n",
      "bulgaria 8 7 4 4\n",
      "bulgarian 9 8 4 5\n",
      "bulger 6 6 2 4\n",
      "bullet 6 5 2 4\n",
      "bulli 5 4 2 3\n",
      "bump 4 4 1 3\n",
      "buner 5 5 2 3\n",
      "bunga 5 5 2 3\n",
      "bunker 6 6 2 4\n",
      "bureau 6 5 4 2\n",
      "bureaucraci 11 7 6 5\n",
      "bureaucrat 10 7 5 5\n",
      "burger 6 5 2 4\n",
      "burglar 7 6 2 5\n",
      "burglari 8 7 3 5\n",
      "buri 4 4 2 2\n",
      "burk 4 4 1 3\n",
      "burma 5 5 2 3\n",
      "burn 4 4 1 3\n",
      "burnt 5 5 1 4\n",
      "burst 5 5 1 4\n",
      "bush 4 4 1 3\n",
      "bushehr 7 6 2 5\n",
      "bushfir 7 7 2 5\n",
      "busi 4 4 2 2\n",
      "businessman 11 8 4 7\n",
      "bust 4 4 1 3\n",
      "butina 6 6 3 3\n",
      "buttock 7 6 2 5\n",
      "button 6 5 2 4\n",
      "buy 3 3 2 1\n",
      "buyout 6 5 4 2\n",
      "byfuglien 9 9 4 5\n",
      "byo 3 3 2 1\n",
      "bystand 7 7 2 5\n",
      "ca 2 2 1 1\n",
      "caa 3 2 2 1\n",
      "caae 4 3 3 1\n",
      "cabbi 5 4 2 3\n",
      "cabe 4 4 2 2\n",
      "cabin 5 5 2 3\n",
      "cabinet 7 7 3 4\n",
      "cabl 4 4 1 3\n",
      "cac 3 2 1 2\n",
      "cacc 4 2 1 3\n",
      "cach 4 3 1 3\n",
      "cad 3 3 1 2\n",
      "cadcca 6 3 2 4\n",
      "cadd 4 3 1 3\n",
      "cadet 5 5 2 3\n",
      "cadfc 5 4 1 4\n",
      "cadr 4 4 1 3\n",
      "cae 3 3 2 1\n",
      "caea 4 3 3 1\n",
      "caf 3 3 1 2\n",
      "cafa 4 3 2 2\n",
      "cafe 4 4 2 2\n",
      "cage 4 4 2 2\n",
      "cahqera 7 6 3 4\n",
      "cairo 5 5 3 2\n",
      "calam 5 4 2 3\n",
      "calendar 8 7 3 5\n",
      "calgari 7 6 3 4\n",
      "calif 5 5 2 3\n",
      "california 10 8 5 5\n",
      "californian 11 8 5 6\n",
      "call 4 3 1 3\n",
      "caller 6 5 2 4\n",
      "calm 4 4 1 3\n",
      "calori 6 6 3 3\n",
      "cam 3 3 1 2\n",
      "cambodia 8 7 4 4\n",
      "cambodian 9 8 4 5\n",
      "cambridg 8 8 2 6\n",
      "camera 6 5 3 3\n",
      "cameraman 9 6 4 5\n",
      "cameron 7 7 3 4\n",
      "cameroon 8 7 4 4\n",
      "camp 4 4 1 3\n",
      "campaign 8 7 3 5\n",
      "campbel 7 7 2 5\n",
      "campu 5 5 2 3\n",
      "canada 6 4 3 3\n",
      "canadian 8 5 4 4\n",
      "canal 5 4 2 3\n",
      "cancel 6 5 2 4\n",
      "cancer 6 5 2 4\n",
      "candid 6 5 2 4\n",
      "candlelight 11 10 3 8\n",
      "cann 4 3 1 3\n",
      "cannib 6 5 2 4\n",
      "cannon 6 4 2 4\n",
      "cannot 6 5 2 4\n",
      "cant 4 4 1 3\n",
      "canyon 6 5 3 3\n",
      "cap 3 3 1 2\n",
      "capabl 6 5 2 4\n",
      "capac 5 3 2 3\n",
      "capit 5 5 2 3\n",
      "capitol 7 7 3 4\n",
      "capsiz 6 6 2 4\n",
      "captain 7 6 3 4\n",
      "captcha 7 5 2 5\n",
      "caption 7 7 3 4\n",
      "captiv 6 6 2 4\n",
      "captur 6 6 2 4\n",
      "car 3 3 1 2\n",
      "carbon 6 6 2 4\n",
      "card 4 4 1 3\n",
      "cardin 6 6 2 4\n",
      "cardiovascular 14 10 6 8\n",
      "care 4 4 2 2\n",
      "career 6 4 3 3\n",
      "caregiv 7 7 3 4\n",
      "carey 5 5 3 2\n",
      "cargo 5 5 2 3\n",
      "caribbean 9 7 4 5\n",
      "carl 4 4 1 3\n",
      "carlo 5 5 2 3\n",
      "carniv 6 6 2 4\n",
      "carolin 7 7 3 4\n",
      "carolina 8 7 4 4\n",
      "carri 5 4 2 3\n",
      "carrier 7 5 3 4\n",
      "cartel 6 6 2 4\n",
      "cartersvil 10 9 3 7\n",
      "cartoon 7 6 3 4\n",
      "carv 4 4 1 3\n",
      "cascad 6 4 2 4\n",
      "case 4 4 2 2\n",
      "cash 4 4 1 3\n",
      "casino 6 6 3 3\n",
      "cassava 7 4 3 4\n",
      "cast 4 4 1 3\n",
      "castro 6 6 2 4\n",
      "casualti 8 7 4 4\n",
      "catch 5 4 1 4\n",
      "caterpillar 11 8 4 7\n",
      "cathedr 7 7 2 5\n",
      "cathol 6 6 2 4\n",
      "cattl 5 4 1 4\n",
      "caught 6 6 2 4\n",
      "caus 4 4 2 2\n",
      "caution 7 7 4 3\n",
      "cautiou 7 6 5 2\n",
      "cayle 5 5 3 2\n",
      "cba 3 3 1 2\n",
      "cbacdcea 8 5 3 5\n",
      "cbe 3 3 1 2\n",
      "cbebc 5 3 1 4\n",
      "cbed 4 4 1 3\n",
      "cbeeea 6 4 4 2\n",
      "cbo 3 3 1 2\n",
      "ccabca 6 3 2 4\n",
      "ccdcab 6 4 1 5\n",
      "cceaa 5 3 3 2\n",
      "cddcba 6 4 1 5\n",
      "ce 2 2 1 1\n",
      "cea 3 3 2 1\n",
      "ceaa 4 3 3 1\n",
      "ceacbbf 7 5 2 5\n",
      "ceaeedd 7 4 4 3\n",
      "ceas 4 4 2 2\n",
      "ceasefir 8 7 4 4\n",
      "ceb 3 3 1 2\n",
      "cebeead 7 5 4 3\n",
      "cecdff 6 4 1 5\n",
      "cecil 5 4 2 3\n",
      "ced 3 3 1 2\n",
      "cedcbbf 7 5 1 6\n",
      "cede 4 3 2 2\n",
      "cefa 4 4 2 2\n",
      "ceil 4 4 2 2\n",
      "celebr 6 5 2 4\n",
      "cell 4 3 1 3\n",
      "cellphon 8 7 2 6\n",
      "cemeteri 8 6 4 4\n",
      "cent 4 4 1 3\n",
      "center 6 5 2 4\n",
      "centerra 8 6 3 5\n",
      "centr 5 5 1 4\n",
      "central 7 7 2 5\n",
      "centralafrica 13 9 5 8\n",
      "centraleurop 12 10 5 7\n",
      "centuri 7 7 3 4\n",
      "ceo 3 3 2 1\n",
      "cerberu 7 5 3 4\n",
      "ceremoni 8 7 4 4\n",
      "certifi 7 6 3 4\n",
      "cervic 6 5 2 4\n",
      "cfaf 4 3 1 3\n",
      "cfbaeae 7 5 4 3\n",
      "cfbba 5 4 1 4\n",
      "cfce 4 3 1 3\n",
      "cfeba 5 5 2 3\n",
      "cfedd 5 4 1 4\n",
      "cfee 4 3 2 2\n",
      "cgi 3 3 1 2\n",
      "cgugekixubhul 13 10 5 8\n",
      "chabanova 9 7 4 5\n",
      "chad 4 4 1 3\n",
      "chair 5 5 2 3\n",
      "chairman 8 7 3 5\n",
      "challeng 8 7 2 6\n",
      "chamber 7 7 2 5\n",
      "chan 4 4 1 3\n",
      "chanc 5 4 1 4\n",
      "chandra 7 6 2 5\n",
      "chang 5 5 1 4\n",
      "changer 7 7 2 5\n",
      "channel 7 6 2 5\n",
      "chant 5 5 1 4\n",
      "chao 4 4 2 2\n",
      "chara 5 4 2 3\n",
      "charg 5 5 1 4\n",
      "charish 7 6 2 5\n",
      "charit 6 6 2 4\n",
      "chariti 7 6 3 4\n",
      "charl 5 5 1 4\n",
      "charli 6 6 2 4\n",
      "charlott 8 7 2 6\n",
      "charter 7 6 2 5\n",
      "chase 5 5 2 3\n",
      "chaser 6 6 2 4\n",
      "chat 4 4 1 3\n",
      "chatter 7 6 2 5\n",
      "chavez 6 6 2 4\n",
      "cheap 5 5 2 3\n",
      "cheaper 7 6 3 4\n",
      "cheat 5 5 2 3\n",
      "check 5 4 1 4\n",
      "checkpoint 10 9 3 7\n",
      "cheer 5 4 2 3\n",
      "chemic 6 5 2 4\n",
      "chemist 7 7 2 5\n",
      "chemo 5 5 2 3\n",
      "chemotherapi 12 10 5 7\n",
      "chen 4 4 1 3\n",
      "cheney 6 5 3 3\n",
      "chernobyl 9 9 3 6\n",
      "cheroke 7 6 3 4\n",
      "chesapeak 9 7 4 5\n",
      "chesco 6 5 2 4\n",
      "chevi 5 5 2 3\n",
      "chevron 7 7 2 5\n",
      "chhattisgarh 12 8 3 9\n",
      "chi 3 3 1 2\n",
      "chicago 7 6 3 4\n",
      "chicken 7 6 2 5\n",
      "chidambaram 11 8 4 7\n",
      "chide 5 5 2 3\n",
      "chief 5 5 2 3\n",
      "child 5 5 1 4\n",
      "children 8 8 2 6\n",
      "chile 5 5 2 3\n",
      "chili 5 4 2 3\n",
      "chill 5 4 1 4\n",
      "china 5 5 2 3\n",
      "chines 6 6 2 4\n",
      "chip 4 4 1 3\n",
      "chobani 7 7 3 4\n",
      "choic 5 4 2 3\n",
      "choos 5 4 2 3\n",
      "chop 4 4 1 3\n",
      "chopper 7 6 2 5\n",
      "chose 5 5 2 3\n",
      "chosen 6 6 2 4\n",
      "chri 4 4 1 3\n",
      "christen 8 8 2 6\n",
      "christi 7 6 2 5\n",
      "christian 9 8 3 6\n",
      "chronic 7 6 2 5\n",
      "chronolog 9 7 3 6\n",
      "chuck 5 4 1 4\n",
      "church 6 4 1 5\n",
      "churn 5 5 1 4\n",
      "cia 3 3 2 1\n",
      "cigarett 8 7 3 5\n",
      "cinema 6 6 3 3\n",
      "circu 5 4 2 3\n",
      "circuit 7 5 3 4\n",
      "circular 8 6 3 5\n",
      "cisco 5 4 2 3\n",
      "cite 4 4 2 2\n",
      "citi 4 3 2 2\n",
      "citigroup 9 8 4 5\n",
      "citizen 7 6 3 4\n",
      "citizenship 11 9 4 7\n",
      "civic 5 3 2 3\n",
      "civil 5 4 2 3\n",
      "civilian 8 6 4 4\n",
      "claim 5 5 2 3\n",
      "clapper 7 6 2 5\n",
      "clarient 8 8 3 5\n",
      "clarifi 7 6 3 4\n",
      "clariti 7 6 3 4\n",
      "clash 5 5 1 4\n",
      "class 5 4 1 4\n",
      "classifi 8 6 3 5\n",
      "classroom 9 7 3 6\n",
      "clawback 8 6 2 6\n",
      "clay 4 4 2 2\n",
      "clean 5 5 2 3\n",
      "cleaner 7 6 3 4\n",
      "clear 5 5 2 3\n",
      "clearanc 8 6 3 5\n",
      "clearwir 8 7 3 5\n",
      "clench 6 5 1 5\n",
      "clergi 6 6 2 4\n",
      "cleric 6 5 2 4\n",
      "clerk 5 5 1 4\n",
      "cleveland 9 7 3 6\n",
      "click 5 4 1 4\n",
      "clickabl 8 6 2 6\n",
      "client 6 6 2 4\n",
      "clifford 8 7 2 6\n",
      "climat 6 6 2 4\n",
      "climb 5 5 1 4\n",
      "climber 7 7 2 5\n",
      "cling 5 5 1 4\n",
      "clinic 6 4 2 4\n",
      "clinton 7 6 2 5\n",
      "clip 4 4 1 3\n",
      "cloak 5 5 2 3\n",
      "clock 5 4 1 4\n",
      "close 5 5 2 3\n",
      "closer 6 6 2 4\n",
      "closur 6 6 2 4\n",
      "clot 4 4 1 3\n",
      "cloth 5 5 1 4\n",
      "cloud 5 5 2 3\n",
      "clovi 5 5 2 3\n",
      "clown 5 5 1 4\n",
      "club 4 4 1 3\n",
      "clue 4 4 2 2\n",
      "cluster 7 7 2 5\n",
      "co 2 2 1 1\n",
      "coal 4 4 2 2\n",
      "coalgat 7 6 3 4\n",
      "coalit 6 6 3 3\n",
      "coast 5 5 2 3\n",
      "coastal 7 6 3 4\n",
      "coaster 7 7 3 4\n",
      "cobb 4 3 1 3\n",
      "cobra 5 5 2 3\n",
      "coca 4 3 2 2\n",
      "cocain 6 5 3 3\n",
      "cockpit 7 6 2 5\n",
      "code 4 4 2 2\n",
      "codi 4 4 2 2\n",
      "coe 3 3 2 1\n",
      "coffin 6 5 2 4\n",
      "cog 3 3 1 2\n",
      "cohen 5 5 2 3\n",
      "coin 4 4 2 2\n",
      "cola 4 4 2 2\n",
      "cold 4 4 1 3\n",
      "collabor 8 6 3 5\n",
      "collaps 7 6 2 5\n",
      "colleagu 8 7 4 4\n",
      "collect 7 5 2 5\n",
      "collector 9 6 3 6\n",
      "colleg 6 5 2 4\n",
      "collid 6 5 2 4\n",
      "collis 6 5 2 4\n",
      "colo 4 3 2 2\n",
      "colombia 8 7 4 4\n",
      "colombo 7 5 3 4\n",
      "colonel 7 5 3 4\n",
      "colonist 8 7 3 5\n",
      "color 5 4 2 3\n",
      "colorado 8 6 4 4\n",
      "colossu 7 5 3 4\n",
      "columbia 8 8 4 4\n",
      "columbu 7 6 3 4\n",
      "column 6 6 2 4\n",
      "columnist 9 9 3 6\n",
      "coma 4 4 2 2\n",
      "combat 6 6 2 4\n",
      "combin 6 6 2 4\n",
      "comcast 7 6 2 5\n",
      "come 4 4 2 2\n",
      "comeback 8 7 3 5\n",
      "comelec 7 5 3 4\n",
      "comfort 7 6 2 5\n",
      "comic 5 4 2 3\n",
      "command 7 6 2 5\n",
      "commando 8 6 3 5\n",
      "commenc 7 5 2 5\n",
      "commend 7 6 2 5\n",
      "comment 7 6 2 5\n",
      "commentari 10 9 4 6\n",
      "commerc 7 5 2 5\n",
      "commerci 8 6 3 5\n",
      "commiss 7 5 2 5\n",
      "commission 10 6 4 6\n",
      "commit 6 5 2 4\n",
      "committe 8 6 3 5\n",
      "commod 6 4 2 4\n",
      "common 6 4 2 4\n",
      "commun 6 5 2 4\n",
      "communiqu 9 7 4 5\n",
      "communist 9 8 3 6\n",
      "commut 6 5 2 4\n",
      "compani 7 7 3 4\n",
      "compar 6 6 2 4\n",
      "comparison 10 9 4 6\n",
      "compens 7 7 2 5\n",
      "competit 8 7 3 5\n",
      "complain 8 8 3 5\n",
      "complaint 9 9 3 6\n",
      "complet 7 7 2 5\n",
      "complex 7 7 2 5\n",
      "compli 6 6 2 4\n",
      "compound 8 7 3 5\n",
      "compromis 9 7 3 6\n",
      "comptrol 8 7 2 6\n",
      "comput 6 6 2 4\n",
      "con 3 3 1 2\n",
      "conceal 7 6 3 4\n",
      "conced 6 5 2 4\n",
      "concern 7 5 2 5\n",
      "concert 7 6 2 5\n",
      "concess 7 5 2 5\n",
      "concili 7 5 3 4\n",
      "conclud 7 6 2 5\n",
      "concordia 9 7 4 5\n",
      "concret 7 6 2 5\n",
      "condemn 7 6 2 5\n",
      "condit 6 6 2 4\n",
      "condition 9 6 4 5\n",
      "condo 5 4 2 3\n",
      "condon 6 4 2 4\n",
      "conduct 7 6 2 5\n",
      "cone 4 4 2 2\n",
      "confer 6 6 2 4\n",
      "confess 7 6 2 5\n",
      "confessor 9 7 3 6\n",
      "confid 6 6 2 4\n",
      "confiden 8 7 3 5\n",
      "confin 6 5 2 4\n",
      "confirm 7 7 2 5\n",
      "conflict 8 7 2 6\n",
      "confound 8 6 3 5\n",
      "confront 8 6 2 6\n",
      "confus 6 6 2 4\n",
      "congress 8 7 2 6\n",
      "congression 11 8 4 7\n",
      "congressman 11 9 3 8\n",
      "congressmen 11 8 3 8\n",
      "conman 6 5 2 4\n",
      "conn 4 3 1 3\n",
      "connect 7 5 2 5\n",
      "connecticut 11 7 4 7\n",
      "conquest 8 8 3 5\n",
      "consensu 8 6 3 5\n",
      "consequ 7 7 3 4\n",
      "conserv 7 7 2 5\n",
      "consid 6 6 2 4\n",
      "considin 8 6 3 5\n",
      "consist 7 6 2 5\n",
      "consolid 8 7 3 5\n",
      "conspiraci 10 8 4 6\n",
      "constel 7 7 2 5\n",
      "constitut 9 7 3 6\n",
      "construct 9 7 2 7\n",
      "consul 6 6 2 4\n",
      "consult 7 7 2 5\n",
      "consum 6 6 2 4\n",
      "contact 7 5 2 5\n",
      "contagion 9 7 4 5\n",
      "contain 7 6 3 4\n",
      "contamin 8 7 3 5\n",
      "contempt 8 7 2 6\n",
      "content 7 5 2 5\n",
      "contest 7 6 2 5\n",
      "contin 6 5 2 4\n",
      "continu 7 6 3 4\n",
      "contra 6 6 2 4\n",
      "contract 8 6 2 6\n",
      "contractor 10 6 3 7\n",
      "contradict 10 8 3 7\n",
      "contribut 9 8 3 6\n",
      "control 7 6 2 5\n",
      "controversi 11 9 4 7\n",
      "conundrum 9 7 3 6\n",
      "converg 7 7 2 5\n",
      "convers 7 7 2 5\n",
      "convict 7 6 2 5\n",
      "convinc 7 5 2 5\n",
      "convolut 8 7 3 5\n",
      "convoy 6 5 3 3\n",
      "cook 4 3 2 2\n",
      "cool 4 3 2 2\n",
      "cooler 6 5 3 3\n",
      "cooper 6 5 3 3\n",
      "coordin 7 6 3 4\n",
      "cop 3 3 1 2\n",
      "copacabana 10 6 5 5\n",
      "copi 4 4 2 2\n",
      "copthat 7 6 2 5\n",
      "coptic 6 5 2 4\n",
      "copycat 7 6 3 4\n",
      "copyright 9 9 3 6\n",
      "cord 4 4 1 3\n",
      "cordray 7 6 3 4\n",
      "core 4 4 2 2\n",
      "cori 4 4 2 2\n",
      "corizon 7 6 3 4\n",
      "cork 4 4 1 3\n",
      "corn 4 4 1 3\n",
      "corner 6 5 2 4\n",
      "cornyn 6 5 2 4\n",
      "corolla 7 5 3 4\n",
      "coron 5 4 2 3\n",
      "coronaviru 10 8 5 5\n",
      "corp 4 4 1 3\n",
      "corpor 6 4 2 4\n",
      "corporatebond 13 10 5 8\n",
      "corps 5 5 1 4\n",
      "correct 7 5 2 5\n",
      "correspond 10 8 3 7\n",
      "corrupt 7 6 2 5\n",
      "corso 5 4 2 3\n",
      "cosi 4 4 2 2\n",
      "cost 4 4 1 3\n",
      "costa 5 5 2 3\n",
      "costli 6 6 2 4\n",
      "cotabato 8 5 4 4\n",
      "could 5 5 2 3\n",
      "council 7 6 3 4\n",
      "counsel 7 7 3 4\n",
      "count 5 5 2 3\n",
      "counter 7 7 3 4\n",
      "counterinsurg 13 10 5 8\n",
      "counterpoint 12 9 5 7\n",
      "counti 6 6 3 3\n",
      "countri 7 7 3 4\n",
      "coup 4 4 2 2\n",
      "coupl 5 5 2 3\n",
      "coupon 6 5 3 3\n",
      "cour 4 4 2 2\n",
      "courag 6 6 3 3\n",
      "cours 5 5 2 3\n",
      "court 5 5 2 3\n",
      "courthous 9 7 4 5\n",
      "cousin 6 6 3 3\n",
      "cover 5 5 2 3\n",
      "coverag 7 7 3 4\n",
      "covert 6 6 2 4\n",
      "cow 3 3 1 2\n",
      "cowardli 8 8 3 5\n",
      "coy 3 3 2 1\n",
      "cpart 5 5 1 4\n",
      "cpxix 5 4 1 4\n",
      "crack 5 4 1 4\n",
      "crackdown 9 8 2 7\n",
      "crackstart 10 6 2 8\n",
      "craigslist 10 8 3 7\n",
      "crane 5 5 2 3\n",
      "crash 5 5 1 4\n",
      "crave 5 5 2 3\n",
      "crazi 5 5 2 3\n",
      "creat 5 5 2 3\n",
      "creation 8 8 4 4\n",
      "creativ 7 7 3 4\n",
      "credenti 8 7 3 5\n",
      "credibl 7 7 2 5\n",
      "credit 6 6 2 4\n",
      "creditcard 10 7 3 7\n",
      "credo 5 5 2 3\n",
      "creep 5 4 2 3\n",
      "crew 4 4 1 3\n",
      "cri 3 3 1 2\n",
      "crime 5 5 2 3\n",
      "crimean 7 7 3 4\n",
      "crimin 6 5 2 4\n",
      "crippl 6 5 1 5\n",
      "crise 5 5 2 3\n",
      "crisi 5 4 2 3\n",
      "crist 5 5 1 4\n",
      "critic 6 4 2 4\n",
      "critter 7 5 2 5\n",
      "croatia 7 6 4 3\n",
      "crocodil 8 6 3 5\n",
      "crop 4 4 1 3\n",
      "crore 5 4 2 3\n",
      "cross 5 4 1 4\n",
      "crowd 5 5 1 4\n",
      "crown 5 5 1 4\n",
      "croyl 5 5 2 3\n",
      "crucial 7 6 3 4\n",
      "crude 5 5 2 3\n",
      "cruis 5 5 2 3\n",
      "cruiselin 9 8 4 5\n",
      "crumbl 6 6 1 5\n",
      "crunch 6 5 1 5\n",
      "crush 5 5 1 4\n",
      "cruz 4 4 1 3\n",
      "ctao 4 4 2 2\n",
      "cub 3 3 1 2\n",
      "cuba 4 4 2 2\n",
      "cuban 5 5 2 3\n",
      "cull 4 3 1 3\n",
      "cultiv 6 6 2 4\n",
      "cun 3 3 1 2\n",
      "cuomo 5 4 3 2\n",
      "cup 3 3 1 2\n",
      "cupe 4 4 2 2\n",
      "curat 5 5 2 3\n",
      "curb 4 4 1 3\n",
      "cure 4 4 2 2\n",
      "curfew 6 6 2 4\n",
      "currenc 7 5 2 5\n",
      "curriculum 10 6 4 6\n",
      "curs 4 4 1 3\n",
      "curtail 7 7 3 4\n",
      "curti 5 5 2 3\n",
      "custodi 7 7 3 4\n",
      "custom 6 6 2 4\n",
      "cut 3 3 1 2\n",
      "cvent 5 5 1 4\n",
      "cyber 5 5 2 3\n",
      "cyberattack 11 8 4 7\n",
      "cybercrim 9 7 3 6\n",
      "cyberheist 10 9 4 6\n",
      "cyberpolici 11 9 5 6\n",
      "cybersecur 10 7 4 6\n",
      "cyberspi 8 8 3 5\n",
      "cyberwat 8 8 3 5\n",
      "cyclon 6 5 2 4\n",
      "cypru 5 5 2 3\n",
      "cyrpu 5 5 2 3\n",
      "cyru 4 4 2 2\n",
      "czech 5 4 1 4\n",
      "da 2 2 1 1\n",
      "daa 3 2 2 1\n",
      "daae 4 3 3 1\n",
      "dab 3 3 1 2\n",
      "dabaead 7 4 4 3\n",
      "dabb 4 3 1 3\n",
      "dabe 4 4 2 2\n",
      "dac 3 3 1 2\n",
      "dad 3 2 1 2\n",
      "dadc 4 3 1 3\n",
      "dade 4 3 2 2\n",
      "daeaad 6 3 4 2\n",
      "daeccb 6 5 2 4\n",
      "dafabf 6 4 2 4\n",
      "dagger 6 5 2 4\n",
      "daili 5 4 3 2\n",
      "daimler 7 7 3 4\n",
      "dairi 5 4 3 2\n",
      "dale 4 4 2 2\n",
      "dalla 5 3 2 3\n",
      "dalziel 7 6 3 4\n",
      "damag 5 4 2 3\n",
      "damascu 7 6 3 4\n",
      "damn 4 4 1 3\n",
      "dan 3 3 1 2\n",
      "danger 6 6 2 4\n",
      "daniel 6 6 3 3\n",
      "dare 4 4 2 2\n",
      "daredevil 9 7 4 5\n",
      "darfur 6 5 2 4\n",
      "dark 4 4 1 3\n",
      "darken 6 6 2 4\n",
      "darwin 6 6 2 4\n",
      "dash 4 4 1 3\n",
      "data 4 3 2 2\n",
      "databas 7 5 3 4\n",
      "date 4 4 2 2\n",
      "datebook 8 7 4 4\n",
      "datu 4 4 2 2\n",
      "daughter 8 8 3 5\n",
      "dave 4 4 2 2\n",
      "davi 4 4 2 2\n",
      "david 5 4 2 3\n",
      "dawn 4 4 1 3\n",
      "day 3 3 2 1\n",
      "daybook 7 6 4 3\n",
      "dba 3 3 1 2\n",
      "dbad 4 3 1 3\n",
      "dbaddafac 9 5 3 6\n",
      "dbbe 4 3 1 3\n",
      "dbda 4 3 1 3\n",
      "dbdacdb 7 4 1 6\n",
      "dbde 4 3 1 3\n",
      "dca 3 3 1 2\n",
      "dcaa 4 3 2 2\n",
      "dcab 4 4 1 3\n",
      "dcacdfd 7 4 1 6\n",
      "dcdfec 6 4 1 5\n",
      "dce 3 3 1 2\n",
      "dcecabd 7 5 2 5\n",
      "dcecd 5 3 1 4\n",
      "dcfe 4 4 1 3\n",
      "dcfeeab 7 6 3 4\n",
      "dda 3 2 1 2\n",
      "ddaa 4 2 2 2\n",
      "ddabaceec 9 5 4 5\n",
      "dddec 5 3 1 4\n",
      "dddfce 6 4 1 5\n",
      "dde 3 2 1 2\n",
      "ddea 4 3 2 2\n",
      "de 2 2 1 1\n",
      "dea 3 3 2 1\n",
      "dead 4 3 2 2\n",
      "deadbeat 8 5 4 4\n",
      "deadd 5 3 2 3\n",
      "deadli 6 5 3 3\n",
      "deadliest 9 7 4 5\n",
      "deadlin 7 6 3 4\n",
      "deadlock 8 7 3 5\n",
      "deal 4 4 2 2\n",
      "dealer 6 5 3 3\n",
      "dean 4 4 2 2\n",
      "death 5 5 2 3\n",
      "debat 5 5 2 3\n",
      "debd 4 3 1 3\n",
      "debe 4 3 2 2\n",
      "debit 5 5 2 3\n",
      "debt 4 4 1 3\n",
      "debut 5 5 2 3\n",
      "dec 3 3 1 2\n",
      "decad 5 4 2 3\n",
      "decbef 6 5 2 4\n",
      "decid 5 4 2 3\n",
      "decis 5 5 2 3\n",
      "declar 6 6 2 4\n",
      "declassifi 10 8 4 6\n",
      "declin 6 6 2 4\n",
      "decre 5 4 2 3\n",
      "decreas 7 6 3 4\n",
      "decri 5 5 2 3\n",
      "dedic 5 4 2 3\n",
      "deedef 6 3 3 3\n",
      "deep 4 3 2 2\n",
      "deepen 6 4 3 3\n",
      "deepli 6 5 3 3\n",
      "def 3 3 1 2\n",
      "defa 4 4 2 2\n",
      "defac 5 5 2 3\n",
      "default 7 7 3 4\n",
      "defeat 6 5 3 3\n",
      "defector 8 7 3 5\n",
      "defenc 6 5 2 4\n",
      "defend 6 4 2 4\n",
      "defens 6 5 2 4\n",
      "defer 5 4 2 3\n",
      "defi 4 4 2 2\n",
      "defianc 7 7 3 4\n",
      "defiant 7 7 3 4\n",
      "deficit 7 6 3 4\n",
      "defin 5 5 2 3\n",
      "definit 7 6 3 4\n",
      "defus 5 5 2 3\n",
      "degre 5 4 2 3\n",
      "del 3 3 1 2\n",
      "delay 5 5 3 2\n",
      "deleg 5 4 2 3\n",
      "delet 5 4 2 3\n",
      "delhi 5 5 2 3\n",
      "deliber 7 6 3 4\n",
      "deliv 5 5 2 3\n",
      "deliveri 8 6 4 4\n",
      "dell 4 3 1 3\n",
      "delta 5 5 2 3\n",
      "dem 3 3 1 2\n",
      "demand 6 5 2 4\n",
      "demo 4 4 2 2\n",
      "democraci 9 8 4 5\n",
      "democrat 8 8 3 5\n",
      "demolit 7 7 3 4\n",
      "demonstr 8 8 2 6\n",
      "dempsey 7 6 3 4\n",
      "dene 4 3 2 2\n",
      "deni 4 4 2 2\n",
      "denial 6 6 3 3\n",
      "denison 7 6 3 4\n",
      "denounc 7 6 3 4\n",
      "dental 6 6 2 4\n",
      "dentist 7 6 2 5\n",
      "denver 6 5 2 4\n",
      "depart 6 6 2 4\n",
      "depict 6 6 2 4\n",
      "deploy 6 6 3 3\n",
      "deport 6 6 2 4\n",
      "depos 5 5 2 3\n",
      "depress 7 5 2 5\n",
      "dept 4 4 1 3\n",
      "depth 5 5 1 4\n",
      "deputi 6 6 3 3\n",
      "der 3 3 1 2\n",
      "dera 4 4 2 2\n",
      "derail 6 6 3 3\n",
      "derecho 7 6 3 4\n",
      "derid 5 4 2 3\n",
      "derrick 7 6 2 5\n",
      "desalvo 7 7 3 4\n",
      "describ 7 7 2 5\n",
      "desert 6 5 2 4\n",
      "deserv 6 5 2 4\n",
      "desh 4 4 1 3\n",
      "design 6 6 2 4\n",
      "desper 6 5 2 4\n",
      "despit 6 6 2 4\n",
      "destabilis 10 8 4 6\n",
      "destin 6 6 2 4\n",
      "destroy 7 7 3 4\n",
      "destruct 8 7 2 6\n",
      "detail 6 6 3 3\n",
      "detain 6 6 3 3\n",
      "detaine 7 6 4 3\n",
      "detect 6 4 2 4\n",
      "detent 6 4 2 4\n",
      "deter 5 4 2 3\n",
      "deterr 6 4 2 4\n",
      "detroit 7 6 3 4\n",
      "devast 6 6 2 4\n",
      "develop 7 6 3 4\n",
      "deviant 7 7 3 4\n",
      "devic 5 5 2 3\n",
      "devil 5 5 2 3\n",
      "devote 6 5 3 3\n",
      "dewani 6 6 3 3\n",
      "dfa 3 3 1 2\n",
      "dfaefaa 7 4 4 3\n",
      "dfe 3 3 1 2\n",
      "dfee 4 3 2 2\n",
      "dffebb 6 4 1 5\n",
      "diabetesbremc 13 10 5 8\n",
      "diablo 6 6 3 3\n",
      "dialogu 7 7 4 3\n",
      "diamond 7 6 3 4\n",
      "diana 5 4 3 2\n",
      "diari 5 4 3 2\n",
      "dictat 6 5 2 4\n",
      "dictionari 10 8 5 5\n",
      "diddi 5 2 2 3\n",
      "didnt 5 4 1 4\n",
      "die 3 3 2 1\n",
      "diego 5 5 3 2\n",
      "differ 6 5 2 4\n",
      "difficult 9 7 3 6\n",
      "dig 3 3 1 2\n",
      "digit 5 4 2 3\n",
      "dijsselbloem 12 9 4 8\n",
      "dilemma 7 6 3 4\n",
      "dimaggio 8 6 4 4\n",
      "dime 4 4 2 2\n",
      "dimon 5 5 2 3\n",
      "dine 4 4 2 2\n",
      "ding 4 4 1 3\n",
      "dinner 6 5 2 4\n",
      "dio 3 3 2 1\n",
      "dioces 6 6 3 3\n",
      "dioxid 6 4 3 3\n",
      "dip 3 3 1 2\n",
      "diplomaci 9 8 4 5\n",
      "diplomat 8 8 3 5\n",
      "direct 6 6 2 4\n",
      "director 8 7 3 5\n",
      "disabl 6 6 2 4\n",
      "disadvantag 11 8 4 7\n",
      "disagre 7 7 3 4\n",
      "disappear 9 7 4 5\n",
      "disappoint 10 8 4 6\n",
      "disarm 6 6 2 4\n",
      "disarma 7 6 3 4\n",
      "disarray 8 6 4 4\n",
      "disast 6 5 2 4\n",
      "disband 7 6 2 5\n",
      "disbelief 9 7 4 5\n",
      "discharg 8 8 2 6\n",
      "discipl 7 6 2 5\n",
      "disciplin 9 7 3 6\n",
      "disclos 7 6 2 5\n",
      "disclosur 9 8 3 6\n",
      "discount 8 8 3 5\n",
      "discov 6 6 2 4\n",
      "discoveri 9 8 4 5\n",
      "discredit 9 7 3 6\n",
      "discrimin 9 7 3 6\n",
      "discuss 7 5 2 5\n",
      "diseas 6 5 3 3\n",
      "disgust 7 6 2 5\n",
      "dish 4 4 1 3\n",
      "disk 4 4 1 3\n",
      "dislik 6 5 2 4\n",
      "dismal 6 6 2 4\n",
      "dismantl 8 8 2 6\n",
      "dismiss 7 4 2 5\n",
      "disney 6 6 3 3\n",
      "disord 6 5 2 4\n",
      "dispatch 8 8 2 6\n",
      "dispers 7 6 2 5\n",
      "displac 7 7 2 5\n",
      "display 7 7 3 4\n",
      "disput 6 6 2 4\n",
      "disrepair 9 7 4 5\n",
      "disrupt 7 7 2 5\n",
      "dissect 7 6 2 5\n",
      "dissent 7 6 2 5\n",
      "dissid 6 3 2 4\n",
      "dissolut 8 7 3 5\n",
      "distanc 7 7 2 5\n",
      "distant 7 6 2 5\n",
      "distast 7 5 2 5\n",
      "distributor 11 8 4 7\n",
      "district 8 6 2 6\n",
      "disturb 7 7 2 5\n",
      "dither 6 6 2 4\n",
      "dive 4 4 2 2\n",
      "diver 5 5 2 3\n",
      "divers 6 6 2 4\n",
      "divert 6 6 2 4\n",
      "divest 6 6 2 4\n",
      "divid 5 3 2 3\n",
      "divorc 6 6 2 4\n",
      "django 6 6 2 4\n",
      "dkaim 5 5 2 3\n",
      "dna 3 3 1 2\n",
      "doc 3 3 1 2\n",
      "doctor 6 5 2 4\n",
      "doctrin 7 7 2 5\n",
      "document 8 8 3 5\n",
      "dod 3 2 1 2\n",
      "dodger 6 5 2 4\n",
      "doe 3 3 2 1\n",
      "doesnt 6 6 2 4\n",
      "dog 3 3 1 2\n",
      "doha 4 4 2 2\n",
      "doj 3 3 1 2\n",
      "dol 3 3 1 2\n",
      "dole 4 4 2 2\n",
      "dollar 6 5 2 4\n",
      "dolphin 7 7 2 5\n",
      "domest 6 6 2 4\n",
      "domin 5 5 2 3\n",
      "donald 6 5 2 4\n",
      "donat 5 5 2 3\n",
      "done 4 4 2 2\n",
      "donetsk 7 7 2 5\n",
      "donglu 6 6 2 4\n",
      "donor 5 4 2 3\n",
      "dont 4 4 1 3\n",
      "donut 5 5 2 3\n",
      "doomsday 8 6 4 4\n",
      "door 4 3 2 2\n",
      "dope 4 4 2 2\n",
      "dorito 6 5 3 3\n",
      "dormic 6 6 2 4\n",
      "dot 3 3 1 2\n",
      "dotc 4 4 1 3\n",
      "doubl 5 5 2 3\n",
      "doubt 5 5 2 3\n",
      "doughnut 8 7 3 5\n",
      "dover 5 5 2 3\n",
      "downplay 8 8 3 5\n",
      "downstream 10 10 3 7\n",
      "downtonabbey 12 9 5 7\n",
      "dowri 5 5 2 3\n",
      "dozen 5 5 2 3\n",
      "draft 5 5 1 4\n",
      "drag 4 4 1 3\n",
      "drama 5 4 2 3\n",
      "draw 4 4 1 3\n",
      "drawn 5 5 1 4\n",
      "dream 5 5 2 3\n",
      "dreamlin 8 8 3 5\n",
      "dress 5 4 1 4\n",
      "dri 3 3 1 2\n",
      "drill 5 4 1 4\n",
      "driller 7 5 2 5\n",
      "drink 5 5 1 4\n",
      "driscol 7 7 2 5\n",
      "drive 5 5 2 3\n",
      "driver 6 5 2 4\n",
      "drone 5 5 2 3\n",
      "drop 4 4 1 3\n",
      "drove 5 5 2 3\n",
      "drown 5 5 1 4\n",
      "drug 4 4 1 3\n",
      "drugmak 7 7 2 5\n",
      "drummer 7 5 2 5\n",
      "drunk 5 5 1 4\n",
      "drunken 7 6 2 5\n",
      "dual 4 4 2 2\n",
      "dubai 5 5 3 2\n",
      "duchess 7 6 2 5\n",
      "duckfat 7 7 2 5\n",
      "due 3 3 2 1\n",
      "duel 4 4 2 2\n",
      "dui 3 3 2 1\n",
      "dull 4 3 1 3\n",
      "dumb 4 4 1 3\n",
      "dumbarton 9 9 3 6\n",
      "dump 4 4 1 3\n",
      "dunk 4 4 1 3\n",
      "dunkin 6 5 2 4\n",
      "duo 3 3 2 1\n",
      "dustin 6 6 2 4\n",
      "dutch 5 5 1 4\n",
      "duti 4 4 2 2\n",
      "dvfyktoc 8 8 2 6\n",
      "dwindl 6 5 1 5\n",
      "ea 2 2 2 0\n",
      "eaa 3 2 3 0\n",
      "eaaed 5 3 4 1\n",
      "ead 3 3 2 1\n",
      "eaeabb 6 3 4 2\n",
      "eaf 3 3 2 1\n",
      "eafc 4 4 2 2\n",
      "eager 5 4 3 2\n",
      "eagl 4 4 2 2\n",
      "eaniylgg 8 7 4 4\n",
      "earli 5 5 3 2\n",
      "earlier 7 5 4 3\n",
      "earn 4 4 2 2\n",
      "earth 5 5 2 3\n",
      "earthquak 9 8 4 5\n",
      "eas 3 3 2 1\n",
      "easi 4 4 3 1\n",
      "easier 6 5 4 2\n",
      "east 4 4 2 2\n",
      "easter 6 5 3 3\n",
      "eastern 7 6 3 4\n",
      "eat 3 3 2 1\n",
      "eavesdrop 9 8 4 5\n",
      "eb 2 2 1 1\n",
      "ebay 4 4 3 1\n",
      "ebb 3 2 1 2\n",
      "ebbaf 5 4 2 3\n",
      "ebbcf 5 4 1 4\n",
      "ebc 3 3 1 2\n",
      "ebdfc 5 5 1 4\n",
      "ebe 3 2 2 1\n",
      "ebea 4 3 3 1\n",
      "ebf 3 3 1 2\n",
      "ebfb 4 3 1 3\n",
      "ebook 5 4 3 2\n",
      "ebt 3 3 1 2\n",
      "ec 2 2 1 1\n",
      "ecabec 6 4 3 3\n",
      "ecb 3 3 1 2\n",
      "eccbdc 6 4 1 5\n",
      "eccccab 7 4 2 5\n",
      "ecd 3 3 1 2\n",
      "ecf 3 3 1 2\n",
      "ecfa 4 4 2 2\n",
      "eclips 6 6 2 4\n",
      "econom 6 5 3 3\n",
      "economi 7 6 4 3\n",
      "economist 9 8 4 5\n",
      "ecuador 7 7 4 3\n",
      "ecuadorean 10 8 6 4\n",
      "ecuadorian 10 9 6 4\n",
      "ed 2 2 1 1\n",
      "eda 3 3 2 1\n",
      "edaec 5 4 3 2\n",
      "edbdeac 7 5 3 4\n",
      "edc 3 3 1 2\n",
      "edcbbeae 8 5 4 4\n",
      "edcfa 5 5 2 3\n",
      "edd 3 2 1 2\n",
      "eddcbc 6 4 1 5\n",
      "eddccf 6 4 1 5\n",
      "edfdca 6 5 2 4\n",
      "edfeaacf 8 5 4 4\n",
      "edg 3 3 1 2\n",
      "edit 4 4 2 2\n",
      "editor 6 6 3 3\n",
      "editori 7 6 4 3\n",
      "edl 3 3 1 2\n",
      "edmonton 8 6 3 5\n",
      "edt 3 3 1 2\n",
      "educ 4 4 2 2\n",
      "edward 6 5 2 4\n",
      "eeb 3 2 2 1\n",
      "eec 3 2 2 1\n",
      "eecbb 5 3 2 3\n",
      "eecf 4 3 2 2\n",
      "eed 3 2 2 1\n",
      "eeda 4 3 3 1\n",
      "eedfaf 6 4 3 3\n",
      "eefaa 5 3 4 1\n",
      "eefc 4 3 2 2\n",
      "ef 2 2 1 1\n",
      "efa 3 3 2 1\n",
      "efaefcb 7 5 3 4\n",
      "efbbbb 6 3 1 5\n",
      "efc 3 3 1 2\n",
      "efcceb 6 4 2 4\n",
      "efd 3 3 1 2\n",
      "efdbc 5 5 1 4\n",
      "efe 3 2 2 1\n",
      "eff 3 2 1 2\n",
      "effbff 6 3 1 5\n",
      "effc 4 3 1 3\n",
      "effect 6 4 2 4\n",
      "effici 6 4 3 3\n",
      "effort 6 5 2 4\n",
      "efrea 5 4 3 2\n",
      "efreal 6 5 3 3\n",
      "efreau 6 5 4 2\n",
      "efreono 7 5 4 3\n",
      "efreoo 6 4 4 2\n",
      "efrfkp 6 5 1 5\n",
      "efrfku 6 5 2 4\n",
      "efrfkui 7 6 3 4\n",
      "efrfkur 7 5 2 5\n",
      "efrfm 5 4 1 4\n",
      "efrfmr 6 4 1 5\n",
      "efrfqfr 7 4 1 6\n",
      "efrfro 6 4 2 4\n",
      "egg 3 2 1 2\n",
      "egon 4 4 2 2\n",
      "egypt 5 5 2 3\n",
      "egyptian 8 8 4 4\n",
      "egyptislamist 13 10 5 8\n",
      "eheptgihnqo 11 9 4 7\n",
      "eiffel 6 4 3 3\n",
      "eight 5 5 2 3\n",
      "eilat 5 5 3 2\n",
      "eject 5 4 2 3\n",
      "el 2 2 1 1\n",
      "elan 4 4 2 2\n",
      "elbaradei 9 7 5 4\n",
      "elderli 7 5 3 4\n",
      "elect 5 4 2 3\n",
      "elector 7 6 3 4\n",
      "electr 6 5 2 4\n",
      "electron 8 7 3 5\n",
      "element 7 5 3 4\n",
      "elementari 10 8 5 5\n",
      "eleph 5 4 2 3\n",
      "elev 4 3 2 2\n",
      "eleven 6 4 3 3\n",
      "elgin 5 5 2 3\n",
      "elig 4 4 2 2\n",
      "elim 4 4 2 2\n",
      "elit 4 4 2 2\n",
      "elitist 7 5 3 4\n",
      "elizabeth 9 8 4 5\n",
      "ellsberg 8 6 2 6\n",
      "elsewher 8 6 3 5\n",
      "email 5 5 3 2\n",
      "embassi 7 6 3 4\n",
      "embezzl 7 5 2 5\n",
      "embodi 6 6 3 3\n",
      "embrac 6 6 2 4\n",
      "embroil 7 7 3 4\n",
      "emerg 5 4 2 3\n",
      "emir 4 4 2 2\n",
      "emiss 5 4 2 3\n",
      "emit 4 4 2 2\n",
      "emmi 4 3 2 2\n",
      "emot 4 4 2 2\n",
      "empir 5 5 2 3\n",
      "employ 6 6 3 3\n",
      "employe 7 6 4 3\n",
      "empti 5 5 2 3\n",
      "en 2 2 1 1\n",
      "enclosur 8 8 3 5\n",
      "encount 7 6 3 4\n",
      "encourag 8 8 4 4\n",
      "encroach 8 7 3 5\n",
      "end 3 3 1 2\n",
      "endang 6 5 2 4\n",
      "endless 7 5 2 5\n",
      "endlessli 9 6 3 6\n",
      "endors 6 6 2 4\n",
      "endur 5 5 2 3\n",
      "enemi 5 4 3 2\n",
      "energi 6 5 3 3\n",
      "energyfutur 11 8 5 6\n",
      "enforc 6 6 2 4\n",
      "engag 5 4 2 3\n",
      "engin 5 4 2 3\n",
      "england 7 6 2 5\n",
      "english 7 7 2 5\n",
      "engulf 6 6 2 4\n",
      "enlighten 9 7 3 6\n",
      "enmax 5 5 2 3\n",
      "enough 6 6 3 3\n",
      "enrich 6 6 2 4\n",
      "enrol 5 5 2 3\n",
      "enron 5 4 2 3\n",
      "ensign 6 5 2 4\n",
      "enslav 6 6 2 4\n",
      "ensur 5 5 2 3\n",
      "ent 3 3 1 2\n",
      "enter 5 4 2 3\n",
      "enterpris 9 7 3 6\n",
      "enthusiast 10 8 4 6\n",
      "entir 5 5 2 3\n",
      "entiti 6 4 3 3\n",
      "entranc 7 6 2 5\n",
      "environ 7 6 3 4\n",
      "environment 11 8 4 7\n",
      "envoy 5 5 3 2\n",
      "epa 3 3 2 1\n",
      "eq 2 2 1 1\n",
      "equip 5 5 3 2\n",
      "era 3 3 2 1\n",
      "erdogan 7 7 3 4\n",
      "eri 3 3 2 1\n",
      "erian 5 5 3 2\n",
      "eric 4 4 2 2\n",
      "erod 4 4 2 2\n",
      "errand 6 5 2 4\n",
      "errat 5 4 2 3\n",
      "error 5 3 2 3\n",
      "ert 3 3 1 2\n",
      "erupt 5 5 2 3\n",
      "escal 5 5 2 3\n",
      "escap 5 5 2 3\n",
      "escape 6 5 3 3\n",
      "escort 6 6 2 4\n",
      "eskom 5 5 2 3\n",
      "especi 6 5 3 3\n",
      "espionag 8 8 4 4\n",
      "essam 5 4 2 3\n",
      "essenti 7 5 3 4\n",
      "est 3 3 1 2\n",
      "establish 9 8 3 6\n",
      "estat 5 4 2 3\n",
      "estim 5 5 2 3\n",
      "estrang 7 7 2 5\n",
      "et 2 2 1 1\n",
      "etf 3 3 1 2\n",
      "ethanol 7 7 3 4\n",
      "ethiopia 8 7 5 3\n",
      "etx 3 3 1 2\n",
      "eu 2 2 2 0\n",
      "eugen 5 4 3 2\n",
      "euphoria 8 8 5 3\n",
      "eurasian 8 7 5 3\n",
      "euro 4 4 3 1\n",
      "eurobank 8 8 4 4\n",
      "euronew 7 6 4 3\n",
      "europ 5 5 3 2\n",
      "european 8 7 5 3\n",
      "europeradio 11 8 7 4\n",
      "eurozon 7 6 4 3\n",
      "evacu 5 5 3 2\n",
      "evacue 6 5 4 2\n",
      "evad 4 4 2 2\n",
      "evalu 5 5 3 2\n",
      "evan 4 4 2 2\n",
      "evangel 7 6 3 4\n",
      "even 4 3 2 2\n",
      "event 5 4 2 3\n",
      "ever 4 3 2 2\n",
      "everi 5 4 3 2\n",
      "everybodi 9 8 5 4\n",
      "everyon 7 6 4 3\n",
      "everyth 7 6 3 4\n",
      "evict 5 5 2 3\n",
      "evid 4 4 2 2\n",
      "evil 4 4 2 2\n",
      "evo 3 3 2 1\n",
      "ex 2 2 1 1\n",
      "examin 6 6 3 3\n",
      "exampl 6 6 2 4\n",
      "excav 5 5 2 3\n",
      "exceasefir 10 8 5 5\n",
      "except 6 5 2 4\n",
      "exception 9 8 4 5\n",
      "exchang 7 7 2 5\n",
      "exclus 6 6 2 4\n",
      "exec 4 3 2 2\n",
      "execut 6 5 3 3\n",
      "exelon 6 5 3 3\n",
      "exercis 7 6 3 4\n",
      "exhibit 7 6 3 4\n",
      "exhum 5 5 2 3\n",
      "exil 4 4 2 2\n",
      "exim 4 4 2 2\n",
      "exist 5 5 2 3\n",
      "exit 4 4 2 2\n",
      "exorc 5 5 2 3\n",
      "expand 6 6 2 4\n",
      "expans 6 6 2 4\n",
      "expat 5 5 2 3\n",
      "expatri 7 7 3 4\n",
      "expect 6 5 2 4\n",
      "expel 5 4 2 3\n",
      "expens 6 5 2 4\n",
      "experi 6 5 3 3\n",
      "expert 6 5 2 4\n",
      "expir 5 5 2 3\n",
      "explain 7 7 3 4\n",
      "explod 6 6 2 4\n",
      "exploit 7 7 3 4\n",
      "explor 6 6 2 4\n",
      "explos 6 6 2 4\n",
      "expo 4 4 2 2\n",
      "export 6 6 2 4\n",
      "expos 5 5 2 3\n",
      "exposur 7 7 3 4\n",
      "express 7 5 2 5\n",
      "expuls 6 6 2 4\n",
      "extend 6 5 2 4\n",
      "extens 6 5 2 4\n",
      "extinct 7 6 2 5\n",
      "extra 5 5 2 3\n",
      "extradit 8 7 3 5\n",
      "extraordinari 13 9 6 7\n",
      "extrem 6 5 2 4\n",
      "extremist 9 7 3 6\n",
      "exxon 5 4 2 3\n",
      "eye 3 2 3 0\n",
      "eyelin 6 5 4 2\n",
      "fa 2 2 1 1\n",
      "faa 3 2 2 1\n",
      "faacac 6 3 3 3\n",
      "faacd 5 4 2 3\n",
      "faae 4 3 3 1\n",
      "fab 3 3 1 2\n",
      "fabbc 5 4 1 4\n",
      "fabc 4 4 1 3\n",
      "fabul 5 5 2 3\n",
      "faca 4 3 2 2\n",
      "facaedceab 10 6 5 5\n",
      "facb 4 4 1 3\n",
      "face 4 4 2 2\n",
      "facebook 8 7 4 4\n",
      "facial 6 5 3 3\n",
      "facil 5 5 2 3\n",
      "facilit 7 6 3 4\n",
      "fact 4 4 1 3\n",
      "factbox 7 7 2 5\n",
      "faction 7 7 3 4\n",
      "factor 6 6 2 4\n",
      "factori 7 7 3 4\n",
      "fade 4 4 2 2\n",
      "fae 3 3 2 1\n",
      "faebec 6 5 3 3\n",
      "faff 4 2 1 3\n",
      "faffcd 6 4 1 5\n",
      "fail 4 4 2 2\n",
      "failur 6 6 3 3\n",
      "fair 4 4 2 2\n",
      "fairfield 9 7 4 5\n",
      "fairmont 8 8 3 5\n",
      "faith 5 5 2 3\n",
      "fake 4 4 2 2\n",
      "fall 4 3 1 3\n",
      "fallen 6 5 2 4\n",
      "fallon 6 5 2 4\n",
      "fallout 7 6 3 4\n",
      "fallowfield 11 8 4 7\n",
      "fals 4 4 1 3\n",
      "famili 6 5 3 3\n",
      "famou 5 5 3 2\n",
      "fan 3 3 1 2\n",
      "faotnqi 7 7 3 4\n",
      "far 3 3 1 2\n",
      "farc 4 4 1 3\n",
      "fare 4 4 2 2\n",
      "farewel 7 6 3 4\n",
      "fargo 5 5 2 3\n",
      "farlam 6 5 2 4\n",
      "farm 4 4 1 3\n",
      "farmer 6 5 2 4\n",
      "farmington 10 9 3 7\n",
      "fascin 6 6 2 4\n",
      "fashion 7 7 3 4\n",
      "fast 4 4 1 3\n",
      "faster 6 6 2 4\n",
      "fastest 7 5 2 5\n",
      "fatal 5 4 2 3\n",
      "fate 4 4 2 2\n",
      "fateh 5 5 2 3\n",
      "father 6 6 2 4\n",
      "fault 5 5 2 3\n",
      "favela 6 5 3 3\n",
      "favor 5 5 2 3\n",
      "favorit 7 7 3 4\n",
      "faw 3 3 1 2\n",
      "fay 3 3 2 1\n",
      "fbaab 5 3 2 3\n",
      "fbadcab 7 5 2 5\n",
      "fbe 3 3 1 2\n",
      "fbeb 4 3 1 3\n",
      "fbebfa 6 4 2 4\n",
      "fbfa 4 3 1 3\n",
      "fbfadef 7 5 2 5\n",
      "fbffbe 6 3 1 5\n",
      "fbi 3 3 1 2\n",
      "fca 3 3 1 2\n",
      "fcaa 4 3 2 2\n",
      "fcab 4 4 1 3\n",
      "fcce 4 3 1 3\n",
      "fcdabb 6 5 1 5\n",
      "fcdfa 5 4 1 4\n",
      "fce 3 3 1 2\n",
      "fcea 4 4 2 2\n",
      "fcfe 4 3 1 3\n",
      "fda 3 3 1 2\n",
      "fdce 4 4 1 3\n",
      "fddde 5 3 1 4\n",
      "fde 3 3 1 2\n",
      "fe 2 2 1 1\n",
      "fea 3 3 2 1\n",
      "fear 4 4 2 2\n",
      "feasibl 7 7 3 4\n",
      "featur 6 6 3 3\n",
      "feb 3 3 1 2\n",
      "febcbdf 7 5 1 6\n",
      "fec 3 3 1 2\n",
      "feceb 5 4 2 3\n",
      "fed 3 3 1 2\n",
      "feda 4 4 2 2\n",
      "fedd 4 3 1 3\n",
      "feder 5 4 2 3\n",
      "fee 3 2 2 1\n",
      "feec 4 3 2 2\n",
      "feed 4 3 2 2\n",
      "feefbbc 7 4 2 5\n",
      "feel 4 3 2 2\n",
      "feet 4 3 2 2\n",
      "fefa 4 3 2 2\n",
      "fefcc 5 3 1 4\n",
      "fefdecf 7 4 2 5\n",
      "feinberg 8 7 3 5\n",
      "feinstein 9 6 4 5\n",
      "fell 4 3 1 3\n",
      "felon 5 5 2 3\n",
      "feloni 6 6 3 3\n",
      "felt 4 4 1 3\n",
      "femal 5 5 2 3\n",
      "femen 5 4 2 3\n",
      "feminist 8 7 3 5\n",
      "fenc 4 4 1 3\n",
      "fertil 6 6 2 4\n",
      "fervor 6 5 2 4\n",
      "festiv 6 6 2 4\n",
      "feud 4 4 2 2\n",
      "fewer 5 4 2 3\n",
      "ffa 3 2 1 2\n",
      "ffad 4 3 1 3\n",
      "ffafc 5 3 1 4\n",
      "ffafcb 6 4 1 5\n",
      "ffce 4 3 1 3\n",
      "ffe 3 2 1 2\n",
      "ffecfa 6 4 2 4\n",
      "ffede 5 3 2 3\n",
      "fhfa 4 3 1 3\n",
      "fhgyabsjjcosua 14 11 5 9\n",
      "fi 2 2 1 1\n",
      "fianc 5 5 2 3\n",
      "fiance 6 6 3 3\n",
      "fictiti 7 4 3 4\n",
      "field 5 5 2 3\n",
      "fielder 7 6 3 4\n",
      "fierc 5 5 2 3\n",
      "fiercer 7 5 3 4\n",
      "fiesta 6 6 3 3\n",
      "fife 4 3 2 2\n",
      "fight 5 5 1 4\n",
      "fightback 9 9 2 7\n",
      "fighter 7 7 2 5\n",
      "figur 5 5 2 3\n",
      "file 4 4 2 2\n",
      "filibust 8 7 3 5\n",
      "filipina 8 6 4 4\n",
      "fill 4 3 1 3\n",
      "film 4 4 1 3\n",
      "filwww 6 4 1 5\n",
      "fin 3 3 1 2\n",
      "final 5 5 2 3\n",
      "financ 6 5 2 4\n",
      "financi 7 5 3 4\n",
      "find 4 4 1 3\n",
      "fine 4 4 2 2\n",
      "finger 6 6 2 4\n",
      "finish 6 5 2 4\n",
      "finnish 7 5 2 5\n",
      "fire 4 4 2 2\n",
      "firearm 7 6 3 4\n",
      "firebomb 8 7 3 5\n",
      "firefight 9 7 3 6\n",
      "firm 4 4 1 3\n",
      "firmer 6 5 2 4\n",
      "first 5 5 1 4\n",
      "fisa 4 4 2 2\n",
      "fiscal 6 6 2 4\n",
      "fischer 7 7 2 5\n",
      "fish 4 4 1 3\n",
      "fist 4 4 1 3\n",
      "fit 3 3 1 2\n",
      "fitchburg 9 9 2 7\n",
      "fitow 5 5 2 3\n",
      "five 4 4 2 2\n",
      "fix 3 3 1 2\n",
      "fla 3 3 1 2\n",
      "flag 4 4 1 3\n",
      "flagstar 8 7 2 6\n",
      "flame 5 5 2 3\n",
      "flap 4 4 1 3\n",
      "flare 5 5 2 3\n",
      "flash 5 5 1 4\n",
      "flashpoint 10 10 3 7\n",
      "flat 4 4 1 3\n",
      "flaw 4 4 1 3\n",
      "flee 4 3 2 2\n",
      "fleet 5 4 2 3\n",
      "fli 3 3 1 2\n",
      "flickr 6 6 1 5\n",
      "flight 6 6 1 5\n",
      "flip 4 4 1 3\n",
      "flood 5 4 2 3\n",
      "floodwat 8 7 3 5\n",
      "floor 5 4 2 3\n",
      "florida 7 7 3 4\n",
      "floridian 9 8 4 5\n",
      "flossi 6 5 2 4\n",
      "flow 4 4 1 3\n",
      "flower 6 6 2 4\n",
      "flu 3 3 1 2\n",
      "fluke 5 5 2 3\n",
      "flvuyoafeuv 11 8 6 5\n",
      "fma 3 3 1 2\n",
      "fncynjr 7 6 1 6\n",
      "fnczkyc 7 6 1 6\n",
      "fnday 5 5 2 3\n",
      "fndirev 7 7 2 5\n",
      "fndocq 6 6 1 5\n",
      "fndpeo 6 6 2 4\n",
      "fneuzlbd 8 8 2 6\n",
      "fnhifq 6 5 1 5\n",
      "fnhjut 6 6 1 5\n",
      "fnhocxo 7 6 2 5\n",
      "fnhrvhol 8 7 1 7\n",
      "fniism 6 5 2 4\n",
      "fniisx 6 5 2 4\n",
      "fniithn 7 5 2 5\n",
      "fniittf 7 4 2 5\n",
      "fniivw 6 5 2 4\n",
      "fniiyv 6 5 3 3\n",
      "fnjtyy 6 5 2 4\n",
      "fnlicc 6 5 1 5\n",
      "fnvepb 6 6 1 5\n",
      "fo 2 2 1 1\n",
      "fob 3 3 1 2\n",
      "focu 4 4 2 2\n",
      "foe 3 3 2 1\n",
      "foil 4 4 2 2\n",
      "fold 4 4 1 3\n",
      "folk 4 4 1 3\n",
      "folli 5 4 2 3\n",
      "follow 6 4 2 4\n",
      "font 4 4 1 3\n",
      "fonterra 8 7 3 5\n",
      "food 4 3 2 2\n",
      "fool 4 3 2 2\n",
      "foot 4 3 2 2\n",
      "footag 6 5 3 3\n",
      "footprint 9 7 3 6\n",
      "forc 4 4 1 3\n",
      "ford 4 4 1 3\n",
      "forecast 8 8 3 5\n",
      "foreclosur 10 8 4 6\n",
      "foreign 7 7 3 4\n",
      "foreignminist 13 10 5 8\n",
      "foreman 7 7 3 4\n",
      "forens 6 6 2 4\n",
      "foreskin 8 8 3 5\n",
      "forest 6 6 2 4\n",
      "foretold 8 7 3 5\n",
      "forev 5 5 2 3\n",
      "forg 4 4 1 3\n",
      "forget 6 6 2 4\n",
      "forgotten 9 7 3 6\n",
      "fork 4 4 1 3\n",
      "forklift 8 7 2 6\n",
      "form 4 4 1 3\n",
      "format 6 6 2 4\n",
      "former 6 5 2 4\n",
      "formula 7 7 3 4\n",
      "fort 4 4 1 3\n",
      "forth 5 5 1 4\n",
      "forti 5 5 2 3\n",
      "forum 5 5 2 3\n",
      "forward 7 6 2 5\n",
      "fossil 6 5 2 4\n",
      "foster 6 6 2 4\n",
      "fought 6 6 2 4\n",
      "foul 4 4 2 2\n",
      "found 5 5 2 3\n",
      "foundat 7 7 3 4\n",
      "founder 7 7 3 4\n",
      "four 4 4 2 2\n",
      "foursquar 9 7 4 5\n",
      "fourstardav 11 9 4 7\n",
      "fourth 6 6 2 4\n",
      "fox 3 3 1 2\n",
      "foxx 4 3 1 3\n",
      "fraca 5 4 2 3\n",
      "frack 5 5 1 4\n",
      "fractur 7 6 2 5\n",
      "fragil 6 6 2 4\n",
      "frail 5 5 2 3\n",
      "frame 5 5 2 3\n",
      "fran 4 4 1 3\n",
      "franc 5 5 1 4\n",
      "franci 6 6 2 4\n",
      "francisco 9 8 3 6\n",
      "francoi 7 7 3 4\n",
      "frankfurt 9 7 2 7\n",
      "franko 6 6 2 4\n",
      "fraud 5 5 2 3\n",
      "fraudster 9 8 3 6\n",
      "free 4 3 2 2\n",
      "freed 5 4 2 3\n",
      "freedom 7 6 3 4\n",
      "freez 5 4 2 3\n",
      "freezer 7 4 3 4\n",
      "freight 7 7 2 5\n",
      "fremont 7 7 2 5\n",
      "french 6 6 1 5\n",
      "fresh 5 5 1 4\n",
      "freshmen 8 7 2 6\n",
      "fresno 6 6 2 4\n",
      "fret 4 4 1 3\n",
      "friar 5 4 2 3\n",
      "friction 8 7 3 5\n",
      "friday 6 6 3 3\n",
      "friend 6 6 2 4\n",
      "friendli 8 7 3 5\n",
      "friendliest 11 9 4 7\n",
      "frisco 6 6 2 4\n",
      "frogtown 8 7 2 6\n",
      "front 5 5 1 4\n",
      "frontpag 8 8 2 6\n",
      "frugal 6 6 2 4\n",
      "fruit 5 5 2 3\n",
      "frustrat 8 6 2 6\n",
      "ftvvghcdae 10 9 2 8\n",
      "fuel 4 4 2 2\n",
      "fuell 5 4 2 3\n",
      "fugit 5 5 2 3\n",
      "fuji 4 4 2 2\n",
      "fukushima 9 8 4 5\n",
      "fulfil 6 4 2 4\n",
      "full 4 3 1 3\n",
      "fulli 5 4 2 3\n",
      "fulltext 8 6 2 6\n",
      "function 8 7 3 5\n",
      "fund 4 4 1 3\n",
      "fundrais 8 8 3 5\n",
      "funer 5 5 2 3\n",
      "funni 5 4 2 3\n",
      "furi 4 4 2 2\n",
      "furlough 8 7 3 5\n",
      "furman 6 6 2 4\n",
      "furor 5 4 2 3\n",
      "furthur 7 5 2 5\n",
      "fuse 4 4 2 2\n",
      "futur 5 4 2 3\n",
      "ga 2 2 1 1\n",
      "gabbi 5 4 2 3\n",
      "gaddafi 7 5 3 4\n",
      "gadget 6 5 2 4\n",
      "gadhafi 7 6 3 4\n",
      "gain 4 4 2 2\n",
      "gainer 6 6 3 3\n",
      "gal 3 3 1 2\n",
      "gala 4 3 2 2\n",
      "galat 5 4 2 3\n",
      "galbraith 9 8 3 6\n",
      "galleri 7 6 3 4\n",
      "gallon 6 5 2 4\n",
      "gambit 6 6 2 4\n",
      "gambl 5 5 1 4\n",
      "game 4 4 2 2\n",
      "gamif 5 5 2 3\n",
      "gandolfini 10 8 4 6\n",
      "gang 4 3 1 3\n",
      "gangrap 7 5 2 5\n",
      "gangster 8 7 2 6\n",
      "ganja 5 4 2 3\n",
      "gansu 5 5 2 3\n",
      "gap 3 3 1 2\n",
      "garag 5 3 2 3\n",
      "garcetti 8 7 3 5\n",
      "garment 7 7 2 5\n",
      "garner 6 5 2 4\n",
      "gate 4 4 2 2\n",
      "gather 6 6 2 4\n",
      "gave 4 4 2 2\n",
      "gaxhjuni 8 8 3 5\n",
      "gay 3 3 2 1\n",
      "gaymarriag 10 6 5 5\n",
      "gayright 8 7 3 5\n",
      "gaz 3 3 1 2\n",
      "gaza 4 3 2 2\n",
      "gazett 6 5 2 4\n",
      "gcse 4 4 1 3\n",
      "ge 2 2 1 1\n",
      "gear 4 4 2 2\n",
      "geiss 5 4 2 3\n",
      "gen 3 3 1 2\n",
      "gender 6 5 2 4\n",
      "gene 4 3 2 2\n",
      "gener 5 4 2 3\n",
      "genet 5 4 2 3\n",
      "geneva 6 5 3 3\n",
      "genit 5 5 2 3\n",
      "geniu 5 5 3 2\n",
      "genoa 5 5 3 2\n",
      "genocid 7 7 3 4\n",
      "gensan 6 5 2 4\n",
      "genworth 8 8 2 6\n",
      "geograph 8 7 3 5\n",
      "geographi 9 8 4 5\n",
      "georg 5 4 2 3\n",
      "georgia 7 6 4 3\n",
      "german 6 6 2 4\n",
      "germani 7 7 3 4\n",
      "get 3 3 1 2\n",
      "gezi 4 4 2 2\n",
      "ghetto 6 5 2 4\n",
      "ghiz 4 4 1 3\n",
      "ghost 5 5 1 4\n",
      "gi 2 2 1 1\n",
      "giancarlo 9 8 4 5\n",
      "giant 5 5 2 3\n",
      "gibney 6 6 3 3\n",
      "gibraltar 9 7 3 6\n",
      "gif 3 3 1 2\n",
      "gifford 7 6 2 5\n",
      "gift 4 4 1 3\n",
      "gilani 6 5 3 3\n",
      "gilroy 6 6 3 3\n",
      "giraldi 7 6 3 4\n",
      "girl 4 4 1 3\n",
      "girlfriend 10 8 3 7\n",
      "gitmo 5 5 2 3\n",
      "give 4 4 2 2\n",
      "given 5 5 2 3\n",
      "gladli 6 5 2 4\n",
      "glanc 5 5 1 4\n",
      "glass 5 4 1 4\n",
      "glasshol 8 6 2 6\n",
      "glaxo 5 5 2 3\n",
      "glaze 5 5 2 3\n",
      "glencor 7 7 2 5\n",
      "glich 5 5 1 4\n",
      "glitch 6 6 1 5\n",
      "glivec 6 6 2 4\n",
      "global 6 5 2 4\n",
      "globe 5 5 2 3\n",
      "gmail 5 5 2 3\n",
      "go 2 2 1 1\n",
      "goal 4 4 2 2\n",
      "goe 3 3 2 1\n",
      "goer 4 4 2 2\n",
      "gogh 4 3 1 3\n",
      "golan 5 5 2 3\n",
      "gold 4 4 1 3\n",
      "goldman 7 7 2 5\n",
      "goldmansach 11 10 3 8\n",
      "gone 4 4 2 2\n",
      "goochland 9 8 3 6\n",
      "good 4 3 2 2\n",
      "goodby 6 5 3 3\n",
      "goodlatt 8 6 3 5\n",
      "goodman 7 6 3 4\n",
      "goodwil 7 6 3 4\n",
      "googl 5 3 2 3\n",
      "gop 3 3 1 2\n",
      "gore 4 4 2 2\n",
      "gorg 4 3 1 3\n",
      "gosnel 6 6 2 4\n",
      "got 3 3 1 2\n",
      "gov 3 3 1 2\n",
      "govern 6 6 2 4\n",
      "governmentt 11 8 3 8\n",
      "governor 8 6 3 5\n",
      "govt 4 4 1 3\n",
      "gqaenjrkjm 10 9 2 8\n",
      "grade 5 5 2 3\n",
      "graduat 7 6 3 4\n",
      "graft 5 5 1 4\n",
      "grafton 7 7 2 5\n",
      "grain 5 5 2 3\n",
      "graincorp 9 8 3 6\n",
      "grand 5 5 1 4\n",
      "grandfath 9 8 2 7\n",
      "grandmoth 9 9 2 7\n",
      "grandpar 8 6 2 6\n",
      "grandprix 9 8 2 7\n",
      "grandson 8 7 2 6\n",
      "grant 5 5 1 4\n",
      "graphic 7 7 2 5\n",
      "grate 5 5 2 3\n",
      "grave 5 5 2 3\n",
      "great 5 5 2 3\n",
      "greater 7 5 3 4\n",
      "greec 5 4 2 3\n",
      "greed 5 4 2 3\n",
      "greek 5 4 2 3\n",
      "greeley 7 5 4 3\n",
      "green 5 4 2 3\n",
      "greenest 8 6 3 5\n",
      "greenmountain 13 10 6 7\n",
      "greenpeac 9 7 4 5\n",
      "greet 5 4 2 3\n",
      "gregori 7 5 3 4\n",
      "greyhound 9 9 4 5\n",
      "grid 4 4 1 3\n",
      "grill 5 4 1 4\n",
      "grimm 5 4 1 4\n",
      "grip 4 4 1 3\n",
      "grisli 6 5 2 4\n",
      "grope 5 5 2 3\n",
      "ground 6 6 2 4\n",
      "groundbreak 11 10 4 7\n",
      "groundwat 9 9 3 6\n",
      "group 5 5 2 3\n",
      "grow 4 4 1 3\n",
      "grower 6 5 2 4\n",
      "growth 6 6 1 5\n",
      "grudg 5 4 1 4\n",
      "guam 4 4 2 2\n",
      "guangcheng 10 7 3 7\n",
      "guantanamo 10 7 5 5\n",
      "guarante 8 7 4 4\n",
      "guard 5 5 2 3\n",
      "guardian 8 7 4 4\n",
      "guatemala 9 7 5 4\n",
      "guerrila 8 7 4 4\n",
      "guerrilla 9 7 4 5\n",
      "guest 5 5 2 3\n",
      "guevara 7 6 4 3\n",
      "guid 4 4 2 2\n",
      "guidanc 7 7 3 4\n",
      "guilti 6 5 3 3\n",
      "guinea 6 6 4 2\n",
      "gujarat 7 6 3 4\n",
      "gulf 4 4 1 3\n",
      "gulfport 8 8 2 6\n",
      "gun 3 3 1 2\n",
      "gunfir 6 6 2 4\n",
      "gunman 6 5 2 4\n",
      "gunmen 6 5 2 4\n",
      "gurman 6 6 2 4\n",
      "gurney 6 6 3 3\n",
      "gust 4 4 1 3\n",
      "guthri 6 6 2 4\n",
      "guy 3 3 2 1\n",
      "gyhpijkecpfow 13 12 4 9\n",
      "gypsi 5 5 2 3\n",
      "hack 4 4 1 3\n",
      "hacker 6 6 2 4\n",
      "hackingbr 9 9 2 7\n",
      "hagel 5 5 2 3\n",
      "hagu 4 4 2 2\n",
      "hail 4 4 2 2\n",
      "hair 4 4 2 2\n",
      "haiti 5 4 3 2\n",
      "haj 3 3 1 2\n",
      "hajj 4 3 1 3\n",
      "half 4 4 1 3\n",
      "hall 4 3 1 3\n",
      "halliburton 11 10 4 7\n",
      "halt 4 4 1 3\n",
      "hama 4 3 2 2\n",
      "hamdallah 9 5 3 6\n",
      "hamid 5 5 2 3\n",
      "hamilton 8 8 3 5\n",
      "hammond 7 6 2 5\n",
      "hamper 6 6 2 4\n",
      "hampton 7 7 2 5\n",
      "hand 4 4 1 3\n",
      "handcuf 7 7 2 5\n",
      "handl 5 5 1 4\n",
      "handoff 7 6 2 5\n",
      "handov 6 6 2 4\n",
      "hang 4 4 1 3\n",
      "hangu 5 5 2 3\n",
      "hannah 6 3 2 4\n",
      "hansen 6 5 2 4\n",
      "happ 4 3 1 3\n",
      "happen 6 5 2 4\n",
      "happi 5 4 2 3\n",
      "haqqani 7 5 3 4\n",
      "har 3 3 1 2\n",
      "haram 5 4 2 3\n",
      "hard 4 4 1 3\n",
      "harden 6 6 2 4\n",
      "hardest 7 7 2 5\n",
      "harm 4 4 1 3\n",
      "harri 5 4 2 3\n",
      "harrow 6 5 2 4\n",
      "harsher 7 5 2 5\n",
      "hartford 8 7 2 6\n",
      "hartmay 7 6 3 4\n",
      "harvey 6 6 3 3\n",
      "hasan 5 4 2 3\n",
      "hasbro 6 6 2 4\n",
      "hasch 5 4 1 4\n",
      "hashana 7 4 3 4\n",
      "hashtag 7 5 2 5\n",
      "hassan 6 4 2 4\n",
      "hassen 6 5 2 4\n",
      "hast 4 4 1 3\n",
      "hat 3 3 1 2\n",
      "hatch 5 4 1 4\n",
      "hate 4 4 2 2\n",
      "hatr 4 4 1 3\n",
      "haul 4 4 2 2\n",
      "haven 5 5 2 3\n",
      "havent 6 6 2 4\n",
      "hawaii 6 4 4 2\n",
      "hawk 4 4 1 3\n",
      "haytarma 8 6 4 4\n",
      "hazard 6 5 2 4\n",
      "haze 4 4 2 2\n",
      "hazel 5 5 2 3\n",
      "hbo 3 3 1 2\n",
      "he 2 2 1 1\n",
      "head 4 4 2 2\n",
      "headach 7 5 3 4\n",
      "headphon 8 7 3 5\n",
      "headscarf 9 8 3 6\n",
      "heal 4 4 2 2\n",
      "health 6 5 2 4\n",
      "healthbeat 10 6 4 6\n",
      "healthcar 9 7 3 6\n",
      "healthcast 10 7 3 7\n",
      "healthi 7 6 3 4\n",
      "heaney 6 5 4 2\n",
      "hear 4 4 2 2\n",
      "heard 5 5 2 3\n",
      "heart 5 5 2 3\n",
      "heartbreak 10 7 4 6\n",
      "heartland 9 8 3 6\n",
      "heat 4 4 2 2\n",
      "heater 6 5 3 3\n",
      "heathrow 8 7 3 5\n",
      "heavi 5 5 3 2\n",
      "heavilyarm 10 9 5 5\n",
      "hebron 6 6 2 4\n",
      "hec 3 3 1 2\n",
      "heckl 5 5 1 4\n",
      "heckler 7 6 2 5\n",
      "hedg 4 4 1 3\n",
      "hedgefund 9 7 3 6\n",
      "height 6 5 2 4\n",
      "heighten 8 6 3 5\n",
      "heinz 5 5 2 3\n",
      "heir 4 4 2 2\n",
      "heist 5 5 2 3\n",
      "held 4 4 1 3\n",
      "helen 5 4 2 3\n",
      "helicopt 8 8 3 5\n",
      "hell 4 3 1 3\n",
      "helmet 6 5 2 4\n",
      "help 4 4 1 3\n",
      "hemp 4 4 1 3\n",
      "hempfest 8 7 2 6\n",
      "henderson 9 7 3 6\n",
      "henni 5 4 2 3\n",
      "henryk 6 6 2 4\n",
      "herald 6 6 2 4\n",
      "herbalif 8 8 3 5\n",
      "here 4 3 2 2\n",
      "heritag 7 7 3 4\n",
      "hero 4 4 2 2\n",
      "heroin 6 6 3 3\n",
      "herrera 7 4 3 4\n",
      "hesit 5 5 2 3\n",
      "hewlett 7 5 2 5\n",
      "hezbollah 9 7 3 6\n",
      "hidden 6 5 2 4\n",
      "hide 4 4 2 2\n",
      "hideout 7 7 4 3\n",
      "higg 4 3 1 3\n",
      "high 4 3 1 3\n",
      "higher 6 5 2 4\n",
      "highest 7 6 2 5\n",
      "highlight 9 5 2 7\n",
      "highway 7 6 3 4\n",
      "hijab 5 5 2 3\n",
      "hijack 6 6 2 4\n",
      "hike 4 4 2 2\n",
      "hill 4 3 1 3\n",
      "hillari 7 5 3 4\n",
      "himach 6 5 2 4\n",
      "hinder 6 6 2 4\n",
      "hindu 5 5 2 3\n",
      "hing 4 4 1 3\n",
      "hint 4 4 1 3\n",
      "hip 3 3 1 2\n",
      "hippi 5 3 2 3\n",
      "hire 4 4 2 2\n",
      "hiroshima 9 7 4 5\n",
      "hispan 6 6 2 4\n",
      "histor 6 6 2 4\n",
      "histori 7 6 3 4\n",
      "hit 3 3 1 2\n",
      "hitch 5 4 1 4\n",
      "hitler 6 6 2 4\n",
      "hiv 3 3 1 2\n",
      "hizbollah 9 7 3 6\n",
      "hizbullah 9 7 3 6\n",
      "ho 2 2 1 1\n",
      "hoax 4 4 2 2\n",
      "hobart 6 6 2 4\n",
      "hoffa 5 4 2 3\n",
      "hoffman 7 6 2 5\n",
      "hogan 5 5 2 3\n",
      "hola 4 4 2 2\n",
      "hold 4 4 1 3\n",
      "holder 6 6 2 4\n",
      "holdout 7 6 3 4\n",
      "hole 4 4 2 2\n",
      "holi 4 4 2 2\n",
      "holiday 7 7 4 3\n",
      "holland 7 6 2 5\n",
      "hollow 6 4 2 4\n",
      "hollywood 9 6 4 5\n",
      "holm 4 4 1 3\n",
      "holocaust 9 8 4 5\n",
      "holyhour 8 6 4 4\n",
      "hom 3 3 1 2\n",
      "home 4 4 2 2\n",
      "homeaway 8 7 5 3\n",
      "homebuy 7 7 4 3\n",
      "homeland 8 8 3 5\n",
      "homeless 8 6 3 5\n",
      "homemad 7 6 3 4\n",
      "homicid 7 6 3 4\n",
      "homosexu 8 7 4 4\n",
      "hondura 7 7 3 4\n",
      "honduran 8 7 3 5\n",
      "honest 6 6 2 4\n",
      "honey 5 5 3 2\n",
      "honeybe 7 6 4 3\n",
      "honeymoon 9 6 5 4\n",
      "hong 4 4 1 3\n",
      "hongkkong 9 5 2 7\n",
      "hongkong 8 5 2 6\n",
      "honolulu 8 5 4 4\n",
      "honor 5 4 2 3\n",
      "honorari 8 6 4 4\n",
      "honour 6 5 3 3\n",
      "hood 4 3 2 2\n",
      "hooligan 8 7 4 4\n",
      "hoop 4 3 2 2\n",
      "hop 3 3 1 2\n",
      "hope 4 4 2 2\n",
      "hopi 4 4 2 2\n",
      "hopkin 6 6 2 4\n",
      "horn 4 4 1 3\n",
      "horror 6 3 2 4\n",
      "hors 4 4 1 3\n",
      "horsemeat 9 8 4 5\n",
      "hosni 5 5 2 3\n",
      "hospit 6 6 2 4\n",
      "host 4 4 1 3\n",
      "hostag 6 6 2 4\n",
      "hostess 7 5 2 5\n",
      "hot 3 3 1 2\n",
      "hotel 5 5 2 3\n",
      "hotshot 7 4 2 5\n",
      "hour 4 4 2 2\n",
      "hous 4 4 2 2\n",
      "houston 7 6 3 4\n",
      "howard 6 6 2 4\n",
      "hoyal 5 5 3 2\n",
      "hryschenko 10 9 3 7\n",
      "huawei 6 6 4 2\n",
      "hub 3 3 1 2\n",
      "hubbi 5 4 2 3\n",
      "huddl 5 4 1 4\n",
      "hudson 6 6 2 4\n",
      "huffington 10 8 3 7\n",
      "hug 3 3 1 2\n",
      "huge 4 4 2 2\n",
      "hugo 4 4 2 2\n",
      "hulk 4 4 1 3\n",
      "hum 3 3 1 2\n",
      "human 5 5 2 3\n",
      "humanitarian 12 8 6 6\n",
      "humili 6 5 3 3\n",
      "hummingbird 11 9 3 8\n",
      "hundr 5 5 1 4\n",
      "hungari 7 7 3 4\n",
      "hungarian 9 7 4 5\n",
      "hunger 6 6 2 4\n",
      "hungri 6 6 2 4\n",
      "hunt 4 4 1 3\n",
      "hunter 6 6 2 4\n",
      "hurdl 5 5 1 4\n",
      "hurrican 8 7 3 5\n",
      "hurricans 9 8 3 6\n",
      "hurt 4 4 1 3\n",
      "husband 7 7 2 5\n",
      "hut 3 3 1 2\n",
      "hyde 4 4 2 2\n",
      "hydraul 7 7 3 4\n",
      "hydrocodon 10 7 4 6\n",
      "hypocrisi 9 8 4 5\n",
      "hyundai 7 7 4 3\n",
      "ia 2 2 2 0\n",
      "iaea 4 3 4 0\n",
      "iaeoklgupuphfh 14 11 6 8\n",
      "ibrahim 7 6 3 4\n",
      "icac 4 3 2 2\n",
      "icahn 5 5 2 3\n",
      "icc 3 2 1 2\n",
      "ice 3 3 2 1\n",
      "iceland 7 7 3 4\n",
      "icon 4 4 2 2\n",
      "id 2 2 1 1\n",
      "idaho 5 5 3 2\n",
      "idahoan 7 6 4 3\n",
      "idd 3 2 1 2\n",
      "idea 4 4 3 1\n",
      "ideal 5 5 3 2\n",
      "ident 5 5 2 3\n",
      "identifi 8 6 4 4\n",
      "idinbr 6 5 2 4\n",
      "idinbreac 9 8 4 5\n",
      "idinbrear 9 7 4 5\n",
      "idinbreb 8 6 3 5\n",
      "idinbrebb 9 6 3 6\n",
      "idinbrebvn 10 7 3 7\n",
      "idinbrecx 9 8 3 6\n",
      "idinbreef 9 7 4 5\n",
      "idinbreeh 9 7 4 5\n",
      "idinbref 8 7 3 5\n",
      "idinbreg 8 7 3 5\n",
      "idinbregfk 10 9 3 7\n",
      "idinbreh 8 7 3 5\n",
      "idinbreln 9 7 3 6\n",
      "idinbrelv 9 8 3 6\n",
      "idinbremi 9 7 4 5\n",
      "idinbremu 9 8 4 5\n",
      "idinbremx 9 8 3 6\n",
      "idinbresoj 10 9 4 6\n",
      "idinbreui 9 7 5 4\n",
      "idinbreuv 9 8 4 5\n",
      "idinbreva 9 8 4 5\n",
      "idinbrey 8 7 4 4\n",
      "idinbreyb 9 7 4 5\n",
      "idinbrez 8 7 3 5\n",
      "idind 5 3 2 3\n",
      "idinln 6 4 2 4\n",
      "idinlndkzm 10 7 2 8\n",
      "idinlndmw 9 6 2 7\n",
      "idinlndn 8 4 2 6\n",
      "idinlnean 9 6 4 5\n",
      "idinlnear 9 7 4 5\n",
      "idinlneaw 9 7 4 5\n",
      "idinlnebdo 10 7 4 6\n",
      "idinlnebgi 10 7 4 6\n",
      "idinlnebxi 10 7 4 6\n",
      "idinlnedu 9 6 4 5\n",
      "idinlneev 9 6 4 5\n",
      "idinlneh 8 6 3 5\n",
      "idinlnehb 9 7 3 6\n",
      "idinlneiy 9 6 5 4\n",
      "idinlnej 8 6 3 5\n",
      "idinlnekjd 10 7 3 7\n",
      "idinlnel 8 5 3 5\n",
      "idinlnelmp 10 7 3 7\n",
      "idinlnelpq 10 7 3 7\n",
      "idinlnemff 10 7 3 7\n",
      "idinlnemi 9 6 4 5\n",
      "idinlnen 8 5 3 5\n",
      "idinlnenjb 10 7 3 7\n",
      "idinlnep 8 6 3 5\n",
      "idinlnept 9 7 3 6\n",
      "idinlneqki 10 7 4 6\n",
      "idinlnetkv 10 8 3 7\n",
      "idinlnezd 9 6 3 6\n",
      "idinlnhjq 9 7 2 7\n",
      "idinlnhl 8 5 2 6\n",
      "idinlnhx 8 6 2 6\n",
      "idinlnhzv 9 7 2 7\n",
      "idinlniahq 10 7 4 6\n",
      "idinlnicv 9 6 3 6\n",
      "idinlnidb 9 5 3 6\n",
      "iea 3 3 3 0\n",
      "ifprdjzhatro 12 11 3 9\n",
      "igat 4 4 2 2\n",
      "ignor 5 5 2 3\n",
      "iio 3 2 3 0\n",
      "ike 3 3 2 1\n",
      "ikea 4 4 3 1\n",
      "il 2 2 1 1\n",
      "ill 3 2 1 2\n",
      "illeg 5 4 2 3\n",
      "illinoi 7 4 4 3\n",
      "illus 5 4 2 3\n",
      "illustr 7 6 2 5\n",
      "im 2 2 1 1\n",
      "imag 4 4 2 2\n",
      "imf 3 3 1 2\n",
      "immedi 6 4 3 3\n",
      "immigr 6 4 2 4\n",
      "immin 5 3 2 3\n",
      "immol 5 4 2 3\n",
      "immun 5 4 2 3\n",
      "impact 6 6 2 4\n",
      "impal 5 5 2 3\n",
      "impala 6 5 3 3\n",
      "impass 6 5 2 4\n",
      "implant 7 7 2 5\n",
      "implement 9 7 3 6\n",
      "implic 6 5 2 4\n",
      "import 6 6 2 4\n",
      "impos 5 5 2 3\n",
      "impot 5 5 2 3\n",
      "imprison 8 7 3 5\n",
      "improp 6 5 2 4\n",
      "improv 6 6 2 4\n",
      "impuls 6 6 2 4\n",
      "imxnbevapp 10 9 3 7\n",
      "in 2 2 1 1\n",
      "inact 5 5 2 3\n",
      "inadequ 7 7 4 3\n",
      "inaugur 7 6 4 3\n",
      "inc 3 3 1 2\n",
      "incest 6 6 2 4\n",
      "inch 4 4 1 3\n",
      "incid 5 4 2 3\n",
      "includ 6 6 2 4\n",
      "incom 5 5 2 3\n",
      "increas 7 7 3 4\n",
      "increasingli 12 9 5 7\n",
      "incrit 6 5 2 4\n",
      "incurs 6 6 2 4\n",
      "ind 3 3 1 2\n",
      "indecis 7 6 3 4\n",
      "independ 8 5 3 5\n",
      "index 5 5 2 3\n",
      "indi 4 3 2 2\n",
      "india 5 4 3 2\n",
      "indian 6 4 3 3\n",
      "indiana 7 4 4 3\n",
      "indic 5 4 2 3\n",
      "indict 6 5 2 4\n",
      "indo 4 4 2 2\n",
      "indonesia 9 7 5 4\n",
      "indonesian 10 7 5 5\n",
      "induc 5 5 2 3\n",
      "industri 8 7 3 5\n",
      "indyk 5 5 2 3\n",
      "inexcus 7 7 3 4\n",
      "infect 6 6 2 4\n",
      "inferno 7 6 3 4\n",
      "influenc 8 7 3 5\n",
      "info 4 4 2 2\n",
      "inform 6 6 2 4\n",
      "infosi 6 5 3 3\n",
      "infrastructur 13 9 4 9\n",
      "ingrid 6 5 2 4\n",
      "inhof 5 5 2 3\n",
      "initi 5 3 3 2\n",
      "initiat 7 4 4 3\n",
      "inject 6 6 2 4\n",
      "injur 5 5 2 3\n",
      "injuri 6 5 3 3\n",
      "injustic 8 7 3 5\n",
      "ink 3 3 1 2\n",
      "inmat 5 5 2 3\n",
      "innoc 5 4 2 3\n",
      "innov 5 4 2 3\n",
      "input 5 5 2 3\n",
      "inquiri 7 5 4 3\n",
      "insan 5 4 2 3\n",
      "insid 5 4 2 3\n",
      "insight 7 6 2 5\n",
      "insist 6 4 2 4\n",
      "inspect 7 7 2 5\n",
      "inspector 9 9 3 6\n",
      "inspir 6 5 2 4\n",
      "instal 6 6 2 4\n",
      "instant 7 5 2 5\n",
      "institut 8 5 3 5\n",
      "instructor 10 8 3 7\n",
      "insult 6 6 2 4\n",
      "insur 5 5 2 3\n",
      "insurg 6 6 2 4\n",
      "integr 6 6 2 4\n",
      "intel 5 5 2 3\n",
      "intellig 8 6 3 5\n",
      "intensifi 9 6 4 5\n",
      "intent 6 4 2 4\n",
      "inter 5 5 2 3\n",
      "interc 6 6 2 4\n",
      "intercept 9 7 3 6\n",
      "interest 8 6 3 5\n",
      "interfer 8 6 3 5\n",
      "interim 7 6 3 4\n",
      "interior 8 6 4 4\n",
      "intern 6 5 2 4\n",
      "internet 8 5 3 5\n",
      "interpol 8 8 3 5\n",
      "interrog 8 7 3 5\n",
      "intervent 9 6 3 6\n",
      "interview 9 7 4 5\n",
      "intoler 7 7 3 4\n",
      "introduc 8 8 3 5\n",
      "introduct 9 8 3 6\n",
      "intrud 6 6 2 4\n",
      "intrus 6 6 2 4\n",
      "inuvik 6 5 3 3\n",
      "invas 5 5 2 3\n",
      "invest 6 6 2 4\n",
      "investig 8 7 3 5\n",
      "investigati 11 8 5 6\n",
      "investor 8 8 3 5\n",
      "invit 5 4 2 3\n",
      "involv 6 5 2 4\n",
      "inyemen 7 5 4 3\n",
      "ion 3 3 2 1\n",
      "iowa 4 4 3 1\n",
      "ip 2 2 1 1\n",
      "ipad 4 4 2 2\n",
      "iphon 5 5 2 3\n",
      "ipo 3 3 2 1\n",
      "iqama 5 4 3 2\n",
      "ir 2 2 1 1\n",
      "iran 4 4 2 2\n",
      "iranian 7 4 4 3\n",
      "iraq 4 4 2 2\n",
      "iraqi 5 4 3 2\n",
      "ire 3 3 2 1\n",
      "ireland 7 7 3 4\n",
      "irish 5 4 2 3\n",
      "irishman 8 7 3 5\n",
      "iron 4 4 2 2\n",
      "ironi 5 4 3 2\n",
      "irrefut 7 6 3 4\n",
      "irrepress 9 5 3 6\n",
      "irrit 5 3 2 3\n",
      "irsbrefi 8 6 3 5\n",
      "irss 4 3 1 3\n",
      "irvin 5 4 2 3\n",
      "isdktgidhotab 13 10 4 9\n",
      "ish 3 3 1 2\n",
      "ishrat 6 6 2 4\n",
      "isl 3 3 1 2\n",
      "islam 5 5 2 3\n",
      "islamabad 9 7 4 5\n",
      "islamist 8 6 3 5\n",
      "islamistsbrel 13 9 4 9\n",
      "island 6 6 2 4\n",
      "isol 4 4 2 2\n",
      "isra 4 4 2 2\n",
      "israel 6 6 3 3\n",
      "issa 4 3 2 2\n",
      "issu 4 3 2 2\n",
      "istanbul 8 8 3 5\n",
      "itali 5 4 3 2\n",
      "italian 7 5 4 3\n",
      "item 4 4 2 2\n",
      "iuzbkteo 8 8 4 4\n",
      "ive 3 3 2 1\n",
      "iwf 3 3 1 2\n",
      "ix 2 2 1 1\n",
      "izhvi 5 4 2 3\n",
      "jackpot 7 7 2 5\n",
      "jackson 7 7 2 5\n",
      "jail 4 4 2 2\n",
      "jailbreak 9 8 4 5\n",
      "jal 3 3 1 2\n",
      "jalalabad 9 5 4 5\n",
      "jalili 6 4 3 3\n",
      "jamaica 7 5 4 3\n",
      "jamali 6 5 3 3\n",
      "jamalyan 8 6 4 4\n",
      "jame 4 4 2 2\n",
      "jamestown 9 9 3 6\n",
      "jami 4 4 2 2\n",
      "jan 3 3 1 2\n",
      "janet 5 5 2 3\n",
      "japan 5 4 2 3\n",
      "japanes 7 6 3 4\n",
      "jasmin 6 6 2 4\n",
      "jasper 6 6 2 4\n",
      "jatind 6 6 2 4\n",
      "jazz 4 3 1 3\n",
      "jdsamplead 10 8 3 7\n",
      "jdu 3 3 1 2\n",
      "jeb 3 3 1 2\n",
      "jeep 4 3 2 2\n",
      "jeff 4 3 1 3\n",
      "jensen 6 4 2 4\n",
      "jeopard 7 7 3 4\n",
      "jerejak 7 5 3 4\n",
      "jerri 5 4 2 3\n",
      "jersey 6 5 3 3\n",
      "jerusalem 9 8 4 5\n",
      "jess 4 3 1 3\n",
      "jesu 4 4 2 2\n",
      "jet 3 3 1 2\n",
      "jew 3 3 1 2\n",
      "jewel 5 4 2 3\n",
      "jewish 6 6 2 4\n",
      "jfek 4 4 1 3\n",
      "ji 2 2 1 1\n",
      "jihadi 6 5 3 3\n",
      "jill 4 3 1 3\n",
      "jimmi 5 3 2 3\n",
      "jive 4 4 2 2\n",
      "joan 4 4 2 2\n",
      "job 3 3 1 2\n",
      "jobless 7 6 2 5\n",
      "jodi 4 4 2 2\n",
      "jodiaria 8 6 5 3\n",
      "joe 3 3 2 1\n",
      "jofi 4 4 2 2\n",
      "john 4 4 1 3\n",
      "johnson 7 5 2 5\n",
      "join 4 4 2 2\n",
      "joint 5 5 2 3\n",
      "jointli 7 6 3 4\n",
      "joke 4 4 2 2\n",
      "joli 4 4 2 2\n",
      "jollota 7 5 3 4\n",
      "jolt 4 4 1 3\n",
      "jon 3 3 1 2\n",
      "jone 4 4 2 2\n",
      "jordan 6 6 2 4\n",
      "jose 4 4 2 2\n",
      "joseph 6 6 2 4\n",
      "josephin 8 8 3 5\n",
      "journalist 10 10 4 6\n",
      "journey 7 7 4 3\n",
      "journo 6 5 3 3\n",
      "jovx 4 4 1 3\n",
      "joy 3 3 2 1\n",
      "jpmorgan 8 8 2 6\n",
      "jsxmhnyk 8 8 1 7\n",
      "judg 4 4 1 3\n",
      "judici 6 5 3 3\n",
      "judiciari 9 7 5 4\n",
      "judith 6 6 2 4\n",
      "juic 4 4 2 2\n",
      "juli 4 4 2 2\n",
      "jump 4 4 1 3\n",
      "june 4 4 2 2\n",
      "junk 4 4 1 3\n",
      "jupit 5 5 2 3\n",
      "juri 4 4 2 2\n",
      "juror 5 4 2 3\n",
      "justic 6 6 2 4\n",
      "justifi 7 6 3 4\n",
      "juvenil 7 7 3 4\n",
      "jyehbgax 8 8 3 5\n",
      "kabul 5 5 2 3\n",
      "kaesong 7 7 3 4\n",
      "kahn 4 4 1 3\n",
      "kain 4 4 2 2\n",
      "kalat 5 4 2 3\n",
      "kali 4 4 2 2\n",
      "kan 3 3 1 2\n",
      "kangaroo 8 6 4 4\n",
      "kansa 5 4 2 3\n",
      "kany 4 4 2 2\n",
      "karachi 7 6 3 4\n",
      "kardashian 10 8 4 6\n",
      "karen 5 5 2 3\n",
      "kari 4 4 2 2\n",
      "karman 6 5 2 4\n",
      "karo 4 4 2 2\n",
      "karzai 6 5 3 3\n",
      "kashmir 7 7 2 5\n",
      "kasur 5 5 2 3\n",
      "katami 6 5 3 3\n",
      "kate 4 4 2 2\n",
      "katharin 8 7 3 5\n",
      "kati 4 4 2 2\n",
      "kauffman 8 6 3 5\n",
      "kaufman 7 6 3 4\n",
      "kazakh 6 4 2 4\n",
      "kazakhstan 10 7 3 7\n",
      "keanu 5 5 3 2\n",
      "kedah 5 5 2 3\n",
      "keen 4 3 2 2\n",
      "keep 4 3 2 2\n",
      "keir 4 4 2 2\n",
      "keita 5 5 3 2\n",
      "kenduri 7 7 3 4\n",
      "kennedi 7 5 3 4\n",
      "kenneth 7 5 2 5\n",
      "kent 4 4 1 3\n",
      "kentucki 8 7 3 5\n",
      "kenya 5 5 3 2\n",
      "kenyan 6 5 3 3\n",
      "kenyon 6 5 3 3\n",
      "kerala 6 5 3 3\n",
      "kercher 7 5 2 5\n",
      "kermit 6 6 2 4\n",
      "kerri 5 4 2 3\n",
      "key 3 3 2 1\n",
      "keynot 6 6 3 3\n",
      "keyston 7 7 3 4\n",
      "khan 4 4 1 3\n",
      "khmer 5 5 1 4\n",
      "khodorkovski 12 8 4 8\n",
      "khoso 5 4 2 3\n",
      "khurshid 8 7 2 6\n",
      "ki 2 2 1 1\n",
      "kick 4 3 1 3\n",
      "kickback 8 5 2 6\n",
      "kid 3 3 1 2\n",
      "kidal 5 5 2 3\n",
      "kidnap 6 6 2 4\n",
      "kidnapp 7 6 2 5\n",
      "kidney 6 6 3 3\n",
      "kill 4 3 1 3\n",
      "killer 6 5 2 4\n",
      "killingbreruj 13 10 4 9\n",
      "kim 3 3 1 2\n",
      "kimberli 8 7 3 5\n",
      "kind 4 4 1 3\n",
      "king 4 4 1 3\n",
      "kingman 7 6 2 5\n",
      "kingpin 7 5 2 5\n",
      "kior 4 4 2 2\n",
      "kiribati 8 6 4 4\n",
      "kirk 4 3 1 3\n",
      "kiss 4 3 1 3\n",
      "kitchen 7 7 2 5\n",
      "klaw 4 4 1 3\n",
      "kleyn 5 5 2 3\n",
      "knew 4 4 1 3\n",
      "knick 5 4 1 4\n",
      "knicker 7 6 2 5\n",
      "knit 4 4 1 3\n",
      "knock 5 4 1 4\n",
      "knot 4 4 1 3\n",
      "know 4 4 1 3\n",
      "known 5 4 1 4\n",
      "knox 4 4 1 3\n",
      "kohlschreib 11 10 3 8\n",
      "koirala 7 6 4 3\n",
      "kolar 5 5 2 3\n",
      "komar 5 5 2 3\n",
      "kong 4 4 1 3\n",
      "korea 5 5 3 2\n",
      "korean 6 6 3 3\n",
      "kosovo 6 4 3 3\n",
      "kozhara 7 6 3 4\n",
      "krauthamm 9 7 3 6\n",
      "kremlin 7 7 2 5\n",
      "ksa 3 3 1 2\n",
      "ktkheevpdi 10 8 3 7\n",
      "kudarat 7 6 3 4\n",
      "kumar 5 5 2 3\n",
      "kunm 4 4 1 3\n",
      "kurd 4 4 1 3\n",
      "kurdistan 9 9 3 6\n",
      "kuwait 6 6 3 3\n",
      "kuwaiti 7 6 4 3\n",
      "ky 2 2 1 1\n",
      "kyi 3 3 2 1\n",
      "kyiv 4 4 2 2\n",
      "kyivan 6 6 3 3\n",
      "kyrgyzstan 10 9 3 7\n",
      "la 2 2 1 1\n",
      "lab 3 3 1 2\n",
      "label 5 4 2 3\n",
      "labor 5 5 2 3\n",
      "labour 6 6 3 3\n",
      "lac 3 3 1 2\n",
      "lace 4 4 2 2\n",
      "lack 4 4 1 3\n",
      "laden 5 5 2 3\n",
      "ladi 4 4 2 2\n",
      "lagard 6 5 2 4\n",
      "lago 4 4 2 2\n",
      "laguardia 9 7 5 4\n",
      "lahad 5 4 2 3\n",
      "lahor 5 5 2 3\n",
      "laid 4 4 2 2\n",
      "lake 4 4 2 2\n",
      "lakehead 8 6 4 4\n",
      "lakeland 8 6 3 5\n",
      "lakshmi 7 7 2 5\n",
      "lamb 4 4 1 3\n",
      "lamborn 7 7 2 5\n",
      "lampedusa 9 8 4 5\n",
      "land 4 4 1 3\n",
      "landlord 8 6 2 6\n",
      "landmark 8 7 2 6\n",
      "landmin 7 6 2 5\n",
      "landscap 8 7 2 6\n",
      "landslid 8 6 2 6\n",
      "lane 4 4 2 2\n",
      "langdon 7 6 2 5\n",
      "languag 7 5 3 4\n",
      "lanka 5 4 2 3\n",
      "lantern 7 6 2 5\n",
      "lao 3 3 2 1\n",
      "lapgat 6 5 2 4\n",
      "laptop 6 5 2 4\n",
      "larg 4 4 1 3\n",
      "larger 6 5 2 4\n",
      "largest 7 7 2 5\n",
      "larri 5 4 2 3\n",
      "lash 4 4 1 3\n",
      "lashio 6 6 3 3\n",
      "last 4 4 1 3\n",
      "late 4 4 2 2\n",
      "later 5 5 2 3\n",
      "latest 6 5 2 4\n",
      "latin 5 5 2 3\n",
      "latourett 9 7 4 5\n",
      "launch 6 6 2 4\n",
      "launcher 8 8 3 5\n",
      "launder 7 7 3 4\n",
      "lautenberg 10 9 4 6\n",
      "lavrov 6 5 2 4\n",
      "law 3 3 1 2\n",
      "lawmak 6 5 2 4\n",
      "lawndal 7 5 2 5\n",
      "lawsuit 7 7 3 4\n",
      "lawyer 6 6 3 3\n",
      "lax 3 3 1 2\n",
      "lay 3 3 2 1\n",
      "layoff 6 5 3 3\n",
      "le 2 2 1 1\n",
      "lea 3 3 2 1\n",
      "lead 4 4 2 2\n",
      "leader 6 5 3 3\n",
      "leadership 10 9 4 6\n",
      "leak 4 4 2 2\n",
      "leaker 6 5 3 3\n",
      "leaki 5 5 3 2\n",
      "leap 4 4 2 2\n",
      "learn 5 5 2 3\n",
      "learnt 6 6 2 4\n",
      "leas 4 4 2 2\n",
      "leash 5 5 2 3\n",
      "least 5 5 2 3\n",
      "leav 4 4 2 2\n",
      "lebanon 7 6 3 4\n",
      "lebedev 7 5 3 4\n",
      "led 3 3 1 2\n",
      "lee 3 2 2 1\n",
      "left 4 4 1 3\n",
      "leftist 7 6 2 5\n",
      "leg 3 3 1 2\n",
      "legaci 6 6 3 3\n",
      "legal 5 4 2 3\n",
      "legalis 7 6 3 4\n",
      "legend 6 5 2 4\n",
      "legisl 6 5 2 4\n",
      "legislatur 10 9 4 6\n",
      "legitim 7 6 3 4\n",
      "lego 4 4 2 2\n",
      "lehman 6 6 2 4\n",
      "lend 4 4 1 3\n",
      "lenient 7 5 3 4\n",
      "lenovo 6 5 3 3\n",
      "leon 4 4 2 2\n",
      "lerner 6 4 2 4\n",
      "lesbian 7 7 3 4\n",
      "lesley 6 4 3 3\n",
      "less 4 3 1 3\n",
      "lesser 6 4 2 4\n",
      "lesson 6 5 2 4\n",
      "let 3 3 1 2\n",
      "lethal 6 5 2 4\n",
      "letta 5 4 2 3\n",
      "letter 6 4 2 4\n",
      "level 5 3 2 3\n",
      "levi 4 4 2 2\n",
      "lewi 4 4 2 2\n",
      "lewisvil 8 6 3 5\n",
      "lhota 5 5 2 3\n",
      "li 2 2 1 1\n",
      "liabil 6 4 3 3\n",
      "liabl 5 4 2 3\n",
      "lib 3 3 1 2\n",
      "liber 5 5 2 3\n",
      "liberia 7 6 4 3\n",
      "liberti 7 6 3 4\n",
      "libertyfest 11 9 4 7\n",
      "libertyreserv 13 9 5 8\n",
      "libor 5 5 2 3\n",
      "librari 7 5 3 4\n",
      "libya 5 5 3 2\n",
      "libyan 6 6 3 3\n",
      "licens 6 6 2 4\n",
      "lien 4 4 2 2\n",
      "life 4 4 2 2\n",
      "lifestyl 8 7 3 5\n",
      "lift 4 4 1 3\n",
      "light 5 5 1 4\n",
      "lighter 7 7 2 5\n",
      "like 4 4 2 2\n",
      "likeabl 7 6 3 4\n",
      "liken 5 5 2 3\n",
      "lim 3 3 1 2\n",
      "limbo 5 5 2 3\n",
      "limit 5 4 2 3\n",
      "limo 4 4 2 2\n",
      "limousin 8 7 4 4\n",
      "limt 4 4 1 3\n",
      "lincoln 7 5 2 5\n",
      "line 4 4 2 2\n",
      "ling 4 4 1 3\n",
      "linger 6 6 2 4\n",
      "linguist 8 7 3 5\n",
      "link 4 4 1 3\n",
      "lipstick 8 7 2 6\n",
      "liquor 6 6 3 3\n",
      "lirr 4 3 1 3\n",
      "lisa 4 4 2 2\n",
      "list 4 4 1 3\n",
      "listen 6 6 2 4\n",
      "lit 3 3 1 2\n",
      "liter 5 5 2 3\n",
      "lithium 7 6 3 4\n",
      "littl 5 3 1 4\n",
      "littlesheep 11 7 4 7\n",
      "littlewood 10 7 4 6\n",
      "live 4 4 2 2\n",
      "livingsoci 10 8 4 6\n",
      "llywelyn 8 5 3 5\n",
      "lme 3 3 1 2\n",
      "lo 2 2 1 1\n",
      "loan 4 4 2 2\n",
      "lobbi 5 4 2 3\n",
      "loblaw 6 5 2 4\n",
      "lobster 7 7 2 5\n",
      "local 5 4 2 3\n",
      "locat 5 5 2 3\n",
      "lock 4 4 1 3\n",
      "lockdown 8 7 2 6\n",
      "locket 6 6 2 4\n",
      "loco 4 3 2 2\n",
      "locomot 7 5 3 4\n",
      "lodg 4 4 1 3\n",
      "log 3 3 1 2\n",
      "logic 5 5 2 3\n",
      "logist 6 6 2 4\n",
      "logjam 6 6 2 4\n",
      "london 6 4 2 4\n",
      "lone 4 4 2 2\n",
      "long 4 4 1 3\n",
      "longboard 9 8 3 6\n",
      "longer 6 6 2 4\n",
      "longestru 9 9 3 6\n",
      "longtim 7 7 2 5\n",
      "look 4 3 2 2\n",
      "loom 4 3 2 2\n",
      "loophol 7 4 3 4\n",
      "loot 4 3 2 2\n",
      "looter 6 5 3 3\n",
      "lord 4 4 1 3\n",
      "lori 4 4 2 2\n",
      "lose 4 4 2 2\n",
      "loss 4 3 1 3\n",
      "lost 4 4 1 3\n",
      "lotto 5 3 2 3\n",
      "lou 3 3 2 1\n",
      "loud 4 4 2 2\n",
      "loudli 6 5 3 3\n",
      "loui 4 4 3 1\n",
      "louis 5 5 3 2\n",
      "louisana 8 7 5 3\n",
      "louisiana 9 7 6 3\n",
      "love 4 4 2 2\n",
      "lover 5 5 2 3\n",
      "low 3 3 1 2\n",
      "lower 5 5 2 3\n",
      "lowest 6 6 2 4\n",
      "loyal 5 4 3 2\n",
      "loyalist 8 7 4 4\n",
      "loyalti 7 6 4 3\n",
      "lucia 5 5 3 2\n",
      "luck 4 4 1 3\n",
      "lucki 5 5 2 3\n",
      "lukashenko 10 9 4 6\n",
      "lulzsec 7 6 2 5\n",
      "lunch 5 5 1 4\n",
      "lundbeck 8 8 2 6\n",
      "lung 4 4 1 3\n",
      "luther 6 6 2 4\n",
      "luxembourg 10 9 4 6\n",
      "luxuri 6 5 3 3\n",
      "lynch 5 5 1 4\n",
      "maalula 7 4 4 3\n",
      "mac 3 3 1 2\n",
      "machin 6 6 2 4\n",
      "maci 4 4 2 2\n",
      "macrogen 8 8 3 5\n",
      "mad 3 3 1 2\n",
      "madagascar 10 7 4 6\n",
      "made 4 4 2 2\n",
      "madelein 8 7 4 4\n",
      "magazin 7 6 3 4\n",
      "magnet 6 6 2 4\n",
      "magnitski 9 8 3 6\n",
      "magnitud 8 8 3 5\n",
      "maid 4 4 2 2\n",
      "mail 4 4 2 2\n",
      "mailbox 7 7 3 4\n",
      "maim 4 3 2 2\n",
      "main 4 4 2 2\n",
      "mainland 8 6 3 5\n",
      "maino 5 5 3 2\n",
      "mainstream 10 8 4 6\n",
      "mainten 7 6 3 4\n",
      "major 5 5 2 3\n",
      "majq 4 4 1 3\n",
      "make 4 4 2 2\n",
      "maker 5 5 2 3\n",
      "malala 6 3 3 3\n",
      "malaria 7 5 4 3\n",
      "malaysia 8 6 5 3\n",
      "malaysian 9 7 5 4\n",
      "malcolm 7 5 2 5\n",
      "maldiv 6 6 2 4\n",
      "malfunct 8 8 2 6\n",
      "mali 4 4 2 2\n",
      "malian 6 5 3 3\n",
      "maliki 6 5 3 3\n",
      "mall 4 3 1 3\n",
      "malnourish 10 10 4 6\n",
      "mamata 6 3 3 3\n",
      "mammoth 7 5 2 5\n",
      "man 3 3 1 2\n",
      "manag 5 4 2 3\n",
      "manassa 7 4 3 4\n",
      "mandatori 9 8 4 5\n",
      "mandela 7 6 3 4\n",
      "mandelabreaxu 13 10 6 7\n",
      "manhunt 7 6 2 5\n",
      "mani 4 4 2 2\n",
      "manila 6 5 3 3\n",
      "manisha 7 6 3 4\n",
      "manmohan 8 5 3 5\n",
      "manor 5 5 2 3\n",
      "mansfield 9 9 3 6\n",
      "manslaught 10 9 3 7\n",
      "mansour 7 7 3 4\n",
      "manual 6 5 3 3\n",
      "manufactur 10 8 4 6\n",
      "map 3 3 1 2\n",
      "mar 3 3 1 2\n",
      "marbl 5 5 1 4\n",
      "march 5 5 1 4\n",
      "marcher 7 6 2 5\n",
      "marco 5 5 2 3\n",
      "margaret 8 6 3 5\n",
      "margetuximab 12 10 5 7\n",
      "margin 6 6 2 4\n",
      "marginalis 10 8 4 6\n",
      "mari 4 4 2 2\n",
      "maria 5 4 3 2\n",
      "marian 6 5 3 3\n",
      "marijuana 9 7 5 4\n",
      "marin 5 5 2 3\n",
      "marina 6 5 3 3\n",
      "maripol 7 7 3 4\n",
      "maritim 7 5 3 4\n",
      "mark 4 4 1 3\n",
      "market 6 6 2 4\n",
      "marketplac 10 9 3 7\n",
      "markey 6 6 3 3\n",
      "marlboro 8 6 3 5\n",
      "marlin 6 6 2 4\n",
      "marmaray 8 4 4 4\n",
      "marri 5 4 2 3\n",
      "marriag 7 5 3 4\n",
      "marriott 8 6 3 5\n",
      "mart 4 4 1 3\n",
      "martakert 9 6 3 6\n",
      "martha 6 5 2 4\n",
      "martial 7 6 3 4\n",
      "martin 6 6 2 4\n",
      "marxism 7 6 2 5\n",
      "mask 4 4 1 3\n",
      "mass 4 3 1 3\n",
      "massachusett 12 8 4 8\n",
      "massacr 7 5 2 5\n",
      "massag 6 4 2 4\n",
      "massiv 6 5 2 4\n",
      "mast 4 4 1 3\n",
      "master 6 6 2 4\n",
      "mastercard 10 8 3 7\n",
      "mastermind 10 9 3 7\n",
      "mate 4 4 2 2\n",
      "math 4 4 1 3\n",
      "mathi 5 5 2 3\n",
      "matter 6 5 2 4\n",
      "maui 4 4 3 1\n",
      "mauritania 10 7 6 4\n",
      "max 3 3 1 2\n",
      "maximum 7 5 3 4\n",
      "may 3 3 2 1\n",
      "mayer 5 5 3 2\n",
      "mayflow 7 7 3 4\n",
      "mayhem 6 5 3 3\n",
      "mayim 5 4 3 2\n",
      "mayor 5 5 3 2\n",
      "mbay 4 4 2 2\n",
      "mbia 4 4 2 2\n",
      "mccain 6 5 2 4\n",
      "mccann 6 4 1 5\n",
      "mccartney 9 8 3 6\n",
      "mcclay 6 5 2 4\n",
      "mcclendon 9 7 2 7\n",
      "mcconnel 8 6 2 6\n",
      "mcdaniel 8 8 3 5\n",
      "mcdonald 8 7 2 6\n",
      "mcdonnel 8 7 2 6\n",
      "mcintyr 7 7 2 5\n",
      "mcphee 6 5 2 4\n",
      "meal 4 4 2 2\n",
      "mean 4 4 2 2\n",
      "meant 5 5 2 3\n",
      "measl 5 5 2 3\n",
      "measur 6 6 3 3\n",
      "meat 4 4 2 2\n",
      "med 3 3 1 2\n",
      "medal 5 5 2 3\n",
      "media 5 5 3 2\n",
      "mediat 6 6 3 3\n",
      "medic 5 5 2 3\n",
      "medicaid 8 6 4 4\n",
      "medicar 7 7 3 4\n",
      "meet 4 3 2 2\n",
      "megachurch 10 8 3 7\n",
      "megant 6 6 2 4\n",
      "mele 4 3 2 2\n",
      "melina 6 6 3 3\n",
      "melt 4 4 1 3\n",
      "meltdown 8 8 2 6\n",
      "member 6 4 2 4\n",
      "membership 10 8 3 7\n",
      "memo 4 3 2 2\n",
      "memoir 6 5 3 3\n",
      "memori 6 5 3 3\n",
      "memoris 7 6 3 4\n",
      "men 3 3 1 2\n",
      "menac 5 5 2 3\n",
      "mend 4 4 1 3\n",
      "mening 6 5 2 4\n",
      "menlo 5 5 2 3\n",
      "mental 6 6 2 4\n",
      "menthol 7 7 2 5\n",
      "merchant 8 8 2 6\n",
      "merci 5 5 2 3\n",
      "merciless 9 7 3 6\n",
      "merck 5 5 1 4\n",
      "meredith 8 7 3 5\n",
      "merger 6 4 2 4\n",
      "meridian 8 7 4 4\n",
      "merkel 6 5 2 4\n",
      "mess 4 3 1 3\n",
      "messag 6 5 2 4\n",
      "messi 5 4 2 3\n",
      "messiah 7 6 3 4\n",
      "met 3 3 1 2\n",
      "metadata 8 5 4 4\n",
      "meth 4 4 1 3\n",
      "methan 6 6 2 4\n",
      "metinvest 9 7 3 6\n",
      "metro 5 5 2 3\n",
      "metropc 7 7 2 5\n",
      "metta 5 4 2 3\n",
      "mexican 7 7 3 4\n",
      "mexico 6 6 3 3\n",
      "mfglobal 8 7 2 6\n",
      "mgenjxispvaavn 14 11 4 10\n",
      "mgygepqjct 10 9 2 8\n",
      "miami 5 3 3 2\n",
      "mice 4 4 2 2\n",
      "mich 4 4 1 3\n",
      "michael 7 7 3 4\n",
      "michaud 7 7 3 4\n",
      "michel 6 6 2 4\n",
      "michelleobama 13 9 6 7\n",
      "michigan 8 7 3 5\n",
      "microphon 9 8 3 6\n",
      "microsoft 9 8 3 6\n",
      "mid 3 3 1 2\n",
      "middl 5 4 1 4\n",
      "middleton 9 8 3 6\n",
      "mideast 7 7 3 4\n",
      "midwest 7 7 2 5\n",
      "mig 3 3 1 2\n",
      "might 5 5 1 4\n",
      "migrant 7 7 2 5\n",
      "mike 4 4 2 2\n",
      "mil 3 3 1 2\n",
      "milan 5 5 2 3\n",
      "milder 6 6 2 4\n",
      "mile 4 4 2 2\n",
      "mileston 8 8 3 5\n",
      "milford 7 7 2 5\n",
      "milit 5 4 2 3\n",
      "militantsbred 13 11 4 9\n",
      "militari 8 6 4 4\n",
      "militaris 9 7 4 5\n",
      "militia 7 5 4 3\n",
      "milk 4 4 1 3\n",
      "milleni 7 5 3 4\n",
      "millenni 8 5 3 5\n",
      "millennium 10 6 4 6\n",
      "miller 6 5 2 4\n",
      "millington 10 7 3 7\n",
      "million 7 5 3 4\n",
      "millionth 9 7 3 6\n",
      "milwauke 8 8 4 4\n",
      "mina 4 4 2 2\n",
      "minaret 7 7 3 4\n",
      "mind 4 4 1 3\n",
      "mine 4 4 2 2\n",
      "miner 5 5 2 3\n",
      "mini 4 3 2 2\n",
      "minibu 6 5 3 3\n",
      "minimum 7 4 3 4\n",
      "minist 6 5 2 4\n",
      "ministri 8 6 3 5\n",
      "minneapoli 10 8 5 5\n",
      "minnesota 9 8 4 5\n",
      "minor 5 5 2 3\n",
      "minut 5 5 2 3\n",
      "miracl 6 6 2 4\n",
      "miracul 7 7 3 4\n",
      "mirag 5 5 2 3\n",
      "miriam 6 4 3 3\n",
      "miseri 6 5 3 3\n",
      "misfir 6 5 2 4\n",
      "mislead 7 7 3 4\n",
      "misquot 7 7 3 4\n",
      "miss 4 3 1 3\n",
      "missil 6 4 2 4\n",
      "mission 7 5 3 4\n",
      "missouri 8 6 4 4\n",
      "mistak 6 6 2 4\n",
      "mistaken 8 8 3 5\n",
      "mistrial 8 7 3 5\n",
      "misunderstand 13 10 4 9\n",
      "mittal 6 5 2 4\n",
      "mix 3 3 1 2\n",
      "mo 2 2 1 1\n",
      "mob 3 3 1 2\n",
      "mobil 5 5 2 3\n",
      "mobileart 9 9 4 5\n",
      "mock 4 4 1 3\n",
      "mode 4 4 2 2\n",
      "model 5 5 2 3\n",
      "moder 5 5 2 3\n",
      "modi 4 4 2 2\n",
      "modif 5 5 2 3\n",
      "modifi 6 5 3 3\n",
      "mogadishu 9 9 4 5\n",
      "mogoeng 7 5 3 4\n",
      "moham 5 4 2 3\n",
      "moir 4 4 2 2\n",
      "mojav 5 5 2 3\n",
      "mom 3 2 1 2\n",
      "moment 6 5 2 4\n",
      "momentum 8 6 3 5\n",
      "mona 4 4 2 2\n",
      "monarchi 8 8 3 5\n",
      "monasteri 9 9 4 5\n",
      "monday 6 6 3 3\n",
      "money 5 5 3 2\n",
      "monica 6 6 3 3\n",
      "monitor 7 6 3 4\n",
      "moniz 5 5 2 3\n",
      "monk 4 4 1 3\n",
      "monnin 6 4 2 4\n",
      "monopoli 8 6 4 4\n",
      "monsanto 8 6 3 5\n",
      "monsignor 9 7 3 6\n",
      "monsoon 7 4 3 4\n",
      "monster 7 7 2 5\n",
      "mont 4 4 1 3\n",
      "montagnard 10 8 3 7\n",
      "montana 7 5 3 4\n",
      "monteith 8 7 3 5\n",
      "montenegro 10 7 4 6\n",
      "month 5 5 1 4\n",
      "monthli 7 7 2 5\n",
      "montt 5 4 1 4\n",
      "monument 8 6 3 5\n",
      "mood 4 3 2 2\n",
      "moon 4 3 2 2\n",
      "moral 5 5 2 3\n",
      "morgental 9 9 3 6\n",
      "morn 4 4 1 3\n",
      "morningstar 11 9 3 8\n",
      "moro 4 3 2 2\n",
      "morocco 7 4 3 4\n",
      "morri 5 4 2 3\n",
      "morsi 5 5 2 3\n",
      "mortar 6 5 2 4\n",
      "mortgag 7 6 2 5\n",
      "moscow 6 5 2 4\n",
      "mosqu 5 5 2 3\n",
      "mossad 6 5 2 4\n",
      "mote 4 4 2 2\n",
      "mother 6 6 2 4\n",
      "motion 6 5 3 3\n",
      "motiv 5 5 2 3\n",
      "motorbik 8 7 3 5\n",
      "motorcycl 9 7 3 6\n",
      "motorway 8 7 4 4\n",
      "mount 5 5 2 3\n",
      "mourn 5 5 2 3\n",
      "mourner 7 6 3 4\n",
      "mouth 5 5 2 3\n",
      "move 4 4 2 2\n",
      "movemb 6 5 2 4\n",
      "movement 8 6 3 5\n",
      "movi 4 4 2 2\n",
      "mow 3 3 1 2\n",
      "mozambiqu 9 8 4 5\n",
      "mpumalanga 10 7 4 6\n",
      "mta 3 3 1 2\n",
      "mubarak 7 6 3 4\n",
      "much 4 4 1 3\n",
      "mudslid 7 6 2 5\n",
      "mugab 5 5 2 3\n",
      "mujica 6 6 3 3\n",
      "mulayam 7 5 4 3\n",
      "mule 4 4 2 2\n",
      "mull 4 3 1 3\n",
      "multimedia 10 8 5 5\n",
      "multin 6 6 2 4\n",
      "multipl 7 6 2 5\n",
      "mum 3 2 1 2\n",
      "mumbai 6 5 3 3\n",
      "municip 7 6 3 4\n",
      "munley 6 6 3 3\n",
      "murad 5 5 2 3\n",
      "murder 6 5 2 4\n",
      "murdoch 7 7 2 5\n",
      "murdock 7 7 2 5\n",
      "murphi 6 6 2 4\n",
      "mursi 5 5 2 3\n",
      "muscl 5 5 1 4\n",
      "museum 6 4 3 3\n",
      "musharraf 9 7 3 6\n",
      "muslim 6 5 2 4\n",
      "must 4 4 1 3\n",
      "mustard 7 7 2 5\n",
      "mutat 5 4 2 3\n",
      "mute 4 4 2 2\n",
      "mutil 5 5 2 3\n",
      "mutual 6 5 3 3\n",
      "muw 3 3 1 2\n",
      "mwotwyhyk 9 7 3 6\n",
      "myanmar 7 5 3 4\n",
      "myer 4 4 2 2\n",
      "mysteri 7 7 3 4\n",
      "myth 4 4 1 3\n",
      "myunghe 7 7 3 4\n",
      "na 2 2 1 1\n",
      "nab 3 3 1 2\n",
      "nabj 4 4 1 3\n",
      "nadal 5 4 2 3\n",
      "nagasaki 8 6 4 4\n",
      "nairobi 7 6 4 3\n",
      "naiv 4 4 2 2\n",
      "nake 4 4 2 2\n",
      "nakoula 7 6 4 3\n",
      "name 4 4 2 2\n",
      "napol 5 5 2 3\n",
      "narendra 8 5 3 5\n",
      "nari 4 4 2 2\n",
      "narrow 6 5 2 4\n",
      "narrowli 8 7 3 5\n",
      "nasa 4 3 2 2\n",
      "nascent 7 6 2 5\n",
      "naso 4 4 2 2\n",
      "nasti 5 5 2 3\n",
      "nat 3 3 1 2\n",
      "nation 6 5 3 3\n",
      "nationwid 9 7 4 5\n",
      "natl 4 4 1 3\n",
      "nato 4 4 2 2\n",
      "natur 5 5 2 3\n",
      "naval 5 4 2 3\n",
      "navalni 7 5 3 4\n",
      "navenna 7 4 3 4\n",
      "navi 4 4 2 2\n",
      "nawaz 5 4 2 3\n",
      "nazi 4 4 2 2\n",
      "nba 3 3 1 2\n",
      "nbscvdczoo 10 8 2 8\n",
      "ne 2 2 1 1\n",
      "near 4 4 2 2\n",
      "nearer 6 4 3 3\n",
      "nearli 6 6 3 3\n",
      "neb 3 3 1 2\n",
      "nebraska 8 7 3 5\n",
      "neca 4 4 2 2\n",
      "necessari 9 7 4 5\n",
      "need 4 3 2 2\n",
      "negoti 6 6 3 3\n",
      "neighbor 8 8 3 5\n",
      "neighborhood 12 9 5 7\n",
      "neighbour 9 9 4 5\n",
      "neighbourhood 13 10 6 7\n",
      "nelson 6 5 2 4\n",
      "neo 3 3 2 1\n",
      "neonazi 7 6 4 3\n",
      "nepal 5 5 2 3\n",
      "nephew 6 5 2 4\n",
      "nerv 4 4 1 3\n",
      "nestl 5 5 1 4\n",
      "net 3 3 1 2\n",
      "netanyahu 9 7 5 4\n",
      "netflix 7 7 2 5\n",
      "netherland 10 8 3 7\n",
      "network 7 7 2 5\n",
      "neurosci 8 8 4 4\n",
      "neuter 6 5 3 3\n",
      "nevada 6 5 3 3\n",
      "never 5 4 2 3\n",
      "new 3 3 1 2\n",
      "newborn 7 6 2 5\n",
      "newer 5 4 2 3\n",
      "newest 6 5 2 4\n",
      "newjersey 9 7 4 5\n",
      "newlyw 6 5 2 4\n",
      "neworlean 9 7 4 5\n",
      "newport 7 7 2 5\n",
      "news 4 4 1 3\n",
      "newsbreak 9 8 3 6\n",
      "newslett 8 6 2 6\n",
      "newspap 7 6 2 5\n",
      "newsrewind 10 7 3 7\n",
      "newtown 7 5 2 5\n",
      "newyork 7 7 3 4\n",
      "newzealand 10 7 4 6\n",
      "next 4 4 1 3\n",
      "nexu 4 4 2 2\n",
      "ngo 3 3 1 2\n",
      "nhtsa 5 5 1 4\n",
      "niagara 7 5 4 3\n",
      "nib 3 3 1 2\n",
      "nicaragua 9 7 5 4\n",
      "nice 4 4 2 2\n",
      "nick 4 4 1 3\n",
      "nicol 5 5 2 3\n",
      "nidal 5 5 2 3\n",
      "niger 5 5 2 3\n",
      "nigeria 7 6 4 3\n",
      "nigerian 8 6 4 4\n",
      "night 5 5 1 4\n",
      "nightclub 9 9 2 7\n",
      "nil 3 3 1 2\n",
      "nile 4 4 2 2\n",
      "nine 4 3 2 2\n",
      "nineteen 8 4 4 4\n",
      "nineveh 7 5 3 4\n",
      "nintendo 8 6 3 5\n",
      "nitish 6 5 2 4\n",
      "nixon 5 4 2 3\n",
      "njnoebedg 9 7 3 6\n",
      "nkieafabjea 11 8 6 5\n",
      "nkifarcaaei 11 8 6 5\n",
      "nkigababbd 10 7 3 7\n",
      "nkikalbcjba 11 8 3 8\n",
      "nkmvpbjhiaa 11 10 3 8\n",
      "nknaeejbfc 10 8 3 7\n",
      "nkorea 6 6 3 3\n",
      "nkorean 7 6 3 4\n",
      "nksjeijdbg 10 9 2 8\n",
      "nkxxjxafibi 11 8 3 8\n",
      "nmedic 6 6 2 4\n",
      "noaa 4 3 3 1\n",
      "nobel 5 5 2 3\n",
      "nobodi 6 5 3 3\n",
      "nokia 5 5 3 2\n",
      "nomin 5 4 2 3\n",
      "nomine 6 5 3 3\n",
      "non 3 2 1 2\n",
      "nopd 4 4 1 3\n",
      "nordic 6 6 2 4\n",
      "norfolk 7 6 2 5\n",
      "normal 6 6 2 4\n",
      "north 5 5 1 4\n",
      "northbr 7 6 1 6\n",
      "northeast 9 8 3 6\n",
      "northeastern 12 8 4 8\n",
      "northern 8 6 2 6\n",
      "northwest 9 8 2 7\n",
      "norton 6 4 2 4\n",
      "norway 6 6 3 3\n",
      "norwegian 9 8 4 5\n",
      "note 4 4 2 2\n",
      "nov 3 3 1 2\n",
      "novarti 7 7 3 4\n",
      "novemb 6 6 2 4\n",
      "nowher 6 6 2 4\n",
      "nsa 3 3 1 2\n",
      "nuclear 7 7 3 4\n",
      "nuisanc 7 6 3 4\n",
      "nuke 4 4 2 2\n",
      "nullifi 7 5 3 4\n",
      "number 6 6 2 4\n",
      "numer 5 5 2 3\n",
      "nun 3 2 1 2\n",
      "nuroast 7 7 3 4\n",
      "nurs 4 4 1 3\n",
      "nusra 5 5 2 3\n",
      "nutcniat 8 6 3 5\n",
      "nutrit 6 5 2 4\n",
      "ny 2 2 1 1\n",
      "nyc 3 3 1 2\n",
      "nypd 4 4 1 3\n",
      "nyserda 7 7 3 4\n",
      "oak 3 3 2 1\n",
      "oakaje 6 5 4 2\n",
      "oakland 7 6 3 4\n",
      "obama 5 4 3 2\n",
      "obamacar 8 6 4 4\n",
      "obes 4 4 2 2\n",
      "obey 4 4 3 1\n",
      "obit 4 4 2 2\n",
      "obituari 8 7 5 3\n",
      "obscen 6 6 2 4\n",
      "observ 6 6 2 4\n",
      "obsolet 7 6 3 4\n",
      "obstacl 7 7 2 5\n",
      "obstruct 8 7 2 6\n",
      "obtain 6 6 3 3\n",
      "obviou 6 5 4 2\n",
      "obw 3 3 1 2\n",
      "occas 5 4 2 3\n",
      "occup 5 4 2 3\n",
      "occupi 6 5 3 3\n",
      "ocean 5 5 3 2\n",
      "oct 3 3 1 2\n",
      "octob 5 4 2 3\n",
      "od 2 2 1 1\n",
      "odd 3 2 1 2\n",
      "odor 4 3 2 2\n",
      "odyssey 7 5 4 3\n",
      "oe 2 2 2 0\n",
      "offend 6 5 2 4\n",
      "offens 6 5 2 4\n",
      "offer 5 4 2 3\n",
      "offic 5 4 2 3\n",
      "offici 6 4 3 3\n",
      "offset 6 5 2 4\n",
      "often 5 5 2 3\n",
      "ofw 3 3 1 2\n",
      "ogcq 4 4 1 3\n",
      "ogden 5 5 2 3\n",
      "ogr 3 3 1 2\n",
      "ohio 4 3 3 1\n",
      "ohioan 6 5 4 2\n",
      "oi 2 2 2 0\n",
      "oil 3 3 2 1\n",
      "oilbreq 7 7 3 4\n",
      "oilfield 8 6 4 4\n",
      "ok 2 2 1 1\n",
      "okay 4 4 3 1\n",
      "okinawa 7 6 4 3\n",
      "okla 4 4 2 2\n",
      "oklahoma 8 6 4 4\n",
      "old 3 3 1 2\n",
      "older 5 5 2 3\n",
      "oldest 6 6 2 4\n",
      "olhxlv 6 5 1 5\n",
      "oliv 4 4 2 2\n",
      "olivia 6 5 4 2\n",
      "oll 3 2 1 2\n",
      "olymp 5 5 2 3\n",
      "omaha 5 4 3 2\n",
      "oman 4 4 2 2\n",
      "omqxtx 6 5 1 5\n",
      "one 3 3 2 1\n",
      "onlin 5 4 2 3\n",
      "ontario 7 6 4 3\n",
      "op 2 2 1 1\n",
      "ope 3 3 2 1\n",
      "opec 4 4 2 2\n",
      "open 4 4 2 2\n",
      "oper 4 4 2 2\n",
      "opera 5 5 3 2\n",
      "opinion 7 4 4 3\n",
      "opp 3 2 1 2\n",
      "oppenheim 9 7 4 5\n",
      "oppon 5 3 2 3\n",
      "opportun 8 6 3 5\n",
      "oppos 5 3 2 3\n",
      "opposit 7 5 3 4\n",
      "oprah 5 5 2 3\n",
      "opt 3 3 1 2\n",
      "optim 5 5 2 3\n",
      "optimist 8 6 3 5\n",
      "option 6 5 3 3\n",
      "oqmkhqyysc 10 8 3 7\n",
      "oracl 5 5 2 3\n",
      "orakzai 7 6 4 3\n",
      "orang 5 5 2 3\n",
      "orbit 5 5 2 3\n",
      "ordeal 6 6 3 3\n",
      "order 5 4 2 3\n",
      "ore 3 3 2 1\n",
      "oregon 6 5 3 3\n",
      "organ 5 5 2 3\n",
      "orient 6 6 3 3\n",
      "origin 6 5 3 3\n",
      "orlando 7 6 3 4\n",
      "orlean 6 6 3 3\n",
      "ortega 6 6 3 3\n",
      "ortiz 5 5 2 3\n",
      "orwtqsytinspin 14 10 4 10\n",
      "os 2 2 1 1\n",
      "osama 5 4 3 2\n",
      "osc 3 3 1 2\n",
      "oscar 5 5 2 3\n",
      "osha 4 4 2 2\n",
      "ot 2 2 1 1\n",
      "other 5 5 2 3\n",
      "oust 4 4 2 2\n",
      "ouster 6 6 3 3\n",
      "out 3 3 2 1\n",
      "outag 5 5 3 2\n",
      "outbreak 8 8 4 4\n",
      "outcri 6 6 3 3\n",
      "outdoor 7 5 4 3\n",
      "outflow 7 6 3 4\n",
      "outfox 6 5 3 3\n",
      "outlaw 6 6 3 3\n",
      "outlin 6 6 3 3\n",
      "outlook 7 5 4 3\n",
      "outrag 6 6 3 3\n",
      "outreach 8 8 4 4\n",
      "outsid 6 6 3 3\n",
      "outsourc 8 6 4 4\n",
      "overboard 9 7 4 5\n",
      "overcom 7 6 3 4\n",
      "overcrowd 9 7 3 6\n",
      "overdraft 9 8 3 6\n",
      "overhaul 8 8 4 4\n",
      "overnight 9 9 3 6\n",
      "overpass 8 7 3 5\n",
      "overrid 7 6 3 4\n",
      "overrul 7 6 3 4\n",
      "overse 6 5 3 3\n",
      "oversea 7 6 4 3\n",
      "overshadow 10 9 4 6\n",
      "oversight 9 9 3 6\n",
      "overthrow 9 7 3 6\n",
      "overtur 7 6 3 4\n",
      "overturn 8 7 3 5\n",
      "overwhelmingli 14 11 5 9\n",
      "oveturn 7 7 3 4\n",
      "owen 4 4 2 2\n",
      "owner 5 5 2 3\n",
      "ownership 9 9 3 6\n",
      "oxford 6 5 2 4\n",
      "oycsywrhtwi 11 9 4 7\n",
      "ozark 5 5 2 3\n",
      "pa 2 2 1 1\n",
      "pace 4 4 2 2\n",
      "pacif 5 5 2 3\n",
      "pack 4 4 1 3\n",
      "packard 7 6 2 5\n",
      "pact 4 4 1 3\n",
      "pactera 7 6 3 4\n",
      "pactv 5 5 1 4\n",
      "pad 3 3 1 2\n",
      "paddi 5 4 2 3\n",
      "padlock 7 7 2 5\n",
      "paedophil 9 8 4 5\n",
      "page 4 4 2 2\n",
      "pageant 7 6 3 4\n",
      "paid 4 4 2 2\n",
      "pain 4 4 2 2\n",
      "painkil 7 6 3 4\n",
      "painstak 8 7 3 5\n",
      "paint 5 5 2 3\n",
      "pair 4 4 2 2\n",
      "pak 3 3 1 2\n",
      "paka 4 3 2 2\n",
      "pakistan 8 7 3 5\n",
      "pakistani 9 7 4 5\n",
      "palac 5 4 2 3\n",
      "palazzo 7 5 3 4\n",
      "palestin 8 8 3 5\n",
      "palestinian 11 8 5 6\n",
      "palm 4 4 1 3\n",
      "palo 4 4 2 2\n",
      "pan 3 3 1 2\n",
      "panama 6 4 3 3\n",
      "panda 5 4 2 3\n",
      "pandem 6 6 2 4\n",
      "pandora 7 6 3 4\n",
      "panel 5 5 2 3\n",
      "panhandl 8 6 2 6\n",
      "panic 5 5 2 3\n",
      "panorama 8 6 4 4\n",
      "pant 4 4 1 3\n",
      "pantai 6 5 3 3\n",
      "panther 7 7 2 5\n",
      "papal 5 3 2 3\n",
      "paper 5 4 2 3\n",
      "parachut 8 7 3 5\n",
      "parad 5 4 2 3\n",
      "paradis 7 6 3 4\n",
      "paradox 7 6 3 4\n",
      "paraguay 8 6 5 3\n",
      "parasit 7 6 3 4\n",
      "parchin 7 7 2 5\n",
      "pardon 6 6 2 4\n",
      "parent 6 6 2 4\n",
      "parenthood 10 9 4 6\n",
      "pari 4 4 2 2\n",
      "pariah 6 5 3 3\n",
      "park 4 4 1 3\n",
      "parker 6 5 2 4\n",
      "parkinson 9 8 3 6\n",
      "parliament 10 9 4 6\n",
      "parliamentari 13 9 6 7\n",
      "parol 5 5 2 3\n",
      "part 4 4 1 3\n",
      "parti 5 5 2 3\n",
      "partial 7 6 3 4\n",
      "particip 8 6 3 5\n",
      "partli 6 6 2 4\n",
      "partner 7 6 2 5\n",
      "partum 6 6 2 4\n",
      "pascagoula 10 8 5 5\n",
      "pass 4 3 1 3\n",
      "passag 6 4 2 4\n",
      "passeng 7 6 2 5\n",
      "passersbi 9 7 3 6\n",
      "passion 7 6 3 4\n",
      "past 4 4 1 3\n",
      "pastor 6 6 2 4\n",
      "patch 5 5 1 4\n",
      "patent 6 5 2 4\n",
      "patern 6 6 2 4\n",
      "path 4 4 1 3\n",
      "pathologist 11 9 4 7\n",
      "patient 7 6 3 4\n",
      "patio 5 5 3 2\n",
      "patriarch 9 7 3 6\n",
      "patrol 6 6 2 4\n",
      "paul 4 4 2 2\n",
      "pawan 5 4 2 3\n",
      "paxton 6 6 2 4\n",
      "pay 3 3 2 1\n",
      "payment 7 7 3 4\n",
      "payout 6 6 4 2\n",
      "paypal 6 4 3 3\n",
      "payrol 6 6 3 3\n",
      "peac 4 4 2 2\n",
      "peacekeep 9 5 5 4\n",
      "peach 5 5 2 3\n",
      "peanut 6 6 3 3\n",
      "pedestrian 10 9 4 6\n",
      "pedu 4 4 2 2\n",
      "peel 4 3 2 2\n",
      "pei 3 3 2 1\n",
      "pelham 6 6 2 4\n",
      "pelican 7 7 3 4\n",
      "penalti 7 7 3 4\n",
      "penang 6 5 2 4\n",
      "pendant 7 6 2 5\n",
      "peninsula 9 8 4 5\n",
      "penney 6 4 3 3\n",
      "pennsylvania 12 9 5 7\n",
      "pension 7 6 3 4\n",
      "pentagon 8 7 3 5\n",
      "peopl 5 4 2 3\n",
      "peoria 6 6 4 2\n",
      "pepco 5 4 2 3\n",
      "pepper 6 3 2 4\n",
      "per 3 3 1 2\n",
      "percent 7 6 2 5\n",
      "perez 5 4 2 3\n",
      "perform 7 6 2 5\n",
      "period 6 6 3 3\n",
      "perish 6 6 2 4\n",
      "perjuri 7 6 3 4\n",
      "perman 6 6 2 4\n",
      "permiss 7 6 2 5\n",
      "permit 6 6 2 4\n",
      "perpetu 7 5 3 4\n",
      "perri 5 4 2 3\n",
      "perrigo 7 6 3 4\n",
      "perrysburg 10 8 3 7\n",
      "persist 7 6 2 5\n",
      "person 6 6 2 4\n",
      "persuas 7 6 3 4\n",
      "peru 4 4 2 2\n",
      "pervers 7 5 2 5\n",
      "pervez 6 5 2 4\n",
      "peshawar 8 7 3 5\n",
      "pest 4 4 1 3\n",
      "pesticid 8 7 3 5\n",
      "pet 3 3 1 2\n",
      "peterborough 12 9 5 7\n",
      "petit 5 4 2 3\n",
      "petr 4 4 1 3\n",
      "petraeu 7 6 4 3\n",
      "pfizer 6 6 2 4\n",
      "pgisv 5 5 1 4\n",
      "phailin 7 6 3 4\n",
      "phalanx 7 6 2 5\n",
      "pharmaceut 10 9 4 6\n",
      "pharmaci 8 7 3 5\n",
      "phase 5 5 2 3\n",
      "phelan 6 6 2 4\n",
      "philip 6 4 2 4\n",
      "philipp 7 4 2 5\n",
      "philippin 9 5 3 6\n",
      "philli 6 4 2 4\n",
      "pho 3 3 1 2\n",
      "phoenix 7 7 3 4\n",
      "phone 5 5 2 3\n",
      "phonehack 9 8 3 6\n",
      "phoni 5 5 2 3\n",
      "photo 5 4 2 3\n",
      "photograph 10 7 3 7\n",
      "phuket 6 6 2 4\n",
      "physic 6 6 2 4\n",
      "pic 3 3 1 2\n",
      "pick 4 4 1 3\n",
      "pickup 6 5 2 4\n",
      "pictur 6 6 2 4\n",
      "piec 4 4 2 2\n",
      "piecem 6 5 3 3\n",
      "pierc 5 5 2 3\n",
      "pig 3 3 1 2\n",
      "pile 4 4 2 2\n",
      "pilgrim 7 6 2 5\n",
      "pilgrimag 9 7 3 6\n",
      "pill 4 3 1 3\n",
      "pilot 5 5 2 3\n",
      "pime 4 4 2 2\n",
      "pimp 4 3 1 3\n",
      "pin 3 3 1 2\n",
      "pingre 6 6 2 4\n",
      "pink 4 4 1 3\n",
      "pinoy 5 5 3 2\n",
      "pintada 7 6 3 4\n",
      "pinwheel 8 7 3 5\n",
      "pioneer 7 6 4 3\n",
      "pipe 4 3 2 2\n",
      "pipelin 7 5 3 4\n",
      "piqu 4 4 2 2\n",
      "piraci 6 5 3 3\n",
      "pirat 5 5 2 3\n",
      "pistoriu 8 7 4 4\n",
      "pit 3 3 1 2\n",
      "pitbul 6 6 2 4\n",
      "pitch 5 5 1 4\n",
      "pitt 4 3 1 3\n",
      "pittsfield 10 8 3 7\n",
      "pivot 5 5 2 3\n",
      "pix 3 3 1 2\n",
      "pizarro 7 6 3 4\n",
      "pizza 5 4 2 3\n",
      "place 5 5 2 3\n",
      "plagu 5 5 2 3\n",
      "plain 5 5 2 3\n",
      "plan 4 4 1 3\n",
      "planb 5 5 1 4\n",
      "plane 5 5 2 3\n",
      "planet 6 6 2 4\n",
      "planner 7 6 2 5\n",
      "plant 5 5 1 4\n",
      "plantat 7 5 2 5\n",
      "plastic 7 7 2 5\n",
      "platform 8 8 2 6\n",
      "play 4 4 2 2\n",
      "playground 10 10 4 6\n",
      "plaza 5 4 2 3\n",
      "plea 4 4 2 2\n",
      "plead 5 5 2 3\n",
      "pleas 5 5 2 3\n",
      "pledg 5 5 1 4\n",
      "plot 4 4 1 3\n",
      "plu 3 3 1 2\n",
      "plum 4 4 1 3\n",
      "plummet 7 6 2 5\n",
      "plunder 7 7 2 5\n",
      "plung 5 5 1 4\n",
      "plural 6 5 2 4\n",
      "pmo 3 3 1 2\n",
      "pochter 7 7 2 5\n",
      "pocket 6 6 2 4\n",
      "pocketbook 10 7 4 6\n",
      "pocono 6 4 3 3\n",
      "poet 4 4 2 2\n",
      "point 5 5 2 3\n",
      "pois 4 4 2 2\n",
      "poison 6 5 3 3\n",
      "pol 3 3 1 2\n",
      "poland 6 6 2 4\n",
      "polic 5 5 2 3\n",
      "policeman 9 9 4 5\n",
      "policemen 9 8 4 5\n",
      "polici 6 5 3 3\n",
      "polio 5 4 3 2\n",
      "polish 6 6 2 4\n",
      "polit 5 5 2 3\n",
      "politician 10 8 5 5\n",
      "poll 4 3 1 3\n",
      "pollut 6 5 2 4\n",
      "polytechn 9 9 3 6\n",
      "ponc 4 4 1 3\n",
      "ponder 6 6 2 4\n",
      "poni 4 4 2 2\n",
      "ponzi 5 5 2 3\n",
      "poop 4 2 2 2\n",
      "poor 4 3 2 2\n",
      "poorli 6 5 3 3\n",
      "pop 3 2 1 2\n",
      "pope 4 3 2 2\n",
      "popov 5 3 2 3\n",
      "popul 5 4 2 3\n",
      "popular 7 6 3 4\n",
      "populi 6 5 3 3\n",
      "populist 8 7 3 5\n",
      "pork 4 4 1 3\n",
      "porn 4 4 1 3\n",
      "port 4 4 1 3\n",
      "portabl 7 7 2 5\n",
      "portculli 9 8 3 6\n",
      "portland 8 8 2 6\n",
      "portrait 8 6 3 5\n",
      "portray 7 6 3 4\n",
      "portsmouth 10 8 3 7\n",
      "portug 6 6 2 4\n",
      "pose 4 4 2 2\n",
      "posit 5 5 2 3\n",
      "possibl 7 6 2 5\n",
      "post 4 4 1 3\n",
      "postbank 8 8 2 6\n",
      "poster 6 6 2 4\n",
      "postpon 7 5 2 5\n",
      "pot 3 3 1 2\n",
      "potenti 7 6 3 4\n",
      "potter 6 5 2 4\n",
      "potti 5 4 2 3\n",
      "poultri 7 7 3 4\n",
      "pour 4 4 2 2\n",
      "pow 3 3 1 2\n",
      "powder 6 6 2 4\n",
      "power 5 5 2 3\n",
      "powerbal 8 8 3 5\n",
      "ppwnotiyrltcay 14 11 5 9\n",
      "practic 7 6 2 5\n",
      "pragu 5 5 2 3\n",
      "prais 5 5 2 3\n",
      "prank 5 5 1 4\n",
      "pravda 6 5 2 4\n",
      "pray 4 4 2 2\n",
      "prayer 6 5 3 3\n",
      "pre 3 3 1 2\n",
      "preacher 8 6 3 5\n",
      "predawn 7 7 2 5\n",
      "predict 7 7 2 5\n",
      "prefer 6 4 2 4\n",
      "preliminari 11 8 5 6\n",
      "premedit 8 7 3 5\n",
      "premier 7 5 3 4\n",
      "premium 7 6 3 4\n",
      "prepar 6 4 2 4\n",
      "prescrib 8 7 2 6\n",
      "presenc 7 6 2 5\n",
      "present 7 6 2 5\n",
      "preserv 7 5 2 5\n",
      "presid 6 6 2 4\n",
      "presidenti 10 8 4 6\n",
      "press 5 4 1 4\n",
      "presser 7 4 2 5\n",
      "presstv 7 6 1 6\n",
      "pressur 7 5 2 5\n",
      "presum 6 6 2 4\n",
      "pretoria 8 7 4 4\n",
      "prevail 7 7 3 4\n",
      "prevent 7 6 2 5\n",
      "preview 7 6 3 4\n",
      "previou 7 7 4 3\n",
      "prez 4 4 1 3\n",
      "price 5 5 2 3\n",
      "priceless 9 7 3 6\n",
      "pride 5 5 2 3\n",
      "priest 6 6 2 4\n",
      "primari 7 5 3 4\n",
      "prime 5 5 2 3\n",
      "primetim 8 6 3 5\n",
      "princ 5 5 1 4\n",
      "princess 8 7 2 6\n",
      "princip 7 5 2 5\n",
      "print 5 5 1 4\n",
      "printart 8 6 2 6\n",
      "prioriti 8 5 4 4\n",
      "prism 5 5 1 4\n",
      "prison 6 6 2 4\n",
      "privaci 7 6 3 4\n",
      "privat 6 6 2 4\n",
      "privileg 8 7 3 5\n",
      "prize 5 5 2 3\n",
      "pro 3 3 1 2\n",
      "probat 6 6 2 4\n",
      "probe 5 5 2 3\n",
      "problem 7 7 2 5\n",
      "problemat 9 9 3 6\n",
      "procedur 8 7 3 5\n",
      "proceed 7 6 3 4\n",
      "process 7 6 2 5\n",
      "proclam 7 7 2 5\n",
      "produc 6 6 2 4\n",
      "product 7 7 2 5\n",
      "profan 6 6 2 4\n",
      "profession 10 8 4 6\n",
      "professor 9 6 3 6\n",
      "profil 6 6 2 4\n",
      "profit 6 6 2 4\n",
      "program 7 6 2 5\n",
      "programm 8 6 2 6\n",
      "programmat 10 7 3 7\n",
      "progress 8 6 2 6\n",
      "project 7 7 2 5\n",
      "prolong 7 6 2 5\n",
      "promin 6 6 2 4\n",
      "promis 6 6 2 4\n",
      "promot 6 5 2 4\n",
      "prompt 6 5 1 5\n",
      "promursi 8 7 3 5\n",
      "prone 5 5 2 3\n",
      "proof 5 4 2 3\n",
      "propel 6 5 2 4\n",
      "properti 8 6 3 5\n",
      "propos 6 4 2 4\n",
      "proposit 8 6 3 5\n",
      "prosecut 8 8 3 5\n",
      "prosecutor 10 8 4 6\n",
      "prospect 8 7 2 6\n",
      "prostat 7 6 2 5\n",
      "prote 5 5 2 3\n",
      "protect 7 6 2 5\n",
      "protest 7 6 2 5\n",
      "protocol 8 6 3 5\n",
      "prove 5 5 2 3\n",
      "provid 6 6 2 4\n",
      "provinc 7 7 2 5\n",
      "provision 9 7 4 5\n",
      "prowess 7 6 2 5\n",
      "pruitt 6 5 2 4\n",
      "psychiatr 9 9 3 6\n",
      "psychiatrist 12 9 4 8\n",
      "psychward 9 9 2 7\n",
      "pu 2 2 1 1\n",
      "public 6 6 2 4\n",
      "publicpens 10 9 3 7\n",
      "publish 7 7 2 5\n",
      "pulitz 6 6 2 4\n",
      "pull 4 3 1 3\n",
      "pullen 6 5 2 4\n",
      "pullout 7 5 3 4\n",
      "pulpit 6 5 2 4\n",
      "pump 4 3 1 3\n",
      "punctuat 8 6 3 5\n",
      "punish 6 6 2 4\n",
      "pupil 5 4 2 3\n",
      "purchas 7 7 2 5\n",
      "purport 7 5 2 5\n",
      "pursu 5 4 2 3\n",
      "push 4 4 1 3\n",
      "pushback 8 8 2 6\n",
      "put 3 3 1 2\n",
      "putin 5 5 2 3\n",
      "putnam 6 6 2 4\n",
      "puz 3 3 1 2\n",
      "puzzl 5 4 1 4\n",
      "pvu 3 3 1 2\n",
      "pyongyang 9 6 4 5\n",
      "python 6 6 2 4\n",
      "qa 2 2 1 1\n",
      "qabycak 7 6 3 4\n",
      "qaeda 5 4 3 2\n",
      "qaida 5 4 3 2\n",
      "qatada 6 4 3 3\n",
      "qatar 5 4 2 3\n",
      "qi 2 2 1 1\n",
      "qosvhfcc 8 7 1 7\n",
      "qoz 3 3 1 2\n",
      "quak 4 4 2 2\n",
      "qualcomm 8 7 3 5\n",
      "qualifi 7 6 4 3\n",
      "qualiti 7 6 4 3\n",
      "quarantin 9 7 4 5\n",
      "quarri 6 5 3 3\n",
      "quarter 7 6 3 4\n",
      "quay 4 4 3 1\n",
      "quebec 6 5 3 3\n",
      "queen 5 4 3 2\n",
      "quell 5 4 2 3\n",
      "quer 4 4 2 2\n",
      "queri 5 5 3 2\n",
      "question 8 8 4 4\n",
      "quetta 6 5 3 3\n",
      "quick 5 5 2 3\n",
      "quickli 7 6 3 4\n",
      "quiet 5 5 3 2\n",
      "quietli 7 6 4 3\n",
      "quinci 6 5 3 3\n",
      "quinn 5 4 2 3\n",
      "quirico 7 6 4 3\n",
      "quit 4 4 2 2\n",
      "quitter 7 6 3 4\n",
      "quiz 4 4 2 2\n",
      "quo 3 3 2 1\n",
      "quot 4 4 2 2\n",
      "qusair 6 6 3 3\n",
      "qusayr 6 6 3 3\n",
      "qwirp 5 5 1 4\n",
      "rabaa 5 3 3 2\n",
      "rabbi 5 4 2 3\n",
      "race 4 4 2 2\n",
      "racer 5 4 2 3\n",
      "raci 4 4 2 2\n",
      "racial 6 5 3 3\n",
      "racism 6 6 2 4\n",
      "racist 6 6 2 4\n",
      "rad 3 3 1 2\n",
      "rada 4 3 2 2\n",
      "radar 5 3 2 3\n",
      "radiat 6 5 3 3\n",
      "radic 5 5 2 3\n",
      "radio 5 5 3 2\n",
      "radioact 8 7 4 4\n",
      "radioshack 10 9 4 6\n",
      "rafsanjani 10 7 4 6\n",
      "rage 4 4 2 2\n",
      "rai 3 3 2 1\n",
      "raid 4 4 2 2\n",
      "raiffeisen 10 7 5 5\n",
      "rail 4 4 2 2\n",
      "railroad 8 6 4 4\n",
      "railway 7 6 4 3\n",
      "rain 4 4 2 2\n",
      "rainfal 7 6 3 4\n",
      "rainforest 10 9 4 6\n",
      "rais 4 4 2 2\n",
      "rajiv 5 5 2 3\n",
      "rak 3 3 1 2\n",
      "ralli 5 4 2 3\n",
      "ramadan 7 5 3 4\n",
      "ramadhan 8 6 3 5\n",
      "rampag 6 5 2 4\n",
      "ramsey 6 6 3 3\n",
      "ran 3 3 1 2\n",
      "ranbaxi 7 6 3 4\n",
      "random 6 6 2 4\n",
      "rang 4 4 1 3\n",
      "rank 4 4 1 3\n",
      "ransack 7 6 2 5\n",
      "rant 4 4 1 3\n",
      "rape 4 4 2 2\n",
      "rapid 5 5 2 3\n",
      "rapist 6 6 2 4\n",
      "rapper 6 4 2 4\n",
      "raptur 6 5 2 4\n",
      "rare 4 3 2 2\n",
      "rariti 6 4 3 3\n",
      "rat 3 3 1 2\n",
      "rate 4 4 2 2\n",
      "rattl 5 4 1 4\n",
      "raunchi 7 7 3 4\n",
      "ravin 5 5 2 3\n",
      "ray 3 3 2 1\n",
      "raymond 7 7 3 4\n",
      "raytheon 8 8 4 4\n",
      "reach 5 5 2 3\n",
      "react 5 5 2 3\n",
      "reaction 8 8 4 4\n",
      "read 4 4 2 2\n",
      "reader 6 4 3 3\n",
      "readi 5 5 3 2\n",
      "real 4 4 2 2\n",
      "realiti 7 6 4 3\n",
      "realiz 6 6 3 3\n",
      "realli 6 5 3 3\n",
      "reason 6 6 3 3\n",
      "reassur 7 5 3 4\n",
      "rebecca 7 5 3 4\n",
      "rebel 5 4 2 3\n",
      "rebellion 9 7 4 5\n",
      "rebound 7 7 3 4\n",
      "rebrand 7 6 2 5\n",
      "rebuild 7 7 3 4\n",
      "rebuk 5 5 2 3\n",
      "recal 5 5 2 3\n",
      "recant 6 6 2 4\n",
      "recaptur 8 7 3 5\n",
      "receiv 6 5 3 3\n",
      "recent 6 5 2 4\n",
      "recept 6 5 2 4\n",
      "recess 6 4 2 4\n",
      "recipi 6 5 3 3\n",
      "reckless 8 6 2 6\n",
      "reckon 6 6 2 4\n",
      "reclus 6 6 2 4\n",
      "recogn 6 6 2 4\n",
      "recognit 8 8 3 5\n",
      "recommend 9 7 3 6\n",
      "record 6 5 2 4\n",
      "recount 7 7 3 4\n",
      "recov 5 5 2 3\n",
      "recoveri 8 6 4 4\n",
      "recreat 7 5 3 4\n",
      "recruit 7 6 3 4\n",
      "recus 5 5 2 3\n",
      "red 3 3 1 2\n",
      "redcarpet 9 7 3 6\n",
      "redempt 7 6 2 5\n",
      "redesign 8 7 3 5\n",
      "redo 4 4 2 2\n",
      "redston 7 7 2 5\n",
      "reduc 5 5 2 3\n",
      "reduct 6 6 2 4\n",
      "redwood 7 5 3 4\n",
      "reef 4 3 2 2\n",
      "referendum 10 7 4 6\n",
      "refin 5 5 2 3\n",
      "reflect 7 6 2 5\n",
      "refocus 7 7 3 4\n",
      "reform 6 5 2 4\n",
      "reformist 9 8 3 6\n",
      "refug 5 5 2 3\n",
      "refuge 6 5 3 3\n",
      "refus 5 5 2 3\n",
      "regain 6 6 3 3\n",
      "regim 5 5 2 3\n",
      "regiment 8 7 3 5\n",
      "region 6 6 3 3\n",
      "regist 6 6 2 4\n",
      "registr 7 6 2 5\n",
      "registri 8 6 3 5\n",
      "regret 6 4 2 4\n",
      "regrett 7 4 2 5\n",
      "regul 5 5 2 3\n",
      "regulatori 10 9 5 5\n",
      "rehab 5 5 2 3\n",
      "rehman 6 6 2 4\n",
      "reid 4 4 2 2\n",
      "reimpos 7 7 3 4\n",
      "reinstat 8 7 3 5\n",
      "reinvigor 9 7 4 5\n",
      "reit 4 4 2 2\n",
      "reject 6 5 2 4\n",
      "rel 3 3 1 2\n",
      "relat 5 5 2 3\n",
      "relationship 12 11 5 7\n",
      "relaunch 8 8 3 5\n",
      "releas 6 5 3 3\n",
      "reli 4 4 2 2\n",
      "reliabl 7 6 3 4\n",
      "relianc 7 7 3 4\n",
      "relief 6 5 3 3\n",
      "reliev 6 5 3 3\n",
      "religi 6 5 3 3\n",
      "religion 8 7 4 4\n",
      "reliv 5 5 2 3\n",
      "remain 6 6 3 3\n",
      "remand 6 6 2 4\n",
      "remark 6 5 2 4\n",
      "rememb 6 4 2 4\n",
      "remitt 6 5 2 4\n",
      "remot 5 5 2 3\n",
      "remov 5 5 2 3\n",
      "rendit 6 6 2 4\n",
      "renew 5 4 2 3\n",
      "reno 4 4 2 2\n",
      "reopen 6 5 3 3\n",
      "rep 3 3 1 2\n",
      "repair 6 5 3 3\n",
      "repatri 7 6 3 4\n",
      "repeal 6 5 3 3\n",
      "repealbrefc 11 8 4 7\n",
      "repeat 6 5 3 3\n",
      "replac 6 6 2 4\n",
      "replay 6 6 3 3\n",
      "report 6 5 2 4\n",
      "reportedli 10 8 4 6\n",
      "repossess 9 5 3 6\n",
      "repres 6 4 2 4\n",
      "repress 7 4 2 5\n",
      "repriev 7 5 3 4\n",
      "repris 6 5 2 4\n",
      "republican 10 10 4 6\n",
      "reput 5 5 2 3\n",
      "request 7 6 3 4\n",
      "requir 6 5 3 3\n",
      "rerout 6 5 3 3\n",
      "rescu 5 5 2 3\n",
      "rescuer 7 5 3 4\n",
      "research 8 6 3 5\n",
      "resent 6 5 2 4\n",
      "resentenc 9 6 3 6\n",
      "reserv 6 4 2 4\n",
      "reshap 6 6 2 4\n",
      "reshuffl 8 7 2 6\n",
      "resid 5 5 2 3\n",
      "resign 6 6 2 4\n",
      "resist 6 5 2 4\n",
      "resolut 7 7 3 4\n",
      "resolv 6 6 2 4\n",
      "resort 6 5 2 4\n",
      "resound 7 7 3 4\n",
      "resourc 7 6 3 4\n",
      "respect 7 6 2 5\n",
      "respond 7 7 2 5\n",
      "respons 7 6 2 5\n",
      "rest 4 4 1 3\n",
      "restaur 7 6 3 4\n",
      "restiv 6 6 2 4\n",
      "restor 6 5 2 4\n",
      "restraint 9 7 3 6\n",
      "result 6 6 2 4\n",
      "resum 5 5 2 3\n",
      "resumpt 7 7 2 5\n",
      "resurrect 9 6 3 6\n",
      "retail 6 6 3 3\n",
      "retain 6 6 3 3\n",
      "retak 5 5 2 3\n",
      "retali 6 6 3 3\n",
      "retd 4 4 1 3\n",
      "retir 5 4 2 3\n",
      "retreat 7 4 3 4\n",
      "retrial 7 6 3 4\n",
      "return 6 5 2 4\n",
      "reunion 7 6 4 3\n",
      "reunit 6 6 3 3\n",
      "reuter 6 4 3 3\n",
      "revamp 6 6 2 4\n",
      "reveal 6 5 3 3\n",
      "revel 5 4 2 3\n",
      "reveng 6 5 2 4\n",
      "revenu 6 5 3 3\n",
      "revers 6 4 2 4\n",
      "review 6 5 3 3\n",
      "revisit 7 6 3 4\n",
      "revit 5 5 2 3\n",
      "reviv 5 4 2 3\n",
      "revok 5 5 2 3\n",
      "revolt 6 6 2 4\n",
      "revolut 7 7 3 4\n",
      "revolutionari 13 10 7 6\n",
      "reward 6 5 2 4\n",
      "rferl 5 4 1 4\n",
      "rgcrime 7 6 2 5\n",
      "rhetor 6 5 2 4\n",
      "rhino 5 5 2 3\n",
      "rhode 5 5 2 3\n",
      "rhodeisland 11 10 4 7\n",
      "rialto 6 6 3 3\n",
      "rianovosti 10 8 5 5\n",
      "rica 4 4 2 2\n",
      "rice 4 4 2 2\n",
      "rich 4 4 1 3\n",
      "richard 7 6 2 5\n",
      "ricin 5 4 2 3\n",
      "rick 4 4 1 3\n",
      "rickl 5 5 1 4\n",
      "riddl 5 4 1 4\n",
      "riddoch 7 6 2 5\n",
      "ride 4 4 2 2\n",
      "rifl 4 4 1 3\n",
      "rift 4 4 1 3\n",
      "rig 3 3 1 2\n",
      "rigbi 5 4 2 3\n",
      "right 5 5 1 4\n",
      "ring 4 4 1 3\n",
      "ringgit 7 5 2 5\n",
      "ringtail 8 7 3 5\n",
      "rio 3 3 2 1\n",
      "riosmontt 9 7 3 6\n",
      "riot 4 4 2 2\n",
      "rioter 6 5 3 3\n",
      "rip 3 3 1 2\n",
      "rise 4 4 2 2\n",
      "risk 4 4 1 3\n",
      "rite 4 4 2 2\n",
      "ritual 6 6 3 3\n",
      "rival 5 5 2 3\n",
      "river 5 4 2 3\n",
      "riversid 8 6 3 5\n",
      "rivet 5 5 2 3\n",
      "rlem 4 4 1 3\n",
      "road 4 4 2 2\n",
      "roadsid 7 6 3 4\n",
      "roanok 6 5 3 3\n",
      "rob 3 3 1 2\n",
      "robben 6 5 2 4\n",
      "robber 6 4 2 4\n",
      "robberi 7 5 3 4\n",
      "robert 6 5 2 4\n",
      "robinson 8 6 3 5\n",
      "robocal 7 6 3 4\n",
      "robot 5 4 2 3\n",
      "roch 4 4 1 3\n",
      "rock 4 4 1 3\n",
      "rockefel 8 7 3 5\n",
      "rocket 6 6 2 4\n",
      "rode 4 4 2 2\n",
      "rodeo 5 4 3 2\n",
      "roger 5 4 2 3\n",
      "rogu 4 4 2 2\n",
      "rohani 6 6 3 3\n",
      "rohingya 8 8 4 4\n",
      "rok 3 3 1 2\n",
      "role 4 4 2 2\n",
      "roll 4 3 1 3\n",
      "roller 6 4 2 4\n",
      "rollout 7 5 3 4\n",
      "roma 4 4 2 2\n",
      "rome 4 4 2 2\n",
      "romney 6 6 3 3\n",
      "romp 4 4 1 3\n",
      "rompuy 6 6 3 3\n",
      "rooki 5 4 3 2\n",
      "room 4 3 2 2\n",
      "rope 4 4 2 2\n",
      "rose 4 4 2 2\n",
      "rosevil 7 7 3 4\n",
      "rosh 4 4 1 3\n",
      "roshen 6 6 2 4\n",
      "ross 4 3 1 3\n",
      "roug 4 4 2 2\n",
      "rough 5 5 2 3\n",
      "rouhani 7 7 4 3\n",
      "round 5 5 2 3\n",
      "roundli 7 7 3 4\n",
      "roundtabl 9 9 3 6\n",
      "rousseff 8 6 3 5\n",
      "rout 4 4 2 2\n",
      "routin 6 6 3 3\n",
      "row 3 3 1 2\n",
      "rowhani 7 7 3 4\n",
      "royal 5 5 3 2\n",
      "royalbank 9 8 4 5\n",
      "royalti 7 7 4 3\n",
      "royaltypharma 13 9 6 7\n",
      "roycroft 8 6 3 5\n",
      "ru 2 2 1 1\n",
      "rubbish 7 6 2 5\n",
      "rubbl 5 4 1 4\n",
      "rubio 5 5 3 2\n",
      "ruin 4 4 2 2\n",
      "rule 4 4 2 2\n",
      "ruler 5 4 2 3\n",
      "rumor 5 4 2 3\n",
      "run 3 3 1 2\n",
      "runoff 6 5 2 4\n",
      "runway 6 6 3 3\n",
      "rupe 4 4 2 2\n",
      "rural 5 4 2 3\n",
      "rush 4 4 1 3\n",
      "rushmor 7 6 2 5\n",
      "russa 5 4 2 3\n",
      "russia 6 5 3 3\n",
      "russian 7 6 3 4\n",
      "rvwcpart 8 7 1 7\n",
      "rwanda 6 5 2 4\n",
      "ryder 5 4 2 3\n",
      "sabah 5 4 2 3\n",
      "sabotag 7 6 3 4\n",
      "sac 3 3 1 2\n",
      "saccapit 8 6 3 5\n",
      "sack 4 4 1 3\n",
      "sacr 4 4 1 3\n",
      "sacrific 8 6 3 5\n",
      "sad 3 3 1 2\n",
      "sadden 6 5 2 4\n",
      "safe 4 4 2 2\n",
      "safer 5 5 2 3\n",
      "safeti 6 6 3 3\n",
      "safeway 7 6 4 3\n",
      "safrica 7 6 3 4\n",
      "saga 4 3 2 2\n",
      "saggi 5 4 2 3\n",
      "saggyp 6 5 2 4\n",
      "sahakyan 8 6 4 4\n",
      "sahara 6 4 3 3\n",
      "said 4 4 2 2\n",
      "sail 4 4 2 2\n",
      "sailor 6 6 3 3\n",
      "saintsbreb 10 8 3 7\n",
      "sake 4 4 2 2\n",
      "salafist 8 6 3 5\n",
      "salari 6 5 3 3\n",
      "sale 4 4 2 2\n",
      "salesforc 9 8 3 6\n",
      "sall 4 3 1 3\n",
      "salmon 6 6 2 4\n",
      "salut 5 5 2 3\n",
      "samantha 8 6 3 5\n",
      "samesex 7 5 3 4\n",
      "samoa 5 4 3 2\n",
      "sampl 5 5 1 4\n",
      "samsung 7 6 2 5\n",
      "san 3 3 1 2\n",
      "sana 4 3 2 2\n",
      "sanction 8 7 3 5\n",
      "sand 4 4 1 3\n",
      "sandi 5 5 2 3\n",
      "sandwich 8 8 2 6\n",
      "sanford 7 7 2 5\n",
      "sanitari 8 6 4 4\n",
      "santa 5 4 2 3\n",
      "santand 7 5 2 5\n",
      "sarabjit 8 7 3 5\n",
      "sarah 5 4 2 3\n",
      "saratoga 8 6 4 4\n",
      "sarcast 7 5 2 5\n",
      "sarin 5 5 2 3\n",
      "saritha 7 6 3 4\n",
      "sarkozi 7 7 3 4\n",
      "sask 4 3 1 3\n",
      "satellit 8 6 3 5\n",
      "satisfi 7 5 3 4\n",
      "saturday 8 7 4 4\n",
      "saudi 5 5 3 2\n",
      "sausag 6 4 3 3\n",
      "savannah 8 5 3 5\n",
      "save 4 4 2 2\n",
      "savil 5 5 2 3\n",
      "savvi 5 4 2 3\n",
      "saw 3 3 1 2\n",
      "sawiri 6 5 3 3\n",
      "sax 3 3 1 2\n",
      "say 3 3 2 1\n",
      "scale 5 5 2 3\n",
      "scalia 6 5 3 3\n",
      "scam 4 4 1 3\n",
      "scan 4 4 1 3\n",
      "scandal 7 6 2 5\n",
      "scandalridden 13 9 4 9\n",
      "scanner 7 6 2 5\n",
      "scant 5 5 1 4\n",
      "scar 4 4 1 3\n",
      "scarc 5 4 1 4\n",
      "scare 5 5 2 3\n",
      "scari 5 5 2 3\n",
      "scene 5 4 2 3\n",
      "scenic 6 5 2 4\n",
      "sceptic 7 6 2 5\n",
      "schedul 7 7 2 5\n",
      "scheme 6 5 2 4\n",
      "schiel 6 6 2 4\n",
      "schmidt 7 7 1 6\n",
      "schoellkopf 11 9 3 8\n",
      "scholar 7 7 2 5\n",
      "school 6 5 2 4\n",
      "schoolchildren 14 10 4 10\n",
      "schooner 8 7 3 5\n",
      "sci 3 3 1 2\n",
      "scienc 6 5 2 4\n",
      "scientist 9 6 3 6\n",
      "scold 5 5 1 4\n",
      "score 5 5 2 3\n",
      "scoresst 8 6 2 6\n",
      "scot 4 4 1 3\n",
      "scotiabank 10 9 4 6\n",
      "scotland 8 8 2 6\n",
      "scott 5 4 1 4\n",
      "scour 5 5 2 3\n",
      "scout 5 5 2 3\n",
      "scowl 5 5 1 4\n",
      "scream 6 6 2 4\n",
      "screen 6 5 2 4\n",
      "screener 8 5 3 5\n",
      "screw 5 5 1 4\n",
      "scrutin 7 7 2 5\n",
      "scrutini 8 7 3 5\n",
      "scuffl 6 5 1 5\n",
      "scuttlebutt 11 7 3 8\n",
      "sea 3 3 2 1\n",
      "seahawk 7 6 3 4\n",
      "seal 4 4 2 2\n",
      "seamu 5 5 3 2\n",
      "sear 4 4 2 2\n",
      "search 6 6 2 4\n",
      "searcher 8 6 3 5\n",
      "season 6 5 3 3\n",
      "seat 4 4 2 2\n",
      "seattl 6 5 2 4\n",
      "sec 3 3 1 2\n",
      "seclus 6 5 2 4\n",
      "second 6 6 2 4\n",
      "secreci 7 5 3 4\n",
      "secret 6 5 2 4\n",
      "secretari 9 7 4 5\n",
      "secretli 8 7 3 5\n",
      "sectarian 9 8 4 5\n",
      "sector 6 6 2 4\n",
      "secular 7 7 3 4\n",
      "secur 5 5 2 3\n",
      "sedwick 7 7 2 5\n",
      "see 3 2 2 1\n",
      "seed 4 3 2 2\n",
      "seek 4 3 2 2\n",
      "seeker 6 4 3 3\n",
      "seem 4 3 2 2\n",
      "seen 4 3 2 2\n",
      "seeth 5 4 2 3\n",
      "seig 4 4 2 2\n",
      "seiz 4 4 2 2\n",
      "seizur 6 6 3 3\n",
      "seldon 6 6 2 4\n",
      "select 6 5 2 4\n",
      "self 4 4 1 3\n",
      "selfi 5 5 2 3\n",
      "sell 4 3 1 3\n",
      "seller 6 4 2 4\n",
      "semi 4 4 2 2\n",
      "seminar 7 7 3 4\n",
      "semit 5 5 2 3\n",
      "sen 3 3 1 2\n",
      "senat 5 5 2 3\n",
      "send 4 4 1 3\n",
      "seneg 5 4 2 3\n",
      "senior 6 6 3 3\n",
      "sent 4 4 1 3\n",
      "sentenc 7 5 2 5\n",
      "seoul 5 5 3 2\n",
      "separ 5 5 2 3\n",
      "sept 4 4 1 3\n",
      "septemb 7 6 2 5\n",
      "sequest 7 5 3 4\n",
      "sequestr 8 6 3 5\n",
      "serbia 6 6 3 3\n",
      "serbian 7 7 3 4\n",
      "serendra 8 6 3 5\n",
      "sergeant 8 7 3 5\n",
      "seri 4 4 2 2\n",
      "serial 6 6 3 3\n",
      "seriou 6 6 4 2\n",
      "serious 7 6 4 3\n",
      "serv 4 4 1 3\n",
      "servic 6 6 2 4\n",
      "session 7 5 3 4\n",
      "set 3 3 1 2\n",
      "setback 7 7 2 5\n",
      "settl 5 4 1 4\n",
      "settlement 10 6 3 7\n",
      "settler 7 5 2 5\n",
      "seven 5 4 2 3\n",
      "sever 5 4 2 3\n",
      "sewer 5 4 2 3\n",
      "sex 3 3 1 2\n",
      "sexassault 10 7 4 6\n",
      "sexist 6 5 2 4\n",
      "sexual 6 6 3 3\n",
      "sfo 3 3 1 2\n",
      "shabaab 7 4 3 4\n",
      "shabab 6 4 2 4\n",
      "shadow 6 6 2 4\n",
      "shaheen 7 5 3 4\n",
      "shake 5 5 2 3\n",
      "shaken 6 6 2 4\n",
      "shale 5 5 2 3\n",
      "shame 5 5 2 3\n",
      "shangla 7 6 2 5\n",
      "shape 5 5 2 3\n",
      "share 5 5 2 3\n",
      "shariah 7 5 3 4\n",
      "sharif 6 6 2 4\n",
      "shark 5 5 1 4\n",
      "sharon 6 6 2 4\n",
      "sharp 5 5 1 4\n",
      "shatter 7 6 2 5\n",
      "she 3 3 1 2\n",
      "shebab 6 5 2 4\n",
      "shed 4 4 1 3\n",
      "sheena 6 5 3 3\n",
      "shell 5 4 1 4\n",
      "shelter 7 6 2 5\n",
      "shelv 5 5 1 4\n",
      "shepherd 8 6 2 6\n",
      "sheriff 7 6 2 5\n",
      "sherpao 7 7 3 4\n",
      "shevchenko 10 8 3 7\n",
      "shia 4 4 2 2\n",
      "shida 5 5 2 3\n",
      "shield 6 6 2 4\n",
      "shift 5 5 1 4\n",
      "shiit 5 4 2 3\n",
      "shinawatra 10 8 4 6\n",
      "shine 5 5 2 3\n",
      "shinzo 6 6 2 4\n",
      "ship 4 4 1 3\n",
      "shipment 8 8 2 6\n",
      "shipper 7 6 2 5\n",
      "shipwreck 9 9 2 7\n",
      "shirakawa 9 7 4 5\n",
      "shirt 5 5 1 4\n",
      "sho 3 3 1 2\n",
      "shock 5 5 1 4\n",
      "shockwav 8 8 2 6\n",
      "shoe 4 4 2 2\n",
      "shoot 5 4 2 3\n",
      "shooter 7 6 3 4\n",
      "shootingbrelqk 14 13 4 10\n",
      "shootout 8 5 4 4\n",
      "shop 4 4 1 3\n",
      "shore 5 5 2 3\n",
      "short 5 5 1 4\n",
      "shortag 7 7 2 5\n",
      "shortcom 8 7 2 6\n",
      "shortli 7 7 2 5\n",
      "shot 4 4 1 3\n",
      "shoulder 8 8 3 5\n",
      "shouldnt 8 8 2 6\n",
      "shout 5 5 2 3\n",
      "show 4 4 1 3\n",
      "showcas 7 6 2 5\n",
      "showdown 8 6 2 6\n",
      "shrien 6 6 2 4\n",
      "shrine 6 6 2 4\n",
      "shrink 6 6 1 5\n",
      "shrug 5 5 1 4\n",
      "shuanghui 9 7 4 5\n",
      "shun 4 4 1 3\n",
      "shut 4 4 1 3\n",
      "shutdown 8 8 2 6\n",
      "shutter 7 6 2 5\n",
      "shuttl 6 5 1 5\n",
      "sibl 4 4 1 3\n",
      "sichuan 7 7 3 4\n",
      "sicili 6 4 3 3\n",
      "sick 4 4 1 3\n",
      "sicken 6 6 2 4\n",
      "side 4 4 2 2\n",
      "sidelin 7 6 3 4\n",
      "sideshow 8 7 3 5\n",
      "sieg 4 4 2 2\n",
      "sight 5 5 1 4\n",
      "sign 4 4 1 3\n",
      "signal 6 6 2 4\n",
      "signific 8 6 3 5\n",
      "signup 6 6 2 4\n",
      "sikh 4 4 1 3\n",
      "silenc 6 6 2 4\n",
      "silent 6 6 2 4\n",
      "silicon 7 6 3 4\n",
      "silvio 6 5 3 3\n",
      "simpler 7 7 2 5\n",
      "simpli 6 5 2 4\n",
      "simplifi 8 6 3 5\n",
      "simpson 7 6 2 5\n",
      "sinai 5 4 3 2\n",
      "sinc 4 4 1 3\n",
      "singapor 8 8 3 5\n",
      "singh 5 5 1 4\n",
      "singl 5 5 1 4\n",
      "sinist 6 4 2 4\n",
      "sink 4 4 1 3\n",
      "sinkhol 7 7 2 5\n",
      "sir 3 3 1 2\n",
      "sister 6 5 2 4\n",
      "sit 3 3 1 2\n",
      "site 4 4 2 2\n",
      "six 3 3 1 2\n",
      "sixteen 7 6 3 4\n",
      "sixth 5 5 1 4\n",
      "size 4 4 2 2\n",
      "sizzler 7 6 2 5\n",
      "skagit 6 6 2 4\n",
      "skeleton 8 7 3 5\n",
      "skeptic 7 7 2 5\n",
      "sketch 6 6 1 5\n",
      "sketchbook 10 8 3 7\n",
      "skill 5 4 1 4\n",
      "skimmer 7 6 2 5\n",
      "skin 4 4 1 3\n",
      "skip 4 4 1 3\n",
      "skoki 5 4 2 3\n",
      "skorea 6 6 3 3\n",
      "skorean 7 7 3 4\n",
      "skull 5 4 1 4\n",
      "sky 3 3 1 2\n",
      "skydiv 6 6 2 4\n",
      "skype 5 5 2 3\n",
      "skyscrap 8 7 2 6\n",
      "slain 5 5 2 3\n",
      "slam 4 4 1 3\n",
      "slander 7 7 2 5\n",
      "slash 5 4 1 4\n",
      "slaughterhous 13 10 5 8\n",
      "slave 5 5 2 3\n",
      "slay 4 4 2 2\n",
      "sledgehamm 10 8 3 7\n",
      "sleep 5 4 2 3\n",
      "slight 6 6 1 5\n",
      "slightli 8 6 2 6\n",
      "slip 4 4 1 3\n",
      "slit 4 4 1 3\n",
      "slovenia 8 8 4 4\n",
      "slovenian 9 8 4 5\n",
      "slow 4 4 1 3\n",
      "slowdown 8 6 2 6\n",
      "slowest 7 6 2 5\n",
      "slowli 6 5 2 4\n",
      "sluggish 8 6 2 6\n",
      "slum 4 4 1 3\n",
      "slump 5 5 1 4\n",
      "small 5 4 1 4\n",
      "smallest 8 6 2 6\n",
      "smart 5 5 1 4\n",
      "smartphon 9 9 2 7\n",
      "smash 5 4 1 4\n",
      "smell 5 4 1 4\n",
      "smirk 5 5 1 4\n",
      "smith 5 5 1 4\n",
      "smithfield 10 9 3 7\n",
      "smithsonian 11 8 4 7\n",
      "smog 4 4 1 3\n",
      "smoke 5 5 2 3\n",
      "smoker 6 6 2 4\n",
      "smoothli 8 7 3 5\n",
      "smuggl 6 5 1 5\n",
      "smuggler 8 7 2 6\n",
      "snag 4 4 1 3\n",
      "snake 5 5 2 3\n",
      "snatch 6 6 1 5\n",
      "snip 4 4 1 3\n",
      "snoop 5 4 2 3\n",
      "snow 4 4 1 3\n",
      "snowden 7 6 2 5\n",
      "snub 4 4 1 3\n",
      "snyder 6 6 2 4\n",
      "soar 4 4 2 2\n",
      "sobe 4 4 2 2\n",
      "sobey 5 5 3 2\n",
      "social 6 6 3 3\n",
      "societi 7 6 4 3\n",
      "softbank 8 8 2 6\n",
      "softwar 7 7 2 5\n",
      "solar 5 5 2 3\n",
      "sold 4 4 1 3\n",
      "soldier 7 7 3 4\n",
      "solid 5 5 2 3\n",
      "solidar 7 7 3 4\n",
      "solitari 8 7 4 4\n",
      "solut 5 5 2 3\n",
      "solv 4 4 1 3\n",
      "somali 6 6 3 3\n",
      "somalia 7 6 4 3\n",
      "somaliland 10 8 4 6\n",
      "somerset 8 6 3 5\n",
      "sometim 7 6 3 4\n",
      "son 3 3 1 2\n",
      "sono 4 3 2 2\n",
      "soon 4 3 2 2\n",
      "soot 4 3 2 2\n",
      "sooth 5 4 2 3\n",
      "soto 4 3 2 2\n",
      "sought 6 6 2 4\n",
      "soul 4 4 2 2\n",
      "sound 5 5 2 3\n",
      "sourc 5 5 2 3\n",
      "south 5 5 2 3\n",
      "southasia 9 7 5 4\n",
      "southchinasea 13 10 6 7\n",
      "southeast 9 7 4 5\n",
      "southern 8 8 3 5\n",
      "southwest 9 7 3 6\n",
      "southwestern 12 9 4 8\n",
      "soviet 6 6 3 3\n",
      "soweto 6 5 3 3\n",
      "spa 3 3 1 2\n",
      "space 5 5 2 3\n",
      "spacecraft 10 8 3 7\n",
      "spaceshiptwo 12 10 4 8\n",
      "spain 5 5 2 3\n",
      "spam 4 4 1 3\n",
      "spanish 7 6 2 5\n",
      "spare 5 5 2 3\n",
      "spark 5 5 1 4\n",
      "spawn 5 5 1 4\n",
      "speak 5 5 2 3\n",
      "speaker 7 6 3 4\n",
      "spec 4 4 1 3\n",
      "speci 5 5 2 3\n",
      "special 7 7 3 4\n",
      "specialti 9 8 4 5\n",
      "spectacular 11 9 4 7\n",
      "specul 6 6 2 4\n",
      "speech 6 5 2 4\n",
      "speed 5 4 2 3\n",
      "spend 5 5 1 4\n",
      "sperm 5 5 1 4\n",
      "spew 4 4 1 3\n",
      "spi 3 3 1 2\n",
      "spiegel 7 6 3 4\n",
      "spike 5 5 2 3\n",
      "spill 5 4 1 4\n",
      "spiller 7 6 2 5\n",
      "spin 4 4 1 3\n",
      "spine 5 5 2 3\n",
      "spinner 7 6 2 5\n",
      "spiral 6 6 2 4\n",
      "spirit 6 5 2 4\n",
      "spitzer 7 7 2 5\n",
      "splint 6 6 1 5\n",
      "split 5 5 1 4\n",
      "splutter 8 7 2 6\n",
      "spokeswoman 11 9 4 7\n",
      "sponsor 7 5 2 5\n",
      "spot 4 4 1 3\n",
      "spotlight 9 8 2 7\n",
      "spous 5 4 2 3\n",
      "spray 5 5 2 3\n",
      "spread 6 6 2 4\n",
      "spree 5 4 2 3\n",
      "spring 6 6 1 5\n",
      "sprint 6 6 1 5\n",
      "spur 4 4 1 3\n",
      "squad 5 5 2 3\n",
      "squar 5 5 2 3\n",
      "squeez 6 5 3 3\n",
      "srdja 5 5 1 4\n",
      "sri 3 3 1 2\n",
      "sriracha 8 6 3 5\n",
      "stab 4 4 1 3\n",
      "stabil 6 6 2 4\n",
      "stack 5 5 1 4\n",
      "stadium 7 7 3 4\n",
      "staff 5 4 1 4\n",
      "staffer 7 6 2 5\n",
      "stage 5 5 2 3\n",
      "stain 5 5 2 3\n",
      "stakehold 9 9 3 6\n",
      "stalem 6 6 2 4\n",
      "stalin 6 6 2 4\n",
      "stalk 5 5 1 4\n",
      "stall 5 4 1 4\n",
      "stamford 8 8 2 6\n",
      "stamp 5 5 1 4\n",
      "stamped 7 7 2 5\n",
      "stanc 5 5 1 4\n",
      "stand 5 5 1 4\n",
      "standard 8 6 2 6\n",
      "standbi 7 7 2 5\n",
      "standoff 8 7 2 6\n",
      "star 4 4 1 3\n",
      "starbuck 8 8 2 6\n",
      "stare 5 5 2 3\n",
      "staredown 9 9 3 6\n",
      "starker 7 6 2 5\n",
      "starmer 7 6 2 5\n",
      "start 5 4 1 4\n",
      "startup 7 6 2 5\n",
      "stash 5 4 1 4\n",
      "state 5 4 2 3\n",
      "statement 9 6 3 6\n",
      "statewid 8 7 3 5\n",
      "station 7 6 3 4\n",
      "statu 5 4 2 3\n",
      "statutori 9 7 4 5\n",
      "stave 5 5 2 3\n",
      "stay 4 4 2 2\n",
      "stcenturi 9 8 3 6\n",
      "steal 5 5 2 3\n",
      "stealth 7 6 2 5\n",
      "steer 5 4 2 3\n",
      "stemcel 7 6 2 5\n",
      "stennett 8 4 2 6\n",
      "step 4 4 1 3\n",
      "stephen 7 6 2 5\n",
      "stereotyp 9 7 4 5\n",
      "sterl 5 5 1 4\n",
      "steven 6 5 2 4\n",
      "stewart 7 6 2 5\n",
      "stick 5 5 1 4\n",
      "stifl 5 5 1 4\n",
      "still 5 4 1 4\n",
      "sting 5 5 1 4\n",
      "stir 4 4 1 3\n",
      "stock 5 5 1 4\n",
      "stockpil 8 8 2 6\n",
      "stoke 5 5 2 3\n",
      "stole 5 5 2 3\n",
      "stolen 6 6 2 4\n",
      "stomach 7 7 2 5\n",
      "stop 4 4 1 3\n",
      "stopgap 7 6 2 5\n",
      "storag 6 6 2 4\n",
      "store 5 5 2 3\n",
      "stori 5 5 2 3\n",
      "storm 5 5 1 4\n",
      "stoviak 7 7 3 4\n",
      "stow 4 4 1 3\n",
      "straight 8 7 2 6\n",
      "strain 6 6 2 4\n",
      "strand 6 6 1 5\n",
      "strangl 7 7 1 6\n",
      "strangler 9 8 2 7\n",
      "strateg 7 6 2 5\n",
      "strategi 8 7 3 5\n",
      "stratfor 8 6 2 6\n",
      "strauss 7 5 2 5\n",
      "straw 5 5 1 4\n",
      "stray 5 5 2 3\n",
      "street 6 4 2 4\n",
      "strength 8 7 1 7\n",
      "strengthen 10 7 2 8\n",
      "stress 6 4 1 5\n",
      "stretch 7 6 1 6\n",
      "stricken 8 8 2 6\n",
      "strict 6 5 1 5\n",
      "stride 6 6 2 4\n",
      "strife 6 6 2 4\n",
      "strike 6 6 2 4\n",
      "stringer 8 7 2 6\n",
      "strip 5 5 1 4\n",
      "stroke 6 6 2 4\n",
      "strong 6 6 1 5\n",
      "stronger 8 7 2 6\n",
      "strongest 9 7 2 7\n",
      "stronghold 10 9 2 8\n",
      "strongli 8 8 2 6\n",
      "struck 6 6 1 5\n",
      "struggl 7 6 1 6\n",
      "stuart 6 5 2 4\n",
      "stuck 5 5 1 4\n",
      "student 7 6 2 5\n",
      "studi 5 5 2 3\n",
      "stumbl 6 6 1 5\n",
      "stun 4 4 1 3\n",
      "stunt 5 4 1 4\n",
      "stupid 6 6 2 4\n",
      "style 5 5 2 3\n",
      "stylist 7 5 2 5\n",
      "su 2 2 1 1\n",
      "sub 3 3 1 2\n",
      "subcontractor 13 9 4 9\n",
      "subdivis 8 6 3 5\n",
      "submarin 8 8 3 5\n",
      "submit 6 6 2 4\n",
      "subpoena 8 8 4 4\n",
      "subscrib 8 6 2 6\n",
      "subsidi 7 5 3 4\n",
      "substanc 8 7 2 6\n",
      "suburb 6 4 2 4\n",
      "suburban 8 6 3 5\n",
      "success 7 4 2 5\n",
      "succumb 7 5 2 5\n",
      "sud 3 3 1 2\n",
      "sudan 5 5 2 3\n",
      "sudanes 7 6 3 4\n",
      "sudden 6 5 2 4\n",
      "sue 3 3 2 1\n",
      "suffer 6 5 2 4\n",
      "suffici 7 5 3 4\n",
      "suffolk 7 6 2 5\n",
      "sufi 4 4 2 2\n",
      "sugar 5 5 2 3\n",
      "suggest 7 5 2 5\n",
      "suicid 6 5 3 3\n",
      "suit 4 4 2 2\n",
      "suitcas 7 6 3 4\n",
      "sukhoi 6 6 3 3\n",
      "sultan 6 6 2 4\n",
      "sum 3 3 1 2\n",
      "sumatran 8 7 3 5\n",
      "summari 7 6 3 4\n",
      "summer 6 5 2 4\n",
      "summit 6 5 2 4\n",
      "summon 6 5 2 4\n",
      "sumter 6 6 2 4\n",
      "sun 3 3 1 2\n",
      "sunday 6 6 3 3\n",
      "sung 4 4 1 3\n",
      "sunlight 8 8 2 6\n",
      "sunni 5 4 2 3\n",
      "sunshin 7 5 2 5\n",
      "supercomput 11 9 4 7\n",
      "superior 8 7 4 4\n",
      "superpow 8 7 3 5\n",
      "superstorm 10 8 3 7\n",
      "supervis 8 7 3 5\n",
      "supervisor 10 8 4 6\n",
      "suppli 6 5 2 4\n",
      "supplier 8 7 3 5\n",
      "support 7 6 2 5\n",
      "suprem 6 6 2 4\n",
      "supremacist 11 10 4 7\n",
      "sura 4 4 2 2\n",
      "sure 4 4 2 2\n",
      "surfac 6 6 2 4\n",
      "surg 4 4 1 3\n",
      "surgeon 7 7 3 4\n",
      "surgeri 7 6 3 4\n",
      "surgic 6 6 2 4\n",
      "surpass 7 5 2 5\n",
      "surplu 6 5 2 4\n",
      "surpris 7 5 2 5\n",
      "surrend 7 6 2 5\n",
      "surround 8 6 3 5\n",
      "survei 6 6 3 3\n",
      "surveil 7 7 3 4\n",
      "survey 6 6 3 3\n",
      "surviv 6 5 2 4\n",
      "survivalist 11 8 4 7\n",
      "survivor 8 6 3 5\n",
      "susan 5 4 2 3\n",
      "sushmita 8 7 3 5\n",
      "suspect 7 6 2 5\n",
      "suspend 7 6 2 5\n",
      "suspici 7 5 3 4\n",
      "suspicion 9 7 4 5\n",
      "sutay 5 5 3 2\n",
      "suu 3 2 2 1\n",
      "suv 3 3 1 2\n",
      "swallow 7 5 2 5\n",
      "swan 4 4 1 3\n",
      "swap 4 4 1 3\n",
      "swartz 6 6 1 5\n",
      "swat 4 4 1 3\n",
      "sway 4 4 2 2\n",
      "swear 5 5 2 3\n",
      "sweat 5 5 2 3\n",
      "sweden 6 5 2 4\n",
      "swedish 7 6 2 5\n",
      "sweeney 7 5 4 3\n",
      "sweep 5 4 2 3\n",
      "sweet 5 4 2 3\n",
      "swim 4 4 1 3\n",
      "swing 5 5 1 4\n",
      "swiss 5 3 1 4\n",
      "switch 6 6 1 5\n",
      "switzerland 11 11 3 8\n",
      "sworn 5 5 1 4\n",
      "swot 4 4 1 3\n",
      "sydney 6 5 3 3\n",
      "syev 4 4 2 2\n",
      "sygaicmiuru 11 9 6 5\n",
      "symbol 6 6 2 4\n",
      "sympathi 8 8 3 5\n",
      "syosset 7 5 3 4\n",
      "syria 5 5 3 2\n",
      "syrian 6 6 3 3\n",
      "system 6 5 2 4\n",
      "tabard 6 5 2 4\n",
      "tabl 4 4 1 3\n",
      "tablet 6 5 2 4\n",
      "tacki 5 5 2 3\n",
      "tackl 5 5 1 4\n",
      "tactic 6 4 2 4\n",
      "tag 3 3 1 2\n",
      "taho 4 4 2 2\n",
      "taint 5 4 2 3\n",
      "taiwan 6 5 3 3\n",
      "take 4 4 2 2\n",
      "taken 5 5 2 3\n",
      "takeout 7 6 4 3\n",
      "taker 5 5 2 3\n",
      "taksim 6 6 2 4\n",
      "tale 4 4 2 2\n",
      "taliban 7 6 3 4\n",
      "talk 4 4 1 3\n",
      "tallinn 7 5 2 5\n",
      "tan 3 3 1 2\n",
      "tank 4 4 1 3\n",
      "tanker 6 6 2 4\n",
      "tap 3 3 1 2\n",
      "tape 4 4 2 2\n",
      "tar 3 3 1 2\n",
      "tarantino 9 6 4 5\n",
      "target 6 5 2 4\n",
      "tariff 6 5 2 4\n",
      "tarnish 7 7 2 5\n",
      "taser 5 5 2 3\n",
      "task 4 4 1 3\n",
      "tast 4 3 1 3\n",
      "tat 3 2 1 2\n",
      "tatar 5 3 2 3\n",
      "taugher 7 7 3 4\n",
      "taunt 5 4 2 3\n",
      "tawakkul 8 6 3 5\n",
      "tax 3 3 1 2\n",
      "taxi 4 4 2 2\n",
      "tea 3 3 2 1\n",
      "teacher 7 6 3 4\n",
      "team 4 4 2 2\n",
      "teamster 8 6 3 5\n",
      "tear 4 4 2 2\n",
      "tech 4 4 1 3\n",
      "technic 7 6 2 5\n",
      "technician 10 7 4 6\n",
      "techniqu 8 8 3 5\n",
      "technolog 9 8 3 6\n",
      "ted 3 3 1 2\n",
      "tediou 6 6 4 2\n",
      "teen 4 3 2 2\n",
      "teenag 6 5 3 3\n",
      "telecom 7 6 3 4\n",
      "telecomm 8 6 3 5\n",
      "telephon 8 7 3 5\n",
      "televis 7 6 3 4\n",
      "tell 4 3 1 3\n",
      "temp 4 4 1 3\n",
      "temper 6 5 2 4\n",
      "temperatur 10 7 4 6\n",
      "templ 5 5 1 4\n",
      "templat 7 6 2 5\n",
      "temporari 9 8 4 5\n",
      "ten 3 3 1 2\n",
      "tenabl 6 6 2 4\n",
      "tender 6 5 2 4\n",
      "tenn 4 3 1 3\n",
      "tennesse 8 4 3 5\n",
      "tens 4 4 1 3\n",
      "tension 7 6 3 4\n",
      "tent 4 3 1 3\n",
      "tenur 5 5 2 3\n",
      "tepco 5 5 2 3\n",
      "teresa 6 5 3 3\n",
      "teresita 8 6 4 4\n",
      "term 4 4 1 3\n",
      "termin 6 6 2 4\n",
      "terri 5 4 2 3\n",
      "terror 6 4 2 4\n",
      "terrorist 9 6 3 6\n",
      "tesco 5 5 2 3\n",
      "tesoro 6 5 3 3\n",
      "test 4 3 1 3\n",
      "testicl 7 6 2 5\n",
      "testifi 7 5 3 4\n",
      "testimoni 9 7 4 5\n",
      "tevk 4 4 1 3\n",
      "texa 4 4 2 2\n",
      "texan 5 5 2 3\n",
      "text 4 3 1 3\n",
      "tha 3 3 1 2\n",
      "thad 4 4 1 3\n",
      "thai 4 4 2 2\n",
      "thailand 8 7 3 5\n",
      "thank 5 5 1 4\n",
      "that 4 3 1 3\n",
      "thatcher 8 6 2 6\n",
      "thaw 4 4 1 3\n",
      "theater 7 5 3 4\n",
      "theatr 6 5 2 4\n",
      "theft 5 4 1 4\n",
      "therapeut 9 7 4 5\n",
      "therapi 7 7 3 4\n",
      "there 5 4 2 3\n",
      "thief 5 5 2 3\n",
      "thiev 5 5 2 3\n",
      "thing 5 5 1 4\n",
      "think 5 5 1 4\n",
      "third 5 5 1 4\n",
      "thirti 6 4 2 4\n",
      "thoma 5 5 2 3\n",
      "though 6 5 2 4\n",
      "thought 7 5 2 5\n",
      "thousand 8 8 3 5\n",
      "threadneedl 11 8 4 7\n",
      "threat 6 5 2 4\n",
      "threaten 8 6 3 5\n",
      "three 5 4 2 3\n",
      "thrive 6 6 2 4\n",
      "throat 6 5 2 4\n",
      "throne 6 6 2 4\n",
      "throng 6 6 1 5\n",
      "throw 5 5 1 4\n",
      "thrown 6 6 1 5\n",
      "thug 4 4 1 3\n",
      "thunderstorm 12 10 3 9\n",
      "thursday 8 8 3 5\n",
      "thwart 6 5 1 5\n",
      "tia 3 3 2 1\n",
      "tiananmen 9 6 4 5\n",
      "tibet 5 4 2 3\n",
      "tibetan 7 6 3 4\n",
      "ticad 5 5 2 3\n",
      "tick 4 4 1 3\n",
      "ticker 6 6 2 4\n",
      "ticket 6 5 2 4\n",
      "tidewat 7 6 3 4\n",
      "tie 3 3 2 1\n",
      "tierney 7 6 4 3\n",
      "tiger 5 5 2 3\n",
      "tigerglob 9 8 3 6\n",
      "tighten 7 6 2 5\n",
      "tighter 7 6 2 5\n",
      "tightli 7 5 2 5\n",
      "tilli 5 3 2 3\n",
      "tilt 4 3 1 3\n",
      "tim 3 3 1 2\n",
      "timber 6 6 2 4\n",
      "time 4 4 2 2\n",
      "timelin 7 6 3 4\n",
      "timesbi 7 6 3 4\n",
      "timet 5 4 2 3\n",
      "timothi 7 5 3 4\n",
      "tina 4 4 2 2\n",
      "tini 4 3 2 2\n",
      "tip 3 3 1 2\n",
      "tirah 5 5 2 3\n",
      "tit 3 2 1 2\n",
      "titan 5 4 2 3\n",
      "titl 4 3 1 3\n",
      "tmobil 6 6 2 4\n",
      "tna 3 3 1 2\n",
      "tobacco 7 5 3 4\n",
      "today 5 5 3 2\n",
      "todd 4 3 1 3\n",
      "toddler 7 6 2 5\n",
      "toe 3 3 2 1\n",
      "togeth 6 5 2 4\n",
      "tokyo 5 4 3 2\n",
      "told 4 4 1 3\n",
      "toler 5 5 2 3\n",
      "toll 4 3 1 3\n",
      "tomato 6 4 3 3\n",
      "tomb 4 4 1 3\n",
      "tone 4 4 2 2\n",
      "tonko 5 4 2 3\n",
      "took 4 3 2 2\n",
      "tool 4 3 2 2\n",
      "top 3 3 1 2\n",
      "topless 7 6 2 5\n",
      "toppl 5 4 1 4\n",
      "topstori 8 6 3 5\n",
      "torch 5 5 1 4\n",
      "tori 4 4 2 2\n",
      "tornado 7 6 3 4\n",
      "toronto 7 4 3 4\n",
      "torrenti 8 6 3 5\n",
      "tortur 6 4 2 4\n",
      "toss 4 3 1 3\n",
      "total 5 4 2 3\n",
      "tote 4 3 2 2\n",
      "tough 5 5 2 3\n",
      "tougher 7 7 3 4\n",
      "toughest 8 7 3 5\n",
      "tour 4 4 2 2\n",
      "tourism 7 7 3 4\n",
      "tourist 7 6 3 4\n",
      "tourr 5 4 2 3\n",
      "tout 4 3 2 2\n",
      "toward 6 6 2 4\n",
      "tower 5 5 2 3\n",
      "town 4 4 1 3\n",
      "township 8 8 2 6\n",
      "townsvil 8 8 2 6\n",
      "toxic 5 5 2 3\n",
      "toxicolog 9 7 4 5\n",
      "toy 3 3 2 1\n",
      "toyota 6 4 4 2\n",
      "track 5 5 1 4\n",
      "tracker 7 6 2 5\n",
      "trade 5 5 2 3\n",
      "trademark 9 7 3 6\n",
      "trader 6 5 2 4\n",
      "tradit 6 5 2 4\n",
      "traffic 7 6 2 5\n",
      "traffick 8 7 2 6\n",
      "tragedi 7 7 3 4\n",
      "tragic 6 6 2 4\n",
      "trail 5 5 2 3\n",
      "train 5 5 2 3\n",
      "trait 5 4 2 3\n",
      "traitor 7 5 3 4\n",
      "transfer 8 7 2 6\n",
      "transgend 9 8 2 7\n",
      "transit 7 6 2 5\n",
      "transpar 8 6 2 6\n",
      "transplant 10 7 2 8\n",
      "transport 9 7 2 7\n",
      "transsexu 9 8 3 6\n",
      "trap 4 4 1 3\n",
      "trash 5 5 1 4\n",
      "travel 6 6 2 4\n",
      "trayvon 7 7 3 4\n",
      "treason 7 7 3 4\n",
      "treasur 7 6 3 4\n",
      "treat 5 4 2 3\n",
      "treater 7 4 3 4\n",
      "treati 6 5 3 3\n",
      "treatment 9 6 3 6\n",
      "tree 4 3 2 2\n",
      "tremor 6 5 2 4\n",
      "trend 5 5 1 4\n",
      "trendsett 9 6 2 7\n",
      "trespass 8 6 2 6\n",
      "tri 3 3 1 2\n",
      "trial 5 5 2 3\n",
      "triangl 7 7 2 5\n",
      "tribal 6 6 2 4\n",
      "tribesmen 9 8 3 6\n",
      "trick 5 5 1 4\n",
      "tricki 6 5 2 4\n",
      "trigger 7 5 2 5\n",
      "trigona 7 7 3 4\n",
      "trim 4 4 1 3\n",
      "trio 4 4 2 2\n",
      "trip 4 4 1 3\n",
      "tripl 5 5 1 4\n",
      "tripoli 7 6 3 4\n",
      "triumph 7 7 2 5\n",
      "troll 5 4 1 4\n",
      "troop 5 4 2 3\n",
      "trooper 7 5 3 4\n",
      "tropic 6 6 2 4\n",
      "troubl 6 6 2 4\n",
      "truce 5 5 2 3\n",
      "truck 5 5 1 4\n",
      "trucker 7 6 2 5\n",
      "true 4 4 2 2\n",
      "trump 5 5 1 4\n",
      "trust 5 4 1 4\n",
      "truste 6 5 2 4\n",
      "truth 5 4 1 4\n",
      "tsa 3 3 1 2\n",
      "tsbpti 6 5 1 5\n",
      "tskfesq 7 6 1 6\n",
      "tsunami 7 7 3 4\n",
      "tsvangirai 10 8 4 6\n",
      "tuareg 6 6 3 3\n",
      "tuesday 7 7 4 3\n",
      "tuition 7 5 4 3\n",
      "tumblr 6 6 1 5\n",
      "tumor 5 5 2 3\n",
      "tuni 4 4 2 2\n",
      "tunisia 7 6 4 3\n",
      "tunisian 8 6 4 4\n",
      "tunnel 6 5 2 4\n",
      "turbul 6 5 2 4\n",
      "turf 4 4 1 3\n",
      "turk 4 4 1 3\n",
      "turkey 6 6 3 3\n",
      "turkish 7 7 2 5\n",
      "turmoil 7 7 3 4\n",
      "turn 4 4 1 3\n",
      "turnout 7 5 3 4\n",
      "tweak 5 5 2 3\n",
      "tweet 5 3 2 3\n",
      "tweeter 7 4 3 4\n",
      "twenti 6 5 2 4\n",
      "twice 5 5 2 3\n",
      "twin 4 4 1 3\n",
      "twinki 6 5 2 4\n",
      "twist 5 4 1 4\n",
      "twister 7 6 2 5\n",
      "twitter 7 5 2 5\n",
      "two 3 3 1 2\n",
      "ty 2 2 1 1\n",
      "tycoon 6 5 3 3\n",
      "tymoshenko 10 9 4 6\n",
      "typhoid 7 7 3 4\n",
      "typhoon 7 6 3 4\n",
      "uae 3 3 3 0\n",
      "uaswhoapzngthi 14 12 5 9\n",
      "ubuntu 6 4 3 3\n",
      "udq 3 3 1 2\n",
      "ufo 3 3 2 1\n",
      "uganda 6 5 3 3\n",
      "ugliest 7 7 3 4\n",
      "uj 2 2 1 1\n",
      "uk 2 2 1 1\n",
      "ukrain 6 6 3 3\n",
      "ukraini 7 6 4 3\n",
      "ukrainian 9 6 5 4\n",
      "ulq 3 3 1 2\n",
      "ulster 6 6 2 4\n",
      "un 2 2 1 1\n",
      "unaccept 8 7 3 5\n",
      "unaccount 9 6 4 5\n",
      "unaffect 8 7 3 5\n",
      "unansw 6 5 2 4\n",
      "unapprov 8 7 3 5\n",
      "unarm 5 5 2 3\n",
      "uncertain 9 8 4 5\n",
      "uncertainti 11 8 5 6\n",
      "unchain 7 6 3 4\n",
      "unchart 7 7 2 5\n",
      "uncl 4 4 1 3\n",
      "unclear 7 7 3 4\n",
      "unconstitut 11 7 4 7\n",
      "uncov 5 5 2 3\n",
      "underag 7 7 3 4\n",
      "underbank 9 8 3 6\n",
      "underclass 10 9 3 7\n",
      "undercov 8 8 3 5\n",
      "underestim 10 9 4 6\n",
      "undergo 7 7 3 4\n",
      "underground 11 7 4 7\n",
      "underpin 8 7 3 5\n",
      "understand 10 8 3 7\n",
      "underway 8 8 4 4\n",
      "undescend 9 6 3 6\n",
      "uneasi 6 6 4 2\n",
      "unemploy 8 8 4 4\n",
      "unexpectedli 12 10 5 7\n",
      "unexplain 9 8 4 5\n",
      "unfair 6 6 3 3\n",
      "unfinish 8 6 3 5\n",
      "unfriendliest 13 10 5 8\n",
      "unhurt 6 5 2 4\n",
      "unidentifi 10 7 5 5\n",
      "unifi 5 4 3 2\n",
      "union 5 4 3 2\n",
      "uniqu 5 4 3 2\n",
      "uniqur 6 5 3 3\n",
      "unit 4 4 2 2\n",
      "uniti 5 4 3 2\n",
      "univers 7 7 3 4\n",
      "unknown 7 5 2 5\n",
      "unless 6 5 2 4\n",
      "unlik 5 5 2 3\n",
      "unliv 5 5 2 3\n",
      "unman 5 4 2 3\n",
      "unnerv 6 5 2 4\n",
      "unnew 5 4 2 3\n",
      "unpunish 8 6 3 5\n",
      "unregist 8 8 3 5\n",
      "unreli 6 6 3 3\n",
      "unreport 8 7 3 5\n",
      "unrest 6 6 2 4\n",
      "unsaf 5 5 2 3\n",
      "unsc 4 4 1 3\n",
      "unseal 6 6 3 3\n",
      "unseen 6 4 3 3\n",
      "unsnarl 7 6 2 5\n",
      "unsuspect 9 7 3 6\n",
      "unusu 5 3 3 2\n",
      "unveil 6 6 3 3\n",
      "unwant 6 5 2 4\n",
      "unwelcom 8 8 3 5\n",
      "up 2 2 1 1\n",
      "upa 3 3 2 1\n",
      "upbeat 6 6 3 3\n",
      "upcom 5 5 2 3\n",
      "updat 5 5 2 3\n",
      "upend 5 5 2 3\n",
      "upfront 7 7 2 5\n",
      "upgrad 6 6 2 4\n",
      "upheav 6 6 3 3\n",
      "upheld 6 6 2 4\n",
      "uphold 6 6 2 4\n",
      "upload 6 6 3 3\n",
      "upper 5 4 2 3\n",
      "upright 7 7 2 5\n",
      "upris 5 5 2 3\n",
      "uproar 6 5 3 3\n",
      "upscal 6 6 2 4\n",
      "upset 5 5 2 3\n",
      "upton 5 5 2 3\n",
      "uranium 7 6 4 3\n",
      "urban 5 5 2 3\n",
      "urg 3 3 1 2\n",
      "urgenc 6 6 2 4\n",
      "urgent 6 6 2 4\n",
      "urin 4 4 2 2\n",
      "uruguay 7 5 5 2\n",
      "us 2 2 1 1\n",
      "usa 3 3 2 1\n",
      "usairway 8 7 5 3\n",
      "usccb 5 4 1 4\n",
      "usda 4 4 2 2\n",
      "use 3 3 2 1\n",
      "user 4 4 2 2\n",
      "usgulf 6 5 2 4\n",
      "usm 3 3 1 2\n",
      "ususd 5 3 2 3\n",
      "utah 4 4 2 2\n",
      "utica 5 5 3 2\n",
      "uttarakhand 11 8 4 7\n",
      "uvfk 4 4 1 3\n",
      "uxuhagqunct 11 9 4 7\n",
      "va 2 2 1 1\n",
      "vacat 5 4 2 3\n",
      "vaccin 6 5 2 4\n",
      "vacuum 6 5 3 3\n",
      "valentyna 9 7 4 5\n",
      "vallarta 8 5 3 5\n",
      "valley 6 5 3 3\n",
      "valu 4 4 2 2\n",
      "valuabl 7 5 3 4\n",
      "valv 4 3 1 3\n",
      "van 3 3 1 2\n",
      "vandal 6 5 2 4\n",
      "vanish 6 6 2 4\n",
      "vardan 6 5 2 4\n",
      "vari 4 4 2 2\n",
      "vascular 8 7 3 5\n",
      "vast 4 4 1 3\n",
      "vat 3 3 1 2\n",
      "vatanda 7 5 3 4\n",
      "vatican 7 6 3 4\n",
      "vault 5 5 2 3\n",
      "vega 4 4 2 2\n",
      "veget 5 4 2 3\n",
      "vehicl 6 6 2 4\n",
      "vendor 6 6 2 4\n",
      "venezuela 9 7 5 4\n",
      "venezuelan 10 7 5 5\n",
      "ventil 6 6 2 4\n",
      "ventur 6 6 2 4\n",
      "vera 4 4 2 2\n",
      "verdict 7 7 2 5\n",
      "verif 5 5 2 3\n",
      "verifi 6 5 3 3\n",
      "verizon 7 7 3 4\n",
      "vermont 7 7 2 5\n",
      "vernon 6 5 2 4\n",
      "vessel 6 4 2 4\n",
      "vet 3 3 1 2\n",
      "veteran 7 6 3 4\n",
      "veto 4 4 2 2\n",
      "via 3 3 2 1\n",
      "viabil 6 5 3 3\n",
      "viabl 5 5 2 3\n",
      "vibrant 7 7 2 5\n",
      "vibrat 6 6 2 4\n",
      "vice 4 4 2 2\n",
      "vicino 6 5 3 3\n",
      "victim 6 5 2 4\n",
      "victori 7 6 3 4\n",
      "victoria 8 7 4 4\n",
      "vid 3 3 1 2\n",
      "video 5 5 3 2\n",
      "vie 3 3 2 1\n",
      "vienna 6 5 3 3\n",
      "vietnam 7 7 3 4\n",
      "vietnames 9 8 4 5\n",
      "view 4 4 2 2\n",
      "viewer 6 5 3 3\n",
      "viewpoint 9 8 4 5\n",
      "vigil 5 4 2 3\n",
      "villa 5 4 2 3\n",
      "villag 6 5 2 4\n",
      "vinc 4 4 1 3\n",
      "vinegar 7 7 3 4\n",
      "violanc 7 7 3 4\n",
      "violat 6 6 3 3\n",
      "violenc 7 7 3 4\n",
      "violencest 10 9 4 6\n",
      "violent 7 7 3 4\n",
      "violin 6 5 3 3\n",
      "viral 5 5 2 3\n",
      "virgin 6 5 2 4\n",
      "virginia 8 6 4 4\n",
      "viru 4 4 2 2\n",
      "visa 4 4 2 2\n",
      "visakhapatnam 13 10 5 8\n",
      "vision 6 5 3 3\n",
      "visit 5 4 2 3\n",
      "visitor 7 6 3 4\n",
      "vital 5 5 2 3\n",
      "vitro 5 5 2 3\n",
      "vladimir 8 7 3 5\n",
      "vnug 4 4 1 3\n",
      "vnvlqoqao 9 6 3 6\n",
      "voa 3 3 2 1\n",
      "vodafon 7 6 3 4\n",
      "vof 3 3 1 2\n",
      "voic 4 4 2 2\n",
      "void 4 4 2 2\n",
      "volatil 7 6 3 4\n",
      "volcano 7 6 3 4\n",
      "volcanobrek 11 10 4 7\n",
      "volk 4 4 1 3\n",
      "volunt 6 6 2 4\n",
      "volyn 5 5 2 3\n",
      "vonachen 8 7 3 5\n",
      "vote 4 4 2 2\n",
      "voter 5 5 2 3\n",
      "vow 3 3 1 2\n",
      "vox 3 3 1 2\n",
      "vulner 6 6 2 4\n",
      "vuq 3 3 1 2\n",
      "wacha 5 4 2 3\n",
      "wade 4 4 2 2\n",
      "wage 4 4 2 2\n",
      "wagmk 5 5 1 4\n",
      "wait 4 4 2 2\n",
      "waitress 8 7 3 5\n",
      "waiv 4 4 2 2\n",
      "waiver 6 6 3 3\n",
      "wake 4 4 2 2\n",
      "wal 3 3 1 2\n",
      "wale 4 4 2 2\n",
      "walgreen 8 7 3 5\n",
      "waliur 6 6 3 3\n",
      "walk 4 4 1 3\n",
      "walkout 7 7 3 4\n",
      "wall 4 3 1 3\n",
      "walmart 7 6 2 5\n",
      "want 4 4 1 3\n",
      "war 3 3 1 2\n",
      "wardak 6 5 2 4\n",
      "warden 6 6 2 4\n",
      "wareh 5 5 2 3\n",
      "warm 4 4 1 3\n",
      "warn 4 4 1 3\n",
      "warrant 7 5 2 5\n",
      "warrantless 11 8 3 8\n",
      "warren 6 5 2 4\n",
      "warrior 7 5 3 4\n",
      "warsaw 6 4 2 4\n",
      "wartim 6 6 2 4\n",
      "wasena 6 5 3 3\n",
      "wash 4 4 1 3\n",
      "washington 10 9 3 7\n",
      "washingtonst 12 9 3 9\n",
      "washpost 8 7 2 6\n",
      "wast 4 4 1 3\n",
      "watch 5 5 1 4\n",
      "watchdog 8 8 2 6\n",
      "water 5 5 2 3\n",
      "waterbomb 9 8 3 6\n",
      "watersh 7 7 2 5\n",
      "watervil 8 8 3 5\n",
      "wave 4 4 2 2\n",
      "way 3 3 2 1\n",
      "wayn 4 4 2 2\n",
      "waze 4 4 2 2\n",
      "weak 4 4 2 2\n",
      "weaken 6 5 3 3\n",
      "wealth 6 6 2 4\n",
      "wealthi 7 7 3 4\n",
      "weapon 6 6 3 3\n",
      "weaponri 8 8 4 4\n",
      "wear 4 4 2 2\n",
      "wearabl 7 6 3 4\n",
      "wearhous 8 8 4 4\n",
      "weari 5 5 3 2\n",
      "weather 7 6 3 4\n",
      "web 3 3 1 2\n",
      "webcast 7 7 2 5\n",
      "websit 6 6 2 4\n",
      "wed 3 3 1 2\n",
      "wednesday 9 7 4 5\n",
      "week 4 3 2 2\n",
      "weekend 7 5 3 4\n",
      "weekli 6 5 3 3\n",
      "weeklong 8 7 3 5\n",
      "weigh 5 5 2 3\n",
      "weiner 6 5 3 3\n",
      "weird 5 5 2 3\n",
      "welcom 6 6 2 4\n",
      "welfar 6 6 2 4\n",
      "welkom 6 6 2 4\n",
      "well 4 3 1 3\n",
      "welland 7 6 2 5\n",
      "wellington 10 8 3 7\n",
      "wellstat 8 6 2 6\n",
      "wendi 5 5 2 3\n",
      "went 4 4 1 3\n",
      "werent 6 5 2 4\n",
      "west 4 4 1 3\n",
      "westafrica 10 9 4 6\n",
      "western 7 6 2 5\n",
      "weston 6 6 2 4\n",
      "westward 8 7 2 6\n",
      "wet 3 3 1 2\n",
      "weyerhaeus 10 8 6 4\n",
      "what 4 4 1 3\n",
      "wheat 5 5 2 3\n",
      "whecpart 8 8 2 6\n",
      "wher 4 4 1 3\n",
      "whereabout 10 9 5 5\n",
      "whether 7 5 2 5\n",
      "whimper 7 7 2 5\n",
      "whip 4 4 1 3\n",
      "whirlpool 9 7 3 6\n",
      "whistleblow 11 9 3 8\n",
      "white 5 5 2 3\n",
      "whitehous 9 8 4 5\n",
      "whitey 6 6 3 3\n",
      "who 3 3 1 2\n",
      "whole 5 5 2 3\n",
      "wholl 5 4 1 4\n",
      "wi 2 2 1 1\n",
      "wichita 7 6 3 4\n",
      "wide 4 4 2 2\n",
      "widen 5 5 2 3\n",
      "wider 5 5 2 3\n",
      "widespread 10 8 4 6\n",
      "widodo 6 4 3 3\n",
      "widow 5 4 2 3\n",
      "wife 4 4 2 2\n",
      "wikileak 8 6 4 4\n",
      "wikipedia 9 7 5 4\n",
      "wild 4 4 1 3\n",
      "wilder 6 6 2 4\n",
      "wildfir 7 6 2 5\n",
      "wildlif 7 5 2 5\n",
      "willem 6 5 2 4\n",
      "william 7 5 3 4\n",
      "wilton 6 6 2 4\n",
      "win 3 3 1 2\n",
      "wind 4 4 1 3\n",
      "window 6 5 2 4\n",
      "windpip 7 5 2 5\n",
      "wine 4 4 2 2\n",
      "winfield 8 7 3 5\n",
      "wing 4 4 1 3\n",
      "winnipeg 8 6 3 5\n",
      "winst 5 5 1 4\n",
      "winter 6 6 2 4\n",
      "wipe 4 4 2 2\n",
      "wipha 5 5 2 3\n",
      "wire 4 4 2 2\n",
      "wiretap 7 7 3 4\n",
      "wisconsin 9 6 3 6\n",
      "wise 4 4 2 2\n",
      "wish 4 4 1 3\n",
      "wit 3 3 1 2\n",
      "withdraw 8 7 2 6\n",
      "within 6 5 2 4\n",
      "without 7 6 3 4\n",
      "wive 4 4 2 2\n",
      "wobbl 5 4 1 4\n",
      "woe 3 3 2 1\n",
      "wojciech 8 7 3 5\n",
      "woke 4 4 2 2\n",
      "wolf 4 4 1 3\n",
      "wolfpass 8 7 2 6\n",
      "woman 5 5 2 3\n",
      "women 5 5 2 3\n",
      "wont 4 4 1 3\n",
      "woo 3 2 2 1\n",
      "wood 4 3 2 2\n",
      "wooden 6 5 3 3\n",
      "woodmont 8 6 3 5\n",
      "woodsid 7 5 3 4\n",
      "woolwich 8 6 3 5\n",
      "word 4 4 1 3\n",
      "work 4 4 1 3\n",
      "worker 6 5 2 4\n",
      "workshop 8 7 2 6\n",
      "world 5 5 1 4\n",
      "worldbank 9 9 2 7\n",
      "worldwid 8 6 2 6\n",
      "worri 5 4 2 3\n",
      "worsen 6 6 2 4\n",
      "worst 5 5 1 4\n",
      "worth 5 5 1 4\n",
      "would 5 5 2 3\n",
      "wouldnt 7 7 2 5\n",
      "wound 5 5 2 3\n",
      "wrangler 8 7 2 6\n",
      "wrap 4 4 1 3\n",
      "wreath 6 6 2 4\n",
      "wreck 5 5 1 4\n",
      "wrestl 6 6 1 5\n",
      "wright 6 6 1 5\n",
      "write 5 5 2 3\n",
      "writethru 9 7 3 6\n",
      "wrong 5 5 1 4\n",
      "wrongli 7 7 2 5\n",
      "wrote 5 5 2 3\n",
      "wsoad 5 5 2 3\n",
      "wtuydwocais 11 10 5 6\n",
      "wva 3 3 1 2\n",
      "wxnaif 6 6 2 4\n",
      "wxyz 4 4 1 3\n",
      "wyo 3 3 2 1\n",
      "xerox 5 4 2 3\n",
      "xi 2 2 1 1\n",
      "xilai 5 4 3 2\n",
      "xinjiang 8 6 3 5\n",
      "xpacb 5 5 1 4\n",
      "xvcuyrl 7 7 2 5\n",
      "xyjzjuktbo 10 9 3 7\n",
      "yahoo 5 4 4 1\n",
      "yangon 6 5 3 3\n",
      "yanukovych 10 9 5 5\n",
      "yard 4 4 2 2\n",
      "yarnel 6 6 3 3\n",
      "yasmin 6 6 3 3\n",
      "ybopwbjegk 10 9 3 7\n",
      "year 4 4 3 1\n",
      "yellowston 10 8 4 6\n",
      "yemen 5 4 3 2\n",
      "yemeni 6 5 4 2\n",
      "yemini 6 5 4 2\n",
      "yen 3 3 2 1\n",
      "yerevan 7 6 4 3\n",
      "yet 3 3 2 1\n",
      "yield 5 5 3 2\n",
      "yogurt 6 6 3 3\n",
      "york 4 4 2 2\n",
      "yorker 6 5 3 3\n",
      "yosemit 7 7 4 3\n",
      "young 5 5 3 2\n",
      "youngest 8 8 4 4\n",
      "your 4 4 3 1\n",
      "youth 5 5 3 2\n",
      "yr 2 2 1 1\n",
      "yuan 4 4 3 1\n",
      "yulia 5 5 4 1\n",
      "yum 3 3 2 1\n",
      "yyg 3 2 2 1\n",
      "zabul 5 5 2 3\n",
      "zagoklj 7 7 2 5\n",
      "zardari 7 5 3 4\n",
      "zarif 5 5 2 3\n",
      "zarrillo 8 6 3 5\n",
      "zawahri 7 6 3 4\n",
      "zealand 7 6 3 4\n",
      "zealot 6 6 3 3\n",
      "zeidan 6 6 3 3\n",
      "zero 4 4 2 2\n",
      "zeta 4 4 2 2\n",
      "zidan 5 5 2 3\n",
      "zient 5 5 2 3\n",
      "zimbabw 7 6 2 5\n",
      "zimbabwean 10 8 4 6\n",
      "zimmer 6 5 2 4\n",
      "zimmerman 9 7 3 6\n",
      "zimvwodawr 10 9 3 7\n",
      "zipwir 6 5 2 4\n",
      "zone 4 4 2 2\n",
      "zoo 3 2 2 1\n",
      "zookeep 7 5 4 3\n",
      "zuckerberg 10 8 3 7\n",
      "zuma 4 4 2 2\n"
     ]
    }
   ],
   "source": [
    "print('word','#char','#uniquechar','#vowels','#conson')\n",
    "for ii in list(bow_dataset_df.columns):\n",
    "    length=len(ii)\n",
    "    unique=len(set(ii))\n",
    "    num_vow=sum(ii.count(c) for c in vowels)\n",
    "    num_cons=sum(ii.count(c) for c in consonants)\n",
    "    if True:\n",
    "        print(ii,length,unique,num_vow,num_cons)\n",
    "\n",
    "#[[ii,len(ii)-len(set(ii)),len(set(ii)),sum(ii.count(c) for c in vowels),sum(ii.count(c) for c in consonants)] # if len(ii)-len(set(ii))>=4 and sum(ii.count(c) for c in vowels)==6 and sum(ii.count(c) for c in consonants)==3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#this is loading the data for the S&P500 index which we'll be trying to predict\n",
    "sp500=[]\n",
    "with open('data/SP500am.csv','r') as mycsvfile:\n",
    "    reader=csv.reader(mycsvfile)\n",
    "    for row in reader:\n",
    "        row[0]=re.sub('-','',row[0])\n",
    "        sp500+=[row]\n",
    "\n",
    "sp500red=sp500[:963][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20170123',\n",
       " '2267.780029',\n",
       " '2271.780029',\n",
       " '2257.02002',\n",
       " '2265.199951',\n",
       " '3152710000',\n",
       " '2265.199951']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp500[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1022,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "days=list(range(20130401,20130431))+list(range(20130501,20130532))+list(range(20130601,20130631))+list(range(20130701,20130732))+list(range(20130801,20130832))+list(range(20130901,20130931))+list(range(20131001,20131032))\n",
    "days=[str(date)[:4]+'-'+str(date)[4:6]+'-'+str(date)[6:] for date in days]+['2013-11-01']\n",
    "#prev_days=['2013-03-31']+days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(3,int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dataset preparation\n",
    "x_tfidf=[]\n",
    "y_tfidf=[]\n",
    "j=0\n",
    "for i,tomorrow in enumerate(days[1:]):\n",
    "    next_bizday=sp500red[j+1][0]\n",
    "    if tomorrow==next_bizday:\n",
    "        after_we=0.\n",
    "        latest_bizday=sp500red[j][0]\n",
    "        today=days[i]\n",
    "        if latest_bizday!=today:\n",
    "            after_we=1.\n",
    "        x_tfidf+=[list(tfidf_dataset_df.iloc[i])+[after_we,float(sp500red[j][-1])]]\n",
    "        y_tfidf+=[float(sp500red[j+1][-1])]\n",
    "        j+=1\n",
    "\n",
    "x_tfidf=np.array(x_tfidf)\n",
    "y_tfidf=np.array(y_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dataset preparation(bullish/bearish classifier)\n",
    "x_tfidf_class=[]\n",
    "y_tfidf_class=[]\n",
    "j=0\n",
    "for i,tomorrow in enumerate(days[1:]):\n",
    "    next_bizday=sp500red[j+1][0]\n",
    "    if tomorrow==next_bizday:\n",
    "        after_we=0.\n",
    "        latest_bizday=sp500red[j][0]\n",
    "        today=days[i]\n",
    "        if latest_bizday!=today:\n",
    "            after_we=1.\n",
    "        x_tfidf_class+=[list(tfidf_dataset_df.iloc[i])+[after_we,float(sp500red[j][-1])]]\n",
    "        y_tfidf_class+=[(np.sign(float(sp500red[j+1][-1])-float(sp500red[j][-1]))+1)/2.]\n",
    "        j+=1\n",
    "\n",
    "x_tfidf_class=np.array(x_tfidf_class)\n",
    "y_tfidf_class=np.array(y_tfidf_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dataset preparation\n",
    "x_bow=[]\n",
    "y_bow=[]\n",
    "j=0\n",
    "for i,tomorrow in enumerate(days[1:]):\n",
    "    next_bizday=sp500red[j+1][0]\n",
    "    if tomorrow==next_bizday:\n",
    "        after_we=0.\n",
    "        latest_bizday=sp500red[j][0]\n",
    "        today=days[i]\n",
    "        if latest_bizday!=today:\n",
    "            after_we=1.\n",
    "        x_bow+=[list(bow_dataset_df.iloc[i])+[after_we,float(sp500red[j][-1])]]\n",
    "        y_bow+=[float(sp500red[j+1][-1])]\n",
    "        j+=1\n",
    "\n",
    "x_bow=np.array(x_bow)\n",
    "y_bow=np.array(y_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "key_cols=list(tfidf_dataset_df.columns)+['*weekend?','*yesterdayS&P']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1126,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6607"
      ]
     },
     "execution_count": 1126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(key_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(151, 6607)"
      ]
     },
     "execution_count": 1127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_tfidf_class),len(x_tfidf_class[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve\n",
    "from sklearn.linear_model import Lasso, Ridge, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out different regressors on the data, no luck so far :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#this function executes k-fold validation on a particular regressor model chosen by the user, \n",
    "#it outputs k-fold average rmse for the training set, validation set, and a benchmark \"flat\" \n",
    "#model which consists of predicting the same for tomorrow as today's closing\n",
    "\n",
    "def scores(y,yhat,thres):\n",
    "    score=[0,0,0,0]\n",
    "    for ii in zip(yhat,y):\n",
    "        temp=2*np.ceil(ii[0]-thres)-ii[1]\n",
    "        for jj in temp:\n",
    "            score[int(jj)]+=1\n",
    "    fp=score[2]\n",
    "    fn=score[-1]\n",
    "    tp=score[1]\n",
    "    tn=score[0]\n",
    "    #print(fp,fn,tp,tn)\n",
    "    if fn==0:\n",
    "        rec=1.\n",
    "    else:\n",
    "        rec=tp/(tp+fn)\n",
    "    if fp==0:\n",
    "        prec=1.\n",
    "    else:\n",
    "        prec=tp/(tp+fp)\n",
    "    if prec==0. and rec==0.:\n",
    "        f1=0.\n",
    "    else:\n",
    "        f1=2*rec*prec/(rec+prec)\n",
    "    if fp+fn==0:\n",
    "        ji=1.\n",
    "    else:\n",
    "        ji=tp/(fp+fn+tp)\n",
    "    return rec,prec,f1,ji\n",
    "\n",
    "\n",
    "def kfold_val(n_folds_val,x_trainval,y_trainval,regressor,parm,seed):\n",
    "    kf_val = KFold(n_splits=n_folds_val,shuffle=True,random_state=seed)\n",
    "    avg_rms_mod_val=0\n",
    "    avg_rms_mod_train=0\n",
    "    for train_index, val_index in kf_val.split(x_trainval):\n",
    "        x_train, x_val = x_trainval[train_index], x_trainval[val_index]\n",
    "        y_train, y_val = y_trainval[train_index], y_trainval[val_index]\n",
    "        if regressor in {Lasso,Ridge}:\n",
    "            model=regressor(alpha=parm)\n",
    "        elif regressor in {RandomForestRegressor,}:\n",
    "            model=regressor(n_estimators=parm[0],max_features=parm[1])\n",
    "        elif regressor in {MLPRegressor,}:\n",
    "            model=regressor(activation=parm[0],hidden_layer_sizes=parm[1])\n",
    "        elif regressor in {AdaBoostRegressor,}:\n",
    "            model=regressor(n_estimators=parm)\n",
    "        elif regressor in {KNeighborsRegressor,}:\n",
    "            model=regressor(n_neighbors=parm)  \n",
    "        elif regressor in {SVR,}:\n",
    "            model=regressor(C=parm[0],kernel=parm[1])  \n",
    "        else:\n",
    "            print('houston, we have a unknown model problem')\n",
    "            return\n",
    "        model.fit(x_train,y_train)\n",
    "        avg_rms_mod_val+=np.sqrt((sum((model.predict(x_val)-y_val)**2)/len(y_val)))\n",
    "        avg_rms_mod_train+=np.sqrt((sum((model.predict(x_train)-y_train)**2)/len(y_train)))\n",
    "    avg_rms_mod_val=avg_rms_mod_val/n_folds_val\n",
    "    avg_rms_mod_train=avg_rms_mod_train/n_folds_val\n",
    "    print('avg_train_rmse:',avg_rms_mod_train,'avg_validation_rmse:',avg_rms_mod_val)\n",
    "    return\n",
    "\n",
    "def kfold_test(x_trainval,y_trainval,x_test,y_test,regressor,parm):\n",
    "    #x_trainval, x_test, y_trainval, y_test = train_test_split(x, y,test_size=test_fraction)\n",
    "    coeff=True\n",
    "    if regressor in {Lasso,Ridge}:\n",
    "            model=regressor(alpha=parm)\n",
    "    elif regressor in {RandomForestRegressor,}:\n",
    "            model=regressor(n_estimators=parm[0],max_features=parm[1],max_depth=parm[2])\n",
    "            coeff=False\n",
    "    elif regressor in {MLPRegressor,}:\n",
    "            model=regressor(activation=parm[0],hidden_layer_sizes=parm[1])\n",
    "            coeff=False\n",
    "    elif regressor in {AdaBoostRegressor,}:\n",
    "            model=regressor(n_estimators=parm)\n",
    "            coeff=False\n",
    "    elif regressor in {KNeighborsRegressor,}:\n",
    "            model=regressor(n_neighbors=parm)  \n",
    "            coeff=False\n",
    "    elif regressor in {SVR,}:\n",
    "            model=regressor(C=parm[0],kernel=parm[1])  \n",
    "            coeff=False\n",
    "    else:\n",
    "            print('houston, we have a unknown model problem')        \n",
    "            return\n",
    "    model.fit(x_trainval,y_trainval)\n",
    "\n",
    "    rms_mod_test=np.sqrt((sum((model.predict(x_test)-y_test)**2)/len(y_test)))\n",
    "    rms_rand_test=np.sqrt((sum((x_test[:,-1]-y_test)**2)/len(y_test)))\n",
    "    print('model_test_rmse:',rms_mod_test,'flat_test_rmse:',rms_rand_test)\n",
    "   \n",
    "    if coeff:\n",
    "        return model.coef_\n",
    "    return\n",
    "\n",
    "def kfold_val_class(n_folds_val,x_trainval,y_trainval,classifier,parm,seed,thres):\n",
    "    #random_int=random.rand_int(1,1000)\n",
    "    kf_val = KFold(n_splits=n_folds_val,shuffle=True,random_state=seed)\n",
    "    avg_scores_train=np.zeros(3)\n",
    "    avg_scores_val=np.zeros(3)\n",
    "    for train_index, val_index in kf_val.split(x_trainval):\n",
    "        x_train, x_val = x_trainval[train_index], x_trainval[val_index]\n",
    "        y_train, y_val = y_trainval[train_index], y_trainval[val_index]\n",
    "        if classifier in {LogisticRegression,}:\n",
    "            model=classifier(penalty=parm[0], C=parm[1])\n",
    "        elif classifier in {RandomForestClassifier,}:\n",
    "            model=classifier(n_estimators=parm[0],max_features=parm[1],max_depth=parm[2])\n",
    "        elif classifier in {MLPClassifier,}:\n",
    "            model=classifier(activation=parm[0],hidden_layer_sizes=parm[1])\n",
    "        elif classifier in {AdaBoostClassifier,}:\n",
    "            model=classifier(n_estimators=parm)\n",
    "        elif classifier in {KNeighborsClassifier,}:\n",
    "            model=classifier(n_neighbors=parm)  \n",
    "        elif classifier in {SVC,}:\n",
    "            model=classifier(C=parm[0],kernel=parm[1])  \n",
    "        else:\n",
    "            print('houston, we have a unknown model problem')\n",
    "            return\n",
    "        \n",
    "        model.fit(x_train,y_train)\n",
    "        avg_scores_train+=np.array(scores(y_train,model.predict(x_train),[thres])[:3])\n",
    "        avg_scores_val+=np.array(scores(y_val,model.predict(x_val),[thres])[:3])\n",
    "\n",
    "    avg_scores_train=avg_scores_train/n_folds_val    \n",
    "    avg_scores_val=avg_scores_val/n_folds_val\n",
    "    print('avg_train_rec,prec,F1:',list(avg_scores_train),'avg_validation_rec,prec,F1:',list(avg_scores_val))\n",
    "    return\n",
    "\n",
    "def kfold_test_class(x_trainval,y_trainval,x_test,y_test,classifier,parm,thres):\n",
    "    #x_trainval, x_test, y_trainval, y_test = train_test_split(x, y,test_size=test_fraction)\n",
    "    coeff=True\n",
    "    if classifier in {LogisticRegression,}:\n",
    "        model=classifier(penalty=parm[0], C=parm[1])\n",
    "    elif classifier in {RandomForestClassifier,}:\n",
    "        model=classifier(n_estimators=parm[0],max_features=parm[1],max_depth=parm[2])\n",
    "        coeff=False\n",
    "    elif classifier in {MLPClassifier,}:\n",
    "        model=classifier(activation=parm[0],hidden_layer_sizes=parm[1])\n",
    "        coeff=False\n",
    "    elif classifier in {AdaBoostClassifier,}:\n",
    "        model=classifier(n_estimators=parm)\n",
    "        coeff=False\n",
    "    elif classifier in {KNeighborsClassifier,}:\n",
    "        model=classifier(n_neighbors=parm)  \n",
    "        coeff=False\n",
    "    elif classifier in {SVC,}:\n",
    "        model=classifier(C=parm[0],kernel=parm[1])  \n",
    "        coeff=False\n",
    "    else:\n",
    "        print('houston, we have a unknown model problem')\n",
    "        return\n",
    "            \n",
    "    model.fit(x_trainval,y_trainval)\n",
    "\n",
    "    scores_test=np.array(scores(y_test,model.predict(x_test),[thres])[:3])\n",
    "    #avg_scores_val+=np.array(scores(y_val,model.predict(x_val),[thres])[:3])\n",
    "    print('test_rec,prec,F1:',list(scores_test))   \n",
    "    if coeff:\n",
    "        return model.coef_\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6607\n"
     ]
    }
   ],
   "source": [
    "y_tfidf_regclass=np.array(list(zip(y_tfidf,y_tfidf_class)))\n",
    "x_tfidf_trainval, x_tfidf_test, y_tfidf_regclass_trainval, y_tfidf_regclass_test = train_test_split(x_tfidf, y_tfidf_regclass,test_size=0.2)\n",
    "y_tfidf_trainval=y_tfidf_regclass_trainval[:,0]\n",
    "y_tfidf_test=y_tfidf_regclass_test[:,0]\n",
    "y_tfidf_class_trainval=y_tfidf_regclass_trainval[:,1]\n",
    "y_tfidf_class_test=y_tfidf_regclass_test[:,1]\n",
    "print(len(x_tfidf_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of neighbors used = 15\n",
      "avg_train_rec,prec,F1: [0.8926857734013135, 0.68977732715017093, 0.77741402257869652] avg_validation_rec,prec,F1: [0.78452380952380951, 0.60476911976911985, 0.66895572263993308]\n",
      "number of neighbors used = 16\n",
      "avg_train_rec,prec,F1: [0.84211418373841551, 0.70555993737259715, 0.76651441976819235] avg_validation_rec,prec,F1: [0.77023809523809528, 0.67176767676767679, 0.70201359356158111]\n",
      "number of neighbors used = 17\n",
      "avg_train_rec,prec,F1: [0.8941309948465348, 0.68426420811252431, 0.77435015298160526] avg_validation_rec,prec,F1: [0.8320238095238095, 0.60725108225108226, 0.69186299081035929]\n",
      "number of neighbors used = 18\n",
      "avg_train_rec,prec,F1: [0.8645346678489787, 0.69705512138400816, 0.77088738445239946] avg_validation_rec,prec,F1: [0.79345238095238091, 0.61661616161616162, 0.6817143159558019]\n",
      "number of neighbors used = 19\n",
      "avg_train_rec,prec,F1: [0.91040411739270388, 0.66973235481814553, 0.77083834364492021] avg_validation_rec,prec,F1: [0.86246031746031748, 0.62070707070707076, 0.71155744754041961]\n",
      "number of neighbors used = 20\n",
      "avg_train_rec,prec,F1: [0.86399066108021338, 0.68430203117972765, 0.76191020581782209] avg_validation_rec,prec,F1: [0.80773809523809526, 0.63094516594516592, 0.69275701508673637]\n",
      "number of neighbors used = 21\n",
      "avg_train_rec,prec,F1: [0.91326673472854336, 0.65765790674757418, 0.76351455836787407] avg_validation_rec,prec,F1: [0.82023809523809521, 0.59854978354978361, 0.67452614379084963]\n",
      "number of neighbors used = 22\n",
      "avg_train_rec,prec,F1: [0.86420786412884765, 0.66633056459833306, 0.75133546261615536] avg_validation_rec,prec,F1: [0.79662698412698407, 0.60406926406926409, 0.66772015823873399]\n",
      "number of neighbors used = 23\n",
      "avg_train_rec,prec,F1: [0.8981363807395415, 0.64506514780815793, 0.74985556406632048] avg_validation_rec,prec,F1: [0.86952380952380948, 0.60151515151515156, 0.69651444788441685]\n",
      "number of neighbors used = 24\n",
      "avg_train_rec,prec,F1: [0.855085268842951, 0.64942205912583406, 0.73721459627056052] avg_validation_rec,prec,F1: [0.81595238095238098, 0.60750000000000004, 0.68096147230822146]\n",
      "number of neighbors used = 25\n",
      "avg_train_rec,prec,F1: [0.92492649872807908, 0.63632754850643991, 0.75292041775093177] avg_validation_rec,prec,F1: [0.90857142857142859, 0.60550505050505055, 0.7163954002653693]\n",
      "number of neighbors used = 26\n",
      "avg_train_rec,prec,F1: [0.88611057365227686, 0.63993318552941791, 0.74160854061704584] avg_validation_rec,prec,F1: [0.88496031746031745, 0.6193939393939395, 0.71600275197798413]\n",
      "number of neighbors used = 27\n",
      "avg_train_rec,prec,F1: [0.94426832313136089, 0.63002201130326996, 0.75523396912401197] avg_validation_rec,prec,F1: [0.9514285714285714, 0.6133333333333334, 0.7358690844758955]\n",
      "number of neighbors used = 28\n",
      "avg_train_rec,prec,F1: [0.92936154275926008, 0.63345327091978731, 0.75263604889537061] avg_validation_rec,prec,F1: [0.90603174603174597, 0.60227272727272729, 0.70980392156862737]\n",
      "number of neighbors used = 29\n",
      "avg_train_rec,prec,F1: [0.96238644293955977, 0.6258741448765146, 0.75810979500685782] avg_validation_rec,prec,F1: [0.93142857142857127, 0.60833333333333328, 0.72253575114256219]\n"
     ]
    }
   ],
   "source": [
    "seed=random.randint(1,10000)\n",
    "for n_neigh in range(15,30):\n",
    "    print('number of neighbors used =',n_neigh)\n",
    "    kfold_val_class(10,x_tfidf_trainval,y_tfidf_class_trainval,KNeighborsClassifier,n_neigh,seed,0.5)\n",
    "#for C in [1.0+0.1*i for i in range(-5,5)]:\n",
    "#    print('C=',C)\n",
    "#    kfold_val_class(11,x_tfidf_class_trainval,y_tfidf_class_trainval,SVC,[C,'poly'],0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1132,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_rec,prec,F1: [1.0, 0.51724137931034486, 0.68181818181818188]\n"
     ]
    }
   ],
   "source": [
    "kfold_test_class(x_tfidf_trainval,y_tfidf_class_trainval,x_tfidf_test,y_tfidf_class_test,KNeighborsClassifier,27,0.5)\n",
    "#kfold_test_class(x_tfidf_class_trainval,y_tfidf_class_trainval,x_tfidf_class_test,y_tfidf_class_test,SVC,[0.8,'poly'],0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "penalty = l1, C = 2.25\n",
      "avg_train_rec,prec,F1: [0.9970149253731343, 0.63318221342405268, 0.77442941554212519] avg_validation_rec,prec,F1: [0.98750000000000004, 0.62196969696969695, 0.75867315347191489]\n",
      "penalty = l1, C = 2.2600000000000002\n",
      "avg_train_rec,prec,F1: [0.9970149253731343, 0.63382286572457691, 0.77489433523781404] avg_validation_rec,prec,F1: [0.98750000000000004, 0.62196969696969695, 0.75867315347191489]\n",
      "penalty = l1, C = 2.27\n",
      "avg_train_rec,prec,F1: [0.9970149253731343, 0.63382286572457691, 0.77489433523781404] avg_validation_rec,prec,F1: [0.97638888888888897, 0.61969696969696975, 0.75295886775762921]\n",
      "penalty = l1, C = 2.2800000000000002\n",
      "avg_train_rec,prec,F1: [0.9970149253731343, 0.63382286572457691, 0.77489433523781404] avg_validation_rec,prec,F1: [0.97638888888888897, 0.61969696969696975, 0.75295886775762921]\n",
      "penalty = l1, C = 2.29\n",
      "avg_train_rec,prec,F1: [0.9970149253731343, 0.63382286572457691, 0.77489433523781404] avg_validation_rec,prec,F1: [0.97638888888888897, 0.61969696969696975, 0.75295886775762921]\n",
      "penalty = l1, C = 2.3000000000000003\n",
      "avg_train_rec,prec,F1: [0.99549977385798272, 0.63346347668594261, 0.77417354100416802] avg_validation_rec,prec,F1: [0.97638888888888897, 0.61969696969696975, 0.75295886775762921]\n",
      "penalty = l1, C = 2.31\n",
      "avg_train_rec,prec,F1: [0.9970149253731343, 0.63442484236428931, 0.77534466458315621] avg_validation_rec,prec,F1: [0.97638888888888897, 0.61969696969696975, 0.75295886775762921]\n",
      "penalty = l1, C = 2.3200000000000003\n",
      "avg_train_rec,prec,F1: [0.99549977385798272, 0.634065453325655, 0.77462387034951008] avg_validation_rec,prec,F1: [0.97638888888888897, 0.61969696969696975, 0.75295886775762921]\n",
      "penalty = l1, C = 2.33\n",
      "avg_train_rec,prec,F1: [0.99549977385798272, 0.63465389223185098, 0.77506542879106866] avg_validation_rec,prec,F1: [0.97638888888888897, 0.61969696969696975, 0.75295886775762921]\n",
      "penalty = l1, C = 2.3400000000000003\n",
      "avg_train_rec,prec,F1: [0.99549977385798272, 0.63465389223185098, 0.77506542879106866] avg_validation_rec,prec,F1: [0.97638888888888897, 0.61969696969696975, 0.75295886775762921]\n",
      "penalty = l1, C = 2.35\n",
      "avg_train_rec,prec,F1: [0.99549977385798272, 0.63523580018459291, 0.77550903680289818] avg_validation_rec,prec,F1: [0.97638888888888897, 0.61969696969696975, 0.75295886775762921]\n",
      "penalty = l1, C = 2.36\n",
      "avg_train_rec,prec,F1: [0.99549977385798272, 0.63643783439633594, 0.7764087219131095] avg_validation_rec,prec,F1: [0.97638888888888897, 0.61969696969696975, 0.75295886775762921]\n",
      "penalty = l1, C = 2.37\n",
      "avg_train_rec,prec,F1: [0.99549977385798272, 0.63643783439633594, 0.7764087219131095] avg_validation_rec,prec,F1: [0.97638888888888897, 0.61969696969696975, 0.75295886775762921]\n",
      "penalty = l1, C = 2.38\n",
      "avg_train_rec,prec,F1: [0.99549977385798272, 0.63643783439633594, 0.7764087219131095] avg_validation_rec,prec,F1: [0.96388888888888891, 0.6166666666666667, 0.74664307828394505]\n",
      "penalty = l1, C = 2.39\n",
      "avg_train_rec,prec,F1: [0.99549977385798272, 0.63643783439633594, 0.7764087219131095] avg_validation_rec,prec,F1: [0.96388888888888891, 0.6166666666666667, 0.74664307828394505]\n",
      "penalty = l1, C = 2.4\n",
      "avg_train_rec,prec,F1: [0.99549977385798272, 0.63703737592340359, 0.77685535573905384] avg_validation_rec,prec,F1: [0.96388888888888891, 0.6166666666666667, 0.74664307828394505]\n",
      "penalty = l1, C = 2.41\n",
      "avg_train_rec,prec,F1: [0.99549977385798272, 0.63703737592340359, 0.77685535573905384] avg_validation_rec,prec,F1: [0.96388888888888891, 0.6166666666666667, 0.74664307828394505]\n",
      "penalty = l1, C = 2.42\n",
      "avg_train_rec,prec,F1: [0.99549977385798272, 0.63703737592340359, 0.77685535573905384] avg_validation_rec,prec,F1: [0.96388888888888891, 0.6166666666666667, 0.74664307828394505]\n",
      "penalty = l1, C = 2.43\n",
      "avg_train_rec,prec,F1: [0.99549977385798272, 0.63762581482959946, 0.7772969141806122] avg_validation_rec,prec,F1: [0.96388888888888891, 0.6166666666666667, 0.74664307828394505]\n",
      "penalty = l1, C = 2.44\n",
      "avg_train_rec,prec,F1: [0.99549977385798272, 0.63762581482959946, 0.7772969141806122] avg_validation_rec,prec,F1: [0.96388888888888891, 0.6166666666666667, 0.74664307828394505]\n"
     ]
    }
   ],
   "source": [
    "seed=random.randint(1,10000)\n",
    "for C in [2.35+.01*i for i in range(-10,10)]:\n",
    "    print('penalty = l1, C =',C)\n",
    "    kfold_val_class(10,x_tfidf_trainval,y_tfidf_class_trainval,LogisticRegression,['l1',C],seed,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_rec,prec,F1: [1.0, 0.4838709677419355, 0.65217391304347827]\n"
     ]
    }
   ],
   "source": [
    "aa=kfold_test_class(x_tfidf_trainval,y_tfidf_class_trainval,x_tfidf_test,y_tfidf_class_test,LogisticRegression,['l1',2.35],0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=0.8, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='poly',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 896,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model=LogisticRegression(penalty='l2', C=1.) #, dual=False, tol=0.0001,, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='liblinear', max_iter=100, multi_class='ovr', verbose=0, warm_start=False, n_jobs=1)\n",
    "#model=RandomForestClassifier(n_estimators=10,max_features=4000,max_depth=5)#,max_features=7000)\n",
    "#model=AdaBoostClassifier(n_estimators=20)\n",
    "#model=MLPClassifier(activation='logistic',hidden_layer_sizes=(100,10,5))\n",
    "#model=KNeighborsClassifier(n_neighbors=15)\n",
    "model=SVC(C=.8,kernel='poly')\n",
    "model.fit(x_tfidf_class_trainval,y_tfidf_class_trainval)\n",
    "#model.fit(x_bow_class_trainval,y_bow_class_trainval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1045,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  0.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.]),\n",
       " array([ 0.,  1.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  1.,  1.,  1.,  0.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,\n",
       "         1.,  0.,  0.,  0.,  0.]))"
      ]
     },
     "execution_count": 1045,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_tfidf_class_test),y_tfidf_class_test\n",
    "#model.predict(x_bow_class_test),y_bow_class_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7250"
      ]
     },
     "execution_count": 1122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6607"
      ]
     },
     "execution_count": 1102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(key_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha = 0.084\n",
      "avg_train_rmse: 10.3480002029 avg_validation_rmse: 11.5653321874\n",
      "alpha = 0.08410000000000001\n",
      "avg_train_rmse: 10.3506782103 avg_validation_rmse: 11.5654101171\n",
      "alpha = 0.08420000000000001\n",
      "avg_train_rmse: 10.3533588021 avg_validation_rmse: 11.5654879158\n",
      "alpha = 0.0843\n",
      "avg_train_rmse: 10.3560419379 avg_validation_rmse: 11.5655655445\n",
      "alpha = 0.0844\n",
      "avg_train_rmse: 10.3587274889 avg_validation_rmse: 11.5656447032\n",
      "alpha = 0.0845\n",
      "avg_train_rmse: 10.3614157333 avg_validation_rmse: 11.5657240254\n",
      "alpha = 0.08460000000000001\n",
      "avg_train_rmse: 10.3641062877 avg_validation_rmse: 11.5658045491\n",
      "alpha = 0.08470000000000001\n",
      "avg_train_rmse: 10.3667992053 avg_validation_rmse: 11.5658861134\n",
      "alpha = 0.0848\n",
      "avg_train_rmse: 10.3694950168 avg_validation_rmse: 11.5659672315\n",
      "alpha = 0.0849\n",
      "avg_train_rmse: 10.3721929615 avg_validation_rmse: 11.5660490418\n",
      "alpha = 0.085\n",
      "avg_train_rmse: 10.3748932686 avg_validation_rmse: 11.566132381\n",
      "alpha = 0.08510000000000001\n",
      "avg_train_rmse: 10.3775961322 avg_validation_rmse: 11.5662157441\n",
      "alpha = 0.08520000000000001\n",
      "avg_train_rmse: 10.3803014448 avg_validation_rmse: 11.5662999623\n",
      "alpha = 0.0853\n",
      "avg_train_rmse: 10.3830092616 avg_validation_rmse: 11.5663842971\n",
      "alpha = 0.0854\n",
      "avg_train_rmse: 10.3857194027 avg_validation_rmse: 11.5664699222\n",
      "alpha = 0.0855\n",
      "avg_train_rmse: 10.3884322364 avg_validation_rmse: 11.5665557341\n",
      "alpha = 0.08560000000000001\n",
      "avg_train_rmse: 10.3911472004 avg_validation_rmse: 11.5666472119\n",
      "alpha = 0.08570000000000001\n",
      "avg_train_rmse: 10.3938642445 avg_validation_rmse: 11.5667404203\n",
      "alpha = 0.0858\n",
      "avg_train_rmse: 10.3965843067 avg_validation_rmse: 11.5668283254\n",
      "alpha = 0.0859\n",
      "avg_train_rmse: 10.3993068242 avg_validation_rmse: 11.5669174904\n"
     ]
    }
   ],
   "source": [
    "#seed=random.randint(1,10000)\n",
    "seed=1\n",
    "for alpha in [0.085+0.0001*i for i in range(-10,10)]:\n",
    "    print('alpha =',alpha)\n",
    "    kfold_val(10,x_tfidf_trainval,y_tfidf_trainval,Lasso,alpha,seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_test_rmse: 13.86624214 flat_test_rmse: 13.3308619043\n"
     ]
    }
   ],
   "source": [
    "aa=kfold_test(x_tfidf_trainval,y_tfidf_trainval,x_tfidf_test,y_tfidf_test,Lasso,0.085)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2976.0\n",
      "avg_train_rmse: 12.3144325135 avg_validation_rms: 11.8707710762\n",
      "2977.0\n",
      "avg_train_rmse: 12.3144413631 avg_validation_rms: 11.8707710511\n",
      "2978.0\n",
      "avg_train_rmse: 12.3144502141 avg_validation_rms: 11.8707710287\n",
      "2979.0\n",
      "avg_train_rmse: 12.3144590666 avg_validation_rms: 11.870771009\n",
      "2980.0\n",
      "avg_train_rmse: 12.3144679206 avg_validation_rms: 11.8707709922\n",
      "2981.0\n",
      "avg_train_rmse: 12.314476776 avg_validation_rms: 11.870770978\n",
      "2982.0\n",
      "avg_train_rmse: 12.3144856329 avg_validation_rms: 11.8707709667\n",
      "2983.0\n",
      "avg_train_rmse: 12.3144944912 avg_validation_rms: 11.870770958\n",
      "2984.0\n",
      "avg_train_rmse: 12.3145033511 avg_validation_rms: 11.8707709522\n",
      "2985.0\n",
      "avg_train_rmse: 12.3145122123 avg_validation_rms: 11.8707709491\n",
      "2986.0\n",
      "avg_train_rmse: 12.3145210751 avg_validation_rms: 11.8707709488\n",
      "2987.0\n",
      "avg_train_rmse: 12.3145299393 avg_validation_rms: 11.8707709512\n",
      "2988.0\n",
      "avg_train_rmse: 12.3145388049 avg_validation_rms: 11.8707709563\n",
      "2989.0\n",
      "avg_train_rmse: 12.3145476721 avg_validation_rms: 11.8707709643\n",
      "2990.0\n",
      "avg_train_rmse: 12.3145565407 avg_validation_rms: 11.870770975\n",
      "2991.0\n",
      "avg_train_rmse: 12.3145654108 avg_validation_rms: 11.8707709884\n",
      "2992.0\n",
      "avg_train_rmse: 12.3145742823 avg_validation_rms: 11.8707710046\n",
      "2993.0\n",
      "avg_train_rmse: 12.3145831553 avg_validation_rms: 11.8707710236\n",
      "2994.0\n",
      "avg_train_rmse: 12.3145920298 avg_validation_rms: 11.8707710453\n",
      "2995.0\n",
      "avg_train_rmse: 12.3146009058 avg_validation_rms: 11.8707710697\n"
     ]
    }
   ],
   "source": [
    "for alpha in [2986.+1.*i for i in range(-10,10)]:\n",
    "    print(alpha)\n",
    "    kfold_val(10,x_tfidf_trainval,y_tfidf_trainval,Ridge,alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1147,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_test_rmse: 11.8566884105 flat_test_rmse: 11.5372027387\n"
     ]
    }
   ],
   "source": [
    "aa=kfold_test(x_tfidf_trainval,y_tfidf_trainval,x_tfidf_test,y_tfidf_test,Ridge,2986.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of neighbors used = 1\n",
      "avg_train_rmse: 0.0 avg_validation_rmse: 18.7346395695\n",
      "number of neighbors used = 2\n",
      "avg_train_rmse: 9.21099271338 avg_validation_rmse: 15.9749167616\n",
      "number of neighbors used = 3\n",
      "avg_train_rmse: 10.8866941836 avg_validation_rmse: 14.7884607199\n",
      "number of neighbors used = 4\n",
      "avg_train_rmse: 11.5524183245 avg_validation_rmse: 14.2840436635\n",
      "number of neighbors used = 5\n",
      "avg_train_rmse: 11.6823533715 avg_validation_rmse: 13.5676823895\n",
      "number of neighbors used = 6\n",
      "avg_train_rmse: 11.7221060156 avg_validation_rmse: 13.1512017149\n",
      "number of neighbors used = 7\n",
      "avg_train_rmse: 11.7996893153 avg_validation_rmse: 13.2590311689\n",
      "number of neighbors used = 8\n",
      "avg_train_rmse: 11.9868059969 avg_validation_rmse: 13.2673248028\n",
      "number of neighbors used = 9\n",
      "avg_train_rmse: 12.2079140861 avg_validation_rmse: 13.5468349003\n",
      "number of neighbors used = 10\n",
      "avg_train_rmse: 12.4582087268 avg_validation_rmse: 13.4878606957\n",
      "number of neighbors used = 11\n",
      "avg_train_rmse: 12.6908831997 avg_validation_rmse: 13.4942128439\n",
      "number of neighbors used = 12\n",
      "avg_train_rmse: 12.9845522713 avg_validation_rmse: 13.6574739333\n",
      "number of neighbors used = 13\n",
      "avg_train_rmse: 13.2865916333 avg_validation_rmse: 13.9142358209\n",
      "number of neighbors used = 14\n",
      "avg_train_rmse: 13.673538237 avg_validation_rmse: 14.2694893053\n",
      "number of neighbors used = 15\n",
      "avg_train_rmse: 14.0374823251 avg_validation_rmse: 14.5604812333\n"
     ]
    }
   ],
   "source": [
    "seed=random.randint(1,10000)\n",
    "for n_neigh in range(1,16):\n",
    "    print('number of neighbors used =',n_neigh)\n",
    "    kfold_val(10,x_tfidf_trainval,y_tfidf_trainval,KNeighborsRegressor,n_neigh,seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_test_rmse: 12.4388299317 flat_test_rmse: 11.5372027387\n"
     ]
    }
   ],
   "source": [
    "aa=kfold_test(x_tfidf_trainval,y_tfidf_trainval,x_tfidf_test,y_tfidf_test,KNeighborsRegressor,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7341\n",
      "10\n",
      "avg_train_rmse: 6.56125169698 avg_validation_rms: 14.7165505646\n",
      "7341\n",
      "11\n",
      "avg_train_rmse: 6.64534840047 avg_validation_rms: 14.1072301852\n",
      "7341\n",
      "12\n",
      "avg_train_rmse: 6.33431200117 avg_validation_rms: 14.4841635548\n",
      "7341\n",
      "13\n",
      "avg_train_rmse: 6.32376295083 avg_validation_rms: 15.016477313\n",
      "7341\n",
      "14\n",
      "avg_train_rmse: 6.24080044741 avg_validation_rms: 14.668779199\n",
      "7341\n",
      "15\n",
      "avg_train_rmse: 6.33693202241 avg_validation_rms: 13.7060000257\n",
      "7341\n",
      "16\n",
      "avg_train_rmse: 6.08062693388 avg_validation_rms: 14.1038339637\n",
      "7341\n",
      "17\n",
      "avg_train_rmse: 6.156247695 avg_validation_rms: 14.0256138444\n",
      "7341\n",
      "18\n",
      "avg_train_rmse: 6.07984624958 avg_validation_rms: 14.3214477327\n",
      "7341\n",
      "19\n",
      "avg_train_rmse: 6.07170529101 avg_validation_rms: 13.9314306097\n",
      "7341\n",
      "20\n",
      "avg_train_rmse: 6.09830695159 avg_validation_rms: 13.6172442248\n",
      "7341\n",
      "21\n",
      "avg_train_rmse: 5.87636029423 avg_validation_rms: 13.7587788516\n",
      "7341\n",
      "22\n",
      "avg_train_rmse: 5.96883876264 avg_validation_rms: 14.3537764736\n",
      "7341\n",
      "23\n",
      "avg_train_rmse: 6.03990780636 avg_validation_rms: 13.7969236042\n",
      "7341\n",
      "24\n",
      "avg_train_rmse: 5.94405934147 avg_validation_rms: 14.1627551777\n"
     ]
    }
   ],
   "source": [
    "for n_max in range(7341,7342):\n",
    "    for n_estim in range(10,25):\n",
    "        print(n_max)\n",
    "        print(n_estim)\n",
    "        kfold_val(10,x_tfidf_trainval,y_tfidf_trainval,RandomForestRegressor,[n_estim,n_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_test_rms: 11.350971682 flat_test_rms: 11.2206389071\n"
     ]
    }
   ],
   "source": [
    "kfold_test(x_tfidf_trainval,y_tfidf_trainval,x_tfidf_test,y_tfidf_test,RandomForestRegressor,[20,7341])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_rmse: 15.3008899908 avg_validation_rms: 14.4453931395\n"
     ]
    }
   ],
   "source": [
    "kfold_val(10,x_tfidf_trainval,y_tfidf_trainval,MLPRegressor,['relu',(180,)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "avg_train_rmse: 15.3641291893 avg_validation_rmse: 19.3329761201\n",
      "2\n",
      "avg_train_rmse: 15.9220985727 avg_validation_rmse: 20.6843288173\n",
      "3\n",
      "avg_train_rmse: 11.3721752657 avg_validation_rmse: 16.3089140335\n",
      "4\n",
      "avg_train_rmse: 11.5384519305 avg_validation_rmse: 17.0213275843\n",
      "5\n",
      "avg_train_rmse: 9.8236857101 avg_validation_rmse: 16.562585481\n",
      "6\n",
      "avg_train_rmse: 10.1176905109 avg_validation_rmse: 15.7393770592\n",
      "7\n",
      "avg_train_rmse: 9.36440456592 avg_validation_rmse: 14.9201571742\n",
      "8\n",
      "avg_train_rmse: 9.3013977557 avg_validation_rmse: 15.5831706796\n",
      "9\n",
      "avg_train_rmse: 8.78863932644 avg_validation_rmse: 15.4780987087\n",
      "10\n",
      "avg_train_rmse: 8.86877017565 avg_validation_rmse: 15.3959011871\n",
      "11\n",
      "avg_train_rmse: 8.61072739689 avg_validation_rmse: 15.9081479263\n",
      "12\n",
      "avg_train_rmse: 8.54047018929 avg_validation_rmse: 15.4030816243\n",
      "13\n",
      "avg_train_rmse: 8.42922454503 avg_validation_rmse: 14.8967134826\n",
      "14\n",
      "avg_train_rmse: 8.31132330845 avg_validation_rmse: 14.3268475408\n",
      "15\n",
      "avg_train_rmse: 8.22950766821 avg_validation_rmse: 14.6657063045\n",
      "16\n",
      "avg_train_rmse: 8.23945002935 avg_validation_rmse: 15.1788293603\n",
      "17\n",
      "avg_train_rmse: 8.34845572197 avg_validation_rmse: 15.3018342964\n",
      "18\n",
      "avg_train_rmse: 8.03341218885 avg_validation_rmse: 14.6078371051\n",
      "19\n",
      "avg_train_rmse: 7.93764390588 avg_validation_rmse: 15.7387077076\n",
      "20\n",
      "avg_train_rmse: 8.04859380009 avg_validation_rmse: 15.4553580463\n",
      "21\n",
      "avg_train_rmse: 7.87442629546 avg_validation_rmse: 15.5158827976\n",
      "22\n",
      "avg_train_rmse: 7.7969695038 avg_validation_rmse: 14.9281429954\n",
      "23\n",
      "avg_train_rmse: 7.88821166255 avg_validation_rmse: 14.1677647045\n",
      "24\n",
      "avg_train_rmse: 7.92268420198 avg_validation_rmse: 14.9163048731\n",
      "25\n",
      "avg_train_rmse: 7.75700774595 avg_validation_rmse: 15.5213639149\n",
      "26\n",
      "avg_train_rmse: 7.81397019909 avg_validation_rmse: 14.9507698275\n",
      "27\n",
      "avg_train_rmse: 7.65184334125 avg_validation_rmse: 14.3822267496\n",
      "28\n",
      "avg_train_rmse: 7.89101354105 avg_validation_rmse: 15.0024021336\n",
      "29\n",
      "avg_train_rmse: 7.83005510617 avg_validation_rmse: 15.3961067535\n",
      "30\n",
      "avg_train_rmse: 7.71230517493 avg_validation_rmse: 15.5025285644\n",
      "31\n",
      "avg_train_rmse: 7.62008822929 avg_validation_rmse: 15.284892319\n",
      "32\n",
      "avg_train_rmse: 7.65434622411 avg_validation_rmse: 14.9304041529\n",
      "33\n",
      "avg_train_rmse: 7.60951368842 avg_validation_rmse: 15.2880694774\n",
      "34\n",
      "avg_train_rmse: 7.5810280454 avg_validation_rmse: 14.9427468098\n"
     ]
    }
   ],
   "source": [
    "for n_trees in range(1,35):\n",
    "    print(n_trees)\n",
    "    kfold_val(10,x_tfidf_trainval,y_tfidf_trainval,AdaBoostRegressor,n_trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_test_rmse: 14.1840667021 flat_test_rmse: 11.2206389071\n"
     ]
    }
   ],
   "source": [
    "kfold_test(x_tfidf_trainval,y_tfidf_trainval,x_tfidf_test,y_tfidf_test,AdaBoostRegressor,23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Convolution2D, MaxPooling2D, Input,ZeroPadding2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.models import Sequential, Model, model_from_json\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.regularizers import l1,l2,l1l2\n",
    "from keras.optimizers import Nadam, Adagrad\n",
    "\n",
    "#linear regressor\n",
    "inputsred=Input(shape=(len(x[0]),))\n",
    "\n",
    "#xo=Dense(100,activation='relu',W_regularizer=l1(0.005))(inputsred)\n",
    "#xo=LeakyReLU()(xo)\n",
    "#xo=Dropout(0.1)(xo)\n",
    "predsred=Dense(1, activation='relu',W_regularizer=l1(0.005))(inputsred)\n",
    "\n",
    "modelDred = Model(input=inputsred, output=predsred)\n",
    "\n",
    "nadam=Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)\n",
    "adagrad=Adagrad(lr=0.01, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "modelDred.compile(optimizer=adagrad,\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train,x_val,y_train,y_val=train_test_split(x,y,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 97 samples, validate on 11 samples\n",
      "Epoch 1/500\n",
      "0s - loss: 172.5491 - acc: 0.0000e+00 - val_loss: 206.9940 - val_acc: 0.0000e+00\n",
      "Epoch 2/500\n",
      "0s - loss: 172.5445 - acc: 0.0000e+00 - val_loss: 206.9854 - val_acc: 0.0000e+00\n",
      "Epoch 3/500\n",
      "0s - loss: 172.5419 - acc: 0.0000e+00 - val_loss: 206.9785 - val_acc: 0.0000e+00\n",
      "Epoch 4/500\n",
      "0s - loss: 172.5372 - acc: 0.0000e+00 - val_loss: 206.9696 - val_acc: 0.0000e+00\n",
      "Epoch 5/500\n",
      "0s - loss: 172.5332 - acc: 0.0000e+00 - val_loss: 206.9611 - val_acc: 0.0000e+00\n",
      "Epoch 6/500\n",
      "0s - loss: 172.5293 - acc: 0.0000e+00 - val_loss: 206.9532 - val_acc: 0.0000e+00\n",
      "Epoch 7/500\n",
      "0s - loss: 172.5258 - acc: 0.0000e+00 - val_loss: 206.9429 - val_acc: 0.0000e+00\n",
      "Epoch 8/500\n",
      "0s - loss: 172.5216 - acc: 0.0000e+00 - val_loss: 206.9315 - val_acc: 0.0000e+00\n",
      "Epoch 9/500\n",
      "0s - loss: 172.5155 - acc: 0.0000e+00 - val_loss: 206.9224 - val_acc: 0.0000e+00\n",
      "Epoch 10/500\n",
      "0s - loss: 172.5101 - acc: 0.0000e+00 - val_loss: 206.9132 - val_acc: 0.0000e+00\n",
      "Epoch 11/500\n",
      "0s - loss: 172.5057 - acc: 0.0000e+00 - val_loss: 206.9040 - val_acc: 0.0000e+00\n",
      "Epoch 12/500\n",
      "0s - loss: 172.5019 - acc: 0.0000e+00 - val_loss: 206.8969 - val_acc: 0.0000e+00\n",
      "Epoch 13/500\n",
      "0s - loss: 172.4980 - acc: 0.0000e+00 - val_loss: 206.8875 - val_acc: 0.0000e+00\n",
      "Epoch 14/500\n",
      "0s - loss: 172.4943 - acc: 0.0000e+00 - val_loss: 206.8807 - val_acc: 0.0000e+00\n",
      "Epoch 15/500\n",
      "0s - loss: 172.4902 - acc: 0.0000e+00 - val_loss: 206.8721 - val_acc: 0.0000e+00\n",
      "Epoch 16/500\n",
      "0s - loss: 172.4861 - acc: 0.0000e+00 - val_loss: 206.8630 - val_acc: 0.0000e+00\n",
      "Epoch 17/500\n",
      "0s - loss: 172.4825 - acc: 0.0000e+00 - val_loss: 206.8569 - val_acc: 0.0000e+00\n",
      "Epoch 18/500\n",
      "0s - loss: 172.4798 - acc: 0.0000e+00 - val_loss: 206.8500 - val_acc: 0.0000e+00\n",
      "Epoch 19/500\n",
      "0s - loss: 172.4757 - acc: 0.0000e+00 - val_loss: 206.8403 - val_acc: 0.0000e+00\n",
      "Epoch 20/500\n",
      "0s - loss: 172.4703 - acc: 0.0000e+00 - val_loss: 206.8315 - val_acc: 0.0000e+00\n",
      "Epoch 21/500\n",
      "0s - loss: 172.4665 - acc: 0.0000e+00 - val_loss: 206.8230 - val_acc: 0.0000e+00\n",
      "Epoch 22/500\n",
      "0s - loss: 172.4624 - acc: 0.0000e+00 - val_loss: 206.8150 - val_acc: 0.0000e+00\n",
      "Epoch 23/500\n",
      "0s - loss: 172.4586 - acc: 0.0000e+00 - val_loss: 206.8061 - val_acc: 0.0000e+00\n",
      "Epoch 24/500\n",
      "0s - loss: 172.4545 - acc: 0.0000e+00 - val_loss: 206.7968 - val_acc: 0.0000e+00\n",
      "Epoch 25/500\n",
      "0s - loss: 172.4503 - acc: 0.0000e+00 - val_loss: 206.7897 - val_acc: 0.0000e+00\n",
      "Epoch 26/500\n",
      "0s - loss: 172.4467 - acc: 0.0000e+00 - val_loss: 206.7821 - val_acc: 0.0000e+00\n",
      "Epoch 27/500\n",
      "0s - loss: 172.4428 - acc: 0.0000e+00 - val_loss: 206.7729 - val_acc: 0.0000e+00\n",
      "Epoch 28/500\n",
      "0s - loss: 172.4383 - acc: 0.0000e+00 - val_loss: 206.7645 - val_acc: 0.0000e+00\n",
      "Epoch 29/500\n",
      "0s - loss: 172.4340 - acc: 0.0000e+00 - val_loss: 206.7556 - val_acc: 0.0000e+00\n",
      "Epoch 30/500\n",
      "0s - loss: 172.4299 - acc: 0.0000e+00 - val_loss: 206.7466 - val_acc: 0.0000e+00\n",
      "Epoch 31/500\n",
      "0s - loss: 172.4258 - acc: 0.0000e+00 - val_loss: 206.7389 - val_acc: 0.0000e+00\n",
      "Epoch 32/500\n",
      "0s - loss: 172.4226 - acc: 0.0000e+00 - val_loss: 206.7325 - val_acc: 0.0000e+00\n",
      "Epoch 33/500\n",
      "0s - loss: 172.4197 - acc: 0.0000e+00 - val_loss: 206.7214 - val_acc: 0.0000e+00\n",
      "Epoch 34/500\n",
      "0s - loss: 172.4132 - acc: 0.0000e+00 - val_loss: 206.7122 - val_acc: 0.0000e+00\n",
      "Epoch 35/500\n",
      "0s - loss: 172.4100 - acc: 0.0000e+00 - val_loss: 206.7025 - val_acc: 0.0000e+00\n",
      "Epoch 36/500\n",
      "0s - loss: 172.4055 - acc: 0.0000e+00 - val_loss: 206.6934 - val_acc: 0.0000e+00\n",
      "Epoch 37/500\n",
      "0s - loss: 172.4014 - acc: 0.0000e+00 - val_loss: 206.6827 - val_acc: 0.0000e+00\n",
      "Epoch 38/500\n",
      "0s - loss: 172.3965 - acc: 0.0000e+00 - val_loss: 206.6726 - val_acc: 0.0000e+00\n",
      "Epoch 39/500\n",
      "0s - loss: 172.3909 - acc: 0.0000e+00 - val_loss: 206.6648 - val_acc: 0.0000e+00\n",
      "Epoch 40/500\n",
      "0s - loss: 172.3879 - acc: 0.0000e+00 - val_loss: 206.6579 - val_acc: 0.0000e+00\n",
      "Epoch 41/500\n",
      "0s - loss: 172.3837 - acc: 0.0000e+00 - val_loss: 206.6494 - val_acc: 0.0000e+00\n",
      "Epoch 42/500\n",
      "0s - loss: 172.3797 - acc: 0.0000e+00 - val_loss: 206.6412 - val_acc: 0.0000e+00\n",
      "Epoch 43/500\n",
      "0s - loss: 172.3756 - acc: 0.0000e+00 - val_loss: 206.6321 - val_acc: 0.0000e+00\n",
      "Epoch 44/500\n",
      "0s - loss: 172.3721 - acc: 0.0000e+00 - val_loss: 206.6229 - val_acc: 0.0000e+00\n",
      "Epoch 45/500\n",
      "0s - loss: 172.3671 - acc: 0.0000e+00 - val_loss: 206.6147 - val_acc: 0.0000e+00\n",
      "Epoch 46/500\n",
      "0s - loss: 172.3639 - acc: 0.0000e+00 - val_loss: 206.6078 - val_acc: 0.0000e+00\n",
      "Epoch 47/500\n",
      "0s - loss: 172.3599 - acc: 0.0000e+00 - val_loss: 206.5983 - val_acc: 0.0000e+00\n",
      "Epoch 48/500\n",
      "0s - loss: 172.3552 - acc: 0.0000e+00 - val_loss: 206.5892 - val_acc: 0.0000e+00\n",
      "Epoch 49/500\n",
      "0s - loss: 172.3514 - acc: 0.0000e+00 - val_loss: 206.5821 - val_acc: 0.0000e+00\n",
      "Epoch 50/500\n",
      "0s - loss: 172.3480 - acc: 0.0000e+00 - val_loss: 206.5738 - val_acc: 0.0000e+00\n",
      "Epoch 51/500\n",
      "0s - loss: 172.3438 - acc: 0.0000e+00 - val_loss: 206.5659 - val_acc: 0.0000e+00\n",
      "Epoch 52/500\n",
      "0s - loss: 172.3399 - acc: 0.0000e+00 - val_loss: 206.5559 - val_acc: 0.0000e+00\n",
      "Epoch 53/500\n",
      "0s - loss: 172.3355 - acc: 0.0000e+00 - val_loss: 206.5481 - val_acc: 0.0000e+00\n",
      "Epoch 54/500\n",
      "0s - loss: 172.3316 - acc: 0.0000e+00 - val_loss: 206.5393 - val_acc: 0.0000e+00\n",
      "Epoch 55/500\n",
      "0s - loss: 172.3278 - acc: 0.0000e+00 - val_loss: 206.5314 - val_acc: 0.0000e+00\n",
      "Epoch 56/500\n",
      "0s - loss: 172.3255 - acc: 0.0000e+00 - val_loss: 206.5253 - val_acc: 0.0000e+00\n",
      "Epoch 57/500\n",
      "0s - loss: 172.3217 - acc: 0.0000e+00 - val_loss: 206.5160 - val_acc: 0.0000e+00\n",
      "Epoch 58/500\n",
      "0s - loss: 172.3171 - acc: 0.0000e+00 - val_loss: 206.5087 - val_acc: 0.0000e+00\n",
      "Epoch 59/500\n",
      "0s - loss: 172.3141 - acc: 0.0000e+00 - val_loss: 206.5024 - val_acc: 0.0000e+00\n",
      "Epoch 60/500\n",
      "0s - loss: 172.3106 - acc: 0.0000e+00 - val_loss: 206.4933 - val_acc: 0.0000e+00\n",
      "Epoch 61/500\n",
      "0s - loss: 172.3060 - acc: 0.0000e+00 - val_loss: 206.4826 - val_acc: 0.0000e+00\n",
      "Epoch 62/500\n",
      "0s - loss: 172.3034 - acc: 0.0000e+00 - val_loss: 206.4771 - val_acc: 0.0000e+00\n",
      "Epoch 63/500\n",
      "0s - loss: 172.2986 - acc: 0.0000e+00 - val_loss: 206.4671 - val_acc: 0.0000e+00\n",
      "Epoch 64/500\n",
      "0s - loss: 172.2934 - acc: 0.0000e+00 - val_loss: 206.4588 - val_acc: 0.0000e+00\n",
      "Epoch 65/500\n",
      "0s - loss: 172.2891 - acc: 0.0000e+00 - val_loss: 206.4494 - val_acc: 0.0000e+00\n",
      "Epoch 66/500\n",
      "0s - loss: 172.2851 - acc: 0.0000e+00 - val_loss: 206.4398 - val_acc: 0.0000e+00\n",
      "Epoch 67/500\n",
      "0s - loss: 172.2807 - acc: 0.0000e+00 - val_loss: 206.4314 - val_acc: 0.0000e+00\n",
      "Epoch 68/500\n",
      "0s - loss: 172.2775 - acc: 0.0000e+00 - val_loss: 206.4248 - val_acc: 0.0000e+00\n",
      "Epoch 69/500\n",
      "0s - loss: 172.2735 - acc: 0.0000e+00 - val_loss: 206.4168 - val_acc: 0.0000e+00\n",
      "Epoch 70/500\n",
      "0s - loss: 172.2699 - acc: 0.0000e+00 - val_loss: 206.4087 - val_acc: 0.0000e+00\n",
      "Epoch 71/500\n",
      "0s - loss: 172.2663 - acc: 0.0000e+00 - val_loss: 206.3990 - val_acc: 0.0000e+00\n",
      "Epoch 72/500\n",
      "0s - loss: 172.2616 - acc: 0.0000e+00 - val_loss: 206.3907 - val_acc: 0.0000e+00\n",
      "Epoch 73/500\n",
      "0s - loss: 172.2585 - acc: 0.0000e+00 - val_loss: 206.3803 - val_acc: 0.0000e+00\n",
      "Epoch 74/500\n",
      "0s - loss: 172.2539 - acc: 0.0000e+00 - val_loss: 206.3742 - val_acc: 0.0000e+00\n",
      "Epoch 75/500\n",
      "0s - loss: 172.2497 - acc: 0.0000e+00 - val_loss: 206.3662 - val_acc: 0.0000e+00\n",
      "Epoch 76/500\n",
      "0s - loss: 172.2459 - acc: 0.0000e+00 - val_loss: 206.3577 - val_acc: 0.0000e+00\n",
      "Epoch 77/500\n",
      "0s - loss: 172.2434 - acc: 0.0000e+00 - val_loss: 206.3518 - val_acc: 0.0000e+00\n",
      "Epoch 78/500\n",
      "0s - loss: 172.2403 - acc: 0.0000e+00 - val_loss: 206.3459 - val_acc: 0.0000e+00\n",
      "Epoch 79/500\n",
      "0s - loss: 172.2373 - acc: 0.0000e+00 - val_loss: 206.3356 - val_acc: 0.0000e+00\n",
      "Epoch 80/500\n",
      "0s - loss: 172.2314 - acc: 0.0000e+00 - val_loss: 206.3266 - val_acc: 0.0000e+00\n",
      "Epoch 81/500\n",
      "0s - loss: 172.2271 - acc: 0.0000e+00 - val_loss: 206.3180 - val_acc: 0.0000e+00\n",
      "Epoch 82/500\n",
      "0s - loss: 172.2234 - acc: 0.0000e+00 - val_loss: 206.3103 - val_acc: 0.0000e+00\n",
      "Epoch 83/500\n",
      "0s - loss: 172.2200 - acc: 0.0000e+00 - val_loss: 206.3029 - val_acc: 0.0000e+00\n",
      "Epoch 84/500\n",
      "0s - loss: 172.2178 - acc: 0.0000e+00 - val_loss: 206.2985 - val_acc: 0.0000e+00\n",
      "Epoch 85/500\n",
      "0s - loss: 172.2137 - acc: 0.0000e+00 - val_loss: 206.2905 - val_acc: 0.0000e+00\n",
      "Epoch 86/500\n",
      "0s - loss: 172.2104 - acc: 0.0000e+00 - val_loss: 206.2821 - val_acc: 0.0000e+00\n",
      "Epoch 87/500\n",
      "0s - loss: 172.2062 - acc: 0.0000e+00 - val_loss: 206.2728 - val_acc: 0.0000e+00\n",
      "Epoch 88/500\n",
      "0s - loss: 172.2016 - acc: 0.0000e+00 - val_loss: 206.2654 - val_acc: 0.0000e+00\n",
      "Epoch 89/500\n",
      "0s - loss: 172.1990 - acc: 0.0000e+00 - val_loss: 206.2598 - val_acc: 0.0000e+00\n",
      "Epoch 90/500\n",
      "0s - loss: 172.1954 - acc: 0.0000e+00 - val_loss: 206.2530 - val_acc: 0.0000e+00\n",
      "Epoch 91/500\n",
      "0s - loss: 172.1923 - acc: 0.0000e+00 - val_loss: 206.2446 - val_acc: 0.0000e+00\n",
      "Epoch 92/500\n",
      "0s - loss: 172.1887 - acc: 0.0000e+00 - val_loss: 206.2377 - val_acc: 0.0000e+00\n",
      "Epoch 93/500\n",
      "0s - loss: 172.1848 - acc: 0.0000e+00 - val_loss: 206.2301 - val_acc: 0.0000e+00\n",
      "Epoch 94/500\n",
      "0s - loss: 172.1813 - acc: 0.0000e+00 - val_loss: 206.2229 - val_acc: 0.0000e+00\n",
      "Epoch 95/500\n",
      "0s - loss: 172.1778 - acc: 0.0000e+00 - val_loss: 206.2148 - val_acc: 0.0000e+00\n",
      "Epoch 96/500\n",
      "0s - loss: 172.1740 - acc: 0.0000e+00 - val_loss: 206.2067 - val_acc: 0.0000e+00\n",
      "Epoch 97/500\n",
      "0s - loss: 172.1705 - acc: 0.0000e+00 - val_loss: 206.1986 - val_acc: 0.0000e+00\n",
      "Epoch 98/500\n",
      "0s - loss: 172.1687 - acc: 0.0000e+00 - val_loss: 206.1935 - val_acc: 0.0000e+00\n",
      "Epoch 99/500\n",
      "0s - loss: 172.1639 - acc: 0.0000e+00 - val_loss: 206.1847 - val_acc: 0.0000e+00\n",
      "Epoch 100/500\n",
      "0s - loss: 172.1600 - acc: 0.0000e+00 - val_loss: 206.1770 - val_acc: 0.0000e+00\n",
      "Epoch 101/500\n",
      "0s - loss: 172.1578 - acc: 0.0000e+00 - val_loss: 206.1665 - val_acc: 0.0000e+00\n",
      "Epoch 102/500\n",
      "0s - loss: 172.1511 - acc: 0.0000e+00 - val_loss: 206.1571 - val_acc: 0.0000e+00\n",
      "Epoch 103/500\n",
      "0s - loss: 172.1480 - acc: 0.0000e+00 - val_loss: 206.1502 - val_acc: 0.0000e+00\n",
      "Epoch 104/500\n",
      "0s - loss: 172.1442 - acc: 0.0000e+00 - val_loss: 206.1414 - val_acc: 0.0000e+00\n",
      "Epoch 105/500\n",
      "0s - loss: 172.1414 - acc: 0.0000e+00 - val_loss: 206.1364 - val_acc: 0.0000e+00\n",
      "Epoch 106/500\n",
      "0s - loss: 172.1377 - acc: 0.0000e+00 - val_loss: 206.1289 - val_acc: 0.0000e+00\n",
      "Epoch 107/500\n",
      "0s - loss: 172.1340 - acc: 0.0000e+00 - val_loss: 206.1212 - val_acc: 0.0000e+00\n",
      "Epoch 108/500\n",
      "0s - loss: 172.1302 - acc: 0.0000e+00 - val_loss: 206.1131 - val_acc: 0.0000e+00\n",
      "Epoch 109/500\n",
      "0s - loss: 172.1269 - acc: 0.0000e+00 - val_loss: 206.1058 - val_acc: 0.0000e+00\n",
      "Epoch 110/500\n",
      "0s - loss: 172.1233 - acc: 0.0000e+00 - val_loss: 206.0985 - val_acc: 0.0000e+00\n",
      "Epoch 111/500\n",
      "0s - loss: 172.1197 - acc: 0.0000e+00 - val_loss: 206.0902 - val_acc: 0.0000e+00\n",
      "Epoch 112/500\n",
      "0s - loss: 172.1161 - acc: 0.0000e+00 - val_loss: 206.0829 - val_acc: 0.0000e+00\n",
      "Epoch 113/500\n",
      "0s - loss: 172.1132 - acc: 0.0000e+00 - val_loss: 206.0766 - val_acc: 0.0000e+00\n",
      "Epoch 114/500\n",
      "0s - loss: 172.1108 - acc: 0.0000e+00 - val_loss: 206.0672 - val_acc: 0.0000e+00\n",
      "Epoch 115/500\n",
      "0s - loss: 172.1051 - acc: 0.0000e+00 - val_loss: 206.0595 - val_acc: 0.0000e+00\n",
      "Epoch 116/500\n",
      "0s - loss: 172.1013 - acc: 0.0000e+00 - val_loss: 206.0526 - val_acc: 0.0000e+00\n",
      "Epoch 117/500\n",
      "0s - loss: 172.0982 - acc: 0.0000e+00 - val_loss: 206.0455 - val_acc: 0.0000e+00\n",
      "Epoch 118/500\n",
      "0s - loss: 172.0954 - acc: 0.0000e+00 - val_loss: 206.0392 - val_acc: 0.0000e+00\n",
      "Epoch 119/500\n",
      "0s - loss: 172.0933 - acc: 0.0000e+00 - val_loss: 206.0283 - val_acc: 0.0000e+00\n",
      "Epoch 120/500\n",
      "0s - loss: 172.0874 - acc: 0.0000e+00 - val_loss: 206.0193 - val_acc: 0.0000e+00\n",
      "Epoch 121/500\n",
      "0s - loss: 172.0836 - acc: 0.0000e+00 - val_loss: 206.0105 - val_acc: 0.0000e+00\n",
      "Epoch 122/500\n",
      "0s - loss: 172.0799 - acc: 0.0000e+00 - val_loss: 206.0044 - val_acc: 0.0000e+00\n",
      "Epoch 123/500\n",
      "0s - loss: 172.0764 - acc: 0.0000e+00 - val_loss: 205.9970 - val_acc: 0.0000e+00\n",
      "Epoch 124/500\n",
      "0s - loss: 172.0730 - acc: 0.0000e+00 - val_loss: 205.9903 - val_acc: 0.0000e+00\n",
      "Epoch 125/500\n",
      "0s - loss: 172.0696 - acc: 0.0000e+00 - val_loss: 205.9817 - val_acc: 0.0000e+00\n",
      "Epoch 126/500\n",
      "0s - loss: 172.0662 - acc: 0.0000e+00 - val_loss: 205.9724 - val_acc: 0.0000e+00\n",
      "Epoch 127/500\n",
      "0s - loss: 172.0619 - acc: 0.0000e+00 - val_loss: 205.9673 - val_acc: 0.0000e+00\n",
      "Epoch 128/500\n",
      "0s - loss: 172.0590 - acc: 0.0000e+00 - val_loss: 205.9606 - val_acc: 0.0000e+00\n",
      "Epoch 129/500\n",
      "0s - loss: 172.0558 - acc: 0.0000e+00 - val_loss: 205.9525 - val_acc: 0.0000e+00\n",
      "Epoch 130/500\n",
      "0s - loss: 172.0522 - acc: 0.0000e+00 - val_loss: 205.9442 - val_acc: 0.0000e+00\n",
      "Epoch 131/500\n",
      "0s - loss: 172.0479 - acc: 0.0000e+00 - val_loss: 205.9372 - val_acc: 0.0000e+00\n",
      "Epoch 132/500\n",
      "0s - loss: 172.0447 - acc: 0.0000e+00 - val_loss: 205.9288 - val_acc: 0.0000e+00\n",
      "Epoch 133/500\n",
      "0s - loss: 172.0420 - acc: 0.0000e+00 - val_loss: 205.9232 - val_acc: 0.0000e+00\n",
      "Epoch 134/500\n",
      "0s - loss: 172.0383 - acc: 0.0000e+00 - val_loss: 205.9167 - val_acc: 0.0000e+00\n",
      "Epoch 135/500\n",
      "0s - loss: 172.0347 - acc: 0.0000e+00 - val_loss: 205.9088 - val_acc: 0.0000e+00\n",
      "Epoch 136/500\n",
      "0s - loss: 172.0314 - acc: 0.0000e+00 - val_loss: 205.9010 - val_acc: 0.0000e+00\n",
      "Epoch 137/500\n",
      "0s - loss: 172.0286 - acc: 0.0000e+00 - val_loss: 205.8914 - val_acc: 0.0000e+00\n",
      "Epoch 138/500\n",
      "0s - loss: 172.0234 - acc: 0.0000e+00 - val_loss: 205.8841 - val_acc: 0.0000e+00\n",
      "Epoch 139/500\n",
      "0s - loss: 172.0204 - acc: 0.0000e+00 - val_loss: 205.8749 - val_acc: 0.0000e+00\n",
      "Epoch 140/500\n",
      "0s - loss: 172.0158 - acc: 0.0000e+00 - val_loss: 205.8671 - val_acc: 0.0000e+00\n",
      "Epoch 141/500\n",
      "0s - loss: 172.0123 - acc: 0.0000e+00 - val_loss: 205.8601 - val_acc: 0.0000e+00\n",
      "Epoch 142/500\n",
      "0s - loss: 172.0090 - acc: 0.0000e+00 - val_loss: 205.8534 - val_acc: 0.0000e+00\n",
      "Epoch 143/500\n",
      "0s - loss: 172.0056 - acc: 0.0000e+00 - val_loss: 205.8463 - val_acc: 0.0000e+00\n",
      "Epoch 144/500\n",
      "0s - loss: 172.0026 - acc: 0.0000e+00 - val_loss: 205.8378 - val_acc: 0.0000e+00\n",
      "Epoch 145/500\n",
      "0s - loss: 171.9995 - acc: 0.0000e+00 - val_loss: 205.8318 - val_acc: 0.0000e+00\n",
      "Epoch 146/500\n",
      "0s - loss: 171.9958 - acc: 0.0000e+00 - val_loss: 205.8227 - val_acc: 0.0000e+00\n",
      "Epoch 147/500\n",
      "0s - loss: 171.9919 - acc: 0.0000e+00 - val_loss: 205.8148 - val_acc: 0.0000e+00\n",
      "Epoch 148/500\n",
      "0s - loss: 171.9884 - acc: 0.0000e+00 - val_loss: 205.8070 - val_acc: 0.0000e+00\n",
      "Epoch 149/500\n",
      "0s - loss: 171.9854 - acc: 0.0000e+00 - val_loss: 205.7980 - val_acc: 0.0000e+00\n",
      "Epoch 150/500\n",
      "0s - loss: 171.9811 - acc: 0.0000e+00 - val_loss: 205.7921 - val_acc: 0.0000e+00\n",
      "Epoch 151/500\n",
      "0s - loss: 171.9786 - acc: 0.0000e+00 - val_loss: 205.7864 - val_acc: 0.0000e+00\n",
      "Epoch 152/500\n",
      "0s - loss: 171.9753 - acc: 0.0000e+00 - val_loss: 205.7775 - val_acc: 0.0000e+00\n",
      "Epoch 153/500\n",
      "0s - loss: 171.9713 - acc: 0.0000e+00 - val_loss: 205.7699 - val_acc: 0.0000e+00\n",
      "Epoch 154/500\n",
      "0s - loss: 171.9683 - acc: 0.0000e+00 - val_loss: 205.7629 - val_acc: 0.0000e+00\n",
      "Epoch 155/500\n",
      "0s - loss: 171.9639 - acc: 0.0000e+00 - val_loss: 205.7531 - val_acc: 0.0000e+00\n",
      "Epoch 156/500\n",
      "0s - loss: 171.9609 - acc: 0.0000e+00 - val_loss: 205.7463 - val_acc: 0.0000e+00\n",
      "Epoch 157/500\n",
      "0s - loss: 171.9569 - acc: 0.0000e+00 - val_loss: 205.7397 - val_acc: 0.0000e+00\n",
      "Epoch 158/500\n",
      "0s - loss: 171.9538 - acc: 0.0000e+00 - val_loss: 205.7312 - val_acc: 0.0000e+00\n",
      "Epoch 159/500\n",
      "0s - loss: 171.9497 - acc: 0.0000e+00 - val_loss: 205.7242 - val_acc: 0.0000e+00\n",
      "Epoch 160/500\n",
      "0s - loss: 171.9464 - acc: 0.0000e+00 - val_loss: 205.7157 - val_acc: 0.0000e+00\n",
      "Epoch 161/500\n",
      "0s - loss: 171.9430 - acc: 0.0000e+00 - val_loss: 205.7073 - val_acc: 0.0000e+00\n",
      "Epoch 162/500\n",
      "0s - loss: 171.9389 - acc: 0.0000e+00 - val_loss: 205.6988 - val_acc: 0.0000e+00\n",
      "Epoch 163/500\n",
      "0s - loss: 171.9358 - acc: 0.0000e+00 - val_loss: 205.6926 - val_acc: 0.0000e+00\n",
      "Epoch 164/500\n",
      "0s - loss: 171.9336 - acc: 0.0000e+00 - val_loss: 205.6875 - val_acc: 0.0000e+00\n",
      "Epoch 165/500\n",
      "0s - loss: 171.9299 - acc: 0.0000e+00 - val_loss: 205.6804 - val_acc: 0.0000e+00\n",
      "Epoch 166/500\n",
      "0s - loss: 171.9325 - acc: 0.0000e+00 - val_loss: 205.6770 - val_acc: 0.0000e+00\n",
      "Epoch 167/500\n",
      "0s - loss: 171.9260 - acc: 0.0000e+00 - val_loss: 205.6687 - val_acc: 0.0000e+00\n",
      "Epoch 168/500\n",
      "0s - loss: 171.9208 - acc: 0.0000e+00 - val_loss: 205.6609 - val_acc: 0.0000e+00\n",
      "Epoch 169/500\n",
      "0s - loss: 171.9173 - acc: 0.0000e+00 - val_loss: 205.6528 - val_acc: 0.0000e+00\n",
      "Epoch 170/500\n",
      "0s - loss: 171.9140 - acc: 0.0000e+00 - val_loss: 205.6446 - val_acc: 0.0000e+00\n",
      "Epoch 171/500\n",
      "0s - loss: 171.9119 - acc: 0.0000e+00 - val_loss: 205.6395 - val_acc: 0.0000e+00\n",
      "Epoch 172/500\n",
      "0s - loss: 171.9074 - acc: 0.0000e+00 - val_loss: 205.6324 - val_acc: 0.0000e+00\n",
      "Epoch 173/500\n",
      "0s - loss: 171.9047 - acc: 0.0000e+00 - val_loss: 205.6255 - val_acc: 0.0000e+00\n",
      "Epoch 174/500\n",
      "0s - loss: 171.9012 - acc: 0.0000e+00 - val_loss: 205.6190 - val_acc: 0.0000e+00\n",
      "Epoch 175/500\n",
      "0s - loss: 171.8976 - acc: 0.0000e+00 - val_loss: 205.6112 - val_acc: 0.0000e+00\n",
      "Epoch 176/500\n",
      "0s - loss: 171.8946 - acc: 0.0000e+00 - val_loss: 205.6034 - val_acc: 0.0000e+00\n",
      "Epoch 177/500\n",
      "0s - loss: 171.8951 - acc: 0.0000e+00 - val_loss: 205.5928 - val_acc: 0.0000e+00\n",
      "Epoch 178/500\n",
      "0s - loss: 171.8861 - acc: 0.0000e+00 - val_loss: 205.5846 - val_acc: 0.0000e+00\n",
      "Epoch 179/500\n",
      "0s - loss: 171.8829 - acc: 0.0000e+00 - val_loss: 205.5761 - val_acc: 0.0000e+00\n",
      "Epoch 180/500\n",
      "0s - loss: 171.8787 - acc: 0.0000e+00 - val_loss: 205.5677 - val_acc: 0.0000e+00\n",
      "Epoch 181/500\n",
      "0s - loss: 171.8769 - acc: 0.0000e+00 - val_loss: 205.5636 - val_acc: 0.0000e+00\n",
      "Epoch 182/500\n",
      "0s - loss: 171.8731 - acc: 0.0000e+00 - val_loss: 205.5556 - val_acc: 0.0000e+00\n",
      "Epoch 183/500\n",
      "0s - loss: 171.8695 - acc: 0.0000e+00 - val_loss: 205.5496 - val_acc: 0.0000e+00\n",
      "Epoch 184/500\n",
      "0s - loss: 171.8662 - acc: 0.0000e+00 - val_loss: 205.5429 - val_acc: 0.0000e+00\n",
      "Epoch 185/500\n",
      "0s - loss: 171.8637 - acc: 0.0000e+00 - val_loss: 205.5345 - val_acc: 0.0000e+00\n",
      "Epoch 186/500\n",
      "0s - loss: 171.8593 - acc: 0.0000e+00 - val_loss: 205.5269 - val_acc: 0.0000e+00\n",
      "Epoch 187/500\n",
      "0s - loss: 171.8567 - acc: 0.0000e+00 - val_loss: 205.5192 - val_acc: 0.0000e+00\n",
      "Epoch 188/500\n",
      "0s - loss: 171.8538 - acc: 0.0000e+00 - val_loss: 205.5131 - val_acc: 0.0000e+00\n",
      "Epoch 189/500\n",
      "0s - loss: 171.8505 - acc: 0.0000e+00 - val_loss: 205.5042 - val_acc: 0.0000e+00\n",
      "Epoch 190/500\n",
      "0s - loss: 171.8458 - acc: 0.0000e+00 - val_loss: 205.4982 - val_acc: 0.0000e+00\n",
      "Epoch 191/500\n",
      "0s - loss: 171.8430 - acc: 0.0000e+00 - val_loss: 205.4907 - val_acc: 0.0000e+00\n",
      "Epoch 192/500\n",
      "0s - loss: 171.8398 - acc: 0.0000e+00 - val_loss: 205.4820 - val_acc: 0.0000e+00\n",
      "Epoch 193/500\n",
      "0s - loss: 171.8359 - acc: 0.0000e+00 - val_loss: 205.4756 - val_acc: 0.0000e+00\n",
      "Epoch 194/500\n",
      "0s - loss: 171.8326 - acc: 0.0000e+00 - val_loss: 205.4683 - val_acc: 0.0000e+00\n",
      "Epoch 195/500\n",
      "0s - loss: 171.8305 - acc: 0.0000e+00 - val_loss: 205.4596 - val_acc: 0.0000e+00\n",
      "Epoch 196/500\n",
      "0s - loss: 171.8264 - acc: 0.0000e+00 - val_loss: 205.4537 - val_acc: 0.0000e+00\n",
      "Epoch 197/500\n",
      "0s - loss: 171.8229 - acc: 0.0000e+00 - val_loss: 205.4472 - val_acc: 0.0000e+00\n",
      "Epoch 198/500\n",
      "0s - loss: 171.8211 - acc: 0.0000e+00 - val_loss: 205.4417 - val_acc: 0.0000e+00\n",
      "Epoch 199/500\n",
      "0s - loss: 171.8173 - acc: 0.0000e+00 - val_loss: 205.4334 - val_acc: 0.0000e+00\n",
      "Epoch 200/500\n",
      "0s - loss: 171.8134 - acc: 0.0000e+00 - val_loss: 205.4257 - val_acc: 0.0000e+00\n",
      "Epoch 201/500\n",
      "0s - loss: 171.8102 - acc: 0.0000e+00 - val_loss: 205.4192 - val_acc: 0.0000e+00\n",
      "Epoch 202/500\n",
      "0s - loss: 171.8068 - acc: 0.0000e+00 - val_loss: 205.4114 - val_acc: 0.0000e+00\n",
      "Epoch 203/500\n",
      "0s - loss: 171.8033 - acc: 0.0000e+00 - val_loss: 205.4044 - val_acc: 0.0000e+00\n",
      "Epoch 204/500\n",
      "0s - loss: 171.8003 - acc: 0.0000e+00 - val_loss: 205.3975 - val_acc: 0.0000e+00\n",
      "Epoch 205/500\n",
      "0s - loss: 171.7972 - acc: 0.0000e+00 - val_loss: 205.3893 - val_acc: 0.0000e+00\n",
      "Epoch 206/500\n",
      "0s - loss: 171.7940 - acc: 0.0000e+00 - val_loss: 205.3835 - val_acc: 0.0000e+00\n",
      "Epoch 207/500\n",
      "0s - loss: 171.7906 - acc: 0.0000e+00 - val_loss: 205.3759 - val_acc: 0.0000e+00\n",
      "Epoch 208/500\n",
      "0s - loss: 171.7884 - acc: 0.0000e+00 - val_loss: 205.3674 - val_acc: 0.0000e+00\n",
      "Epoch 209/500\n",
      "0s - loss: 171.7844 - acc: 0.0000e+00 - val_loss: 205.3620 - val_acc: 0.0000e+00\n",
      "Epoch 210/500\n",
      "0s - loss: 171.7813 - acc: 0.0000e+00 - val_loss: 205.3555 - val_acc: 0.0000e+00\n",
      "Epoch 211/500\n",
      "0s - loss: 171.7779 - acc: 0.0000e+00 - val_loss: 205.3478 - val_acc: 0.0000e+00\n",
      "Epoch 212/500\n",
      "0s - loss: 171.7746 - acc: 0.0000e+00 - val_loss: 205.3402 - val_acc: 0.0000e+00\n",
      "Epoch 213/500\n",
      "0s - loss: 171.7736 - acc: 0.0000e+00 - val_loss: 205.3361 - val_acc: 0.0000e+00\n",
      "Epoch 214/500\n",
      "0s - loss: 171.7695 - acc: 0.0000e+00 - val_loss: 205.3285 - val_acc: 0.0000e+00\n",
      "Epoch 215/500\n",
      "0s - loss: 171.7657 - acc: 0.0000e+00 - val_loss: 205.3217 - val_acc: 0.0000e+00\n",
      "Epoch 216/500\n",
      "0s - loss: 171.7633 - acc: 0.0000e+00 - val_loss: 205.3131 - val_acc: 0.0000e+00\n",
      "Epoch 217/500\n",
      "0s - loss: 171.7590 - acc: 0.0000e+00 - val_loss: 205.3059 - val_acc: 0.0000e+00\n",
      "Epoch 218/500\n",
      "0s - loss: 171.7558 - acc: 0.0000e+00 - val_loss: 205.2968 - val_acc: 0.0000e+00\n",
      "Epoch 219/500\n",
      "0s - loss: 171.7521 - acc: 0.0000e+00 - val_loss: 205.2883 - val_acc: 0.0000e+00\n",
      "Epoch 220/500\n",
      "0s - loss: 171.7481 - acc: 0.0000e+00 - val_loss: 205.2802 - val_acc: 0.0000e+00\n",
      "Epoch 221/500\n",
      "0s - loss: 171.7481 - acc: 0.0000e+00 - val_loss: 205.2764 - val_acc: 0.0000e+00\n",
      "Epoch 222/500\n",
      "0s - loss: 171.7426 - acc: 0.0000e+00 - val_loss: 205.2693 - val_acc: 0.0000e+00\n",
      "Epoch 223/500\n",
      "0s - loss: 171.7398 - acc: 0.0000e+00 - val_loss: 205.2613 - val_acc: 0.0000e+00\n",
      "Epoch 224/500\n",
      "0s - loss: 171.7366 - acc: 0.0000e+00 - val_loss: 205.2530 - val_acc: 0.0000e+00\n",
      "Epoch 225/500\n",
      "0s - loss: 171.7323 - acc: 0.0000e+00 - val_loss: 205.2450 - val_acc: 0.0000e+00\n",
      "Epoch 226/500\n",
      "0s - loss: 171.7288 - acc: 0.0000e+00 - val_loss: 205.2378 - val_acc: 0.0000e+00\n",
      "Epoch 227/500\n",
      "0s - loss: 171.7255 - acc: 0.0000e+00 - val_loss: 205.2321 - val_acc: 0.0000e+00\n",
      "Epoch 228/500\n",
      "0s - loss: 171.7228 - acc: 0.0000e+00 - val_loss: 205.2258 - val_acc: 0.0000e+00\n",
      "Epoch 229/500\n",
      "0s - loss: 171.7197 - acc: 0.0000e+00 - val_loss: 205.2193 - val_acc: 0.0000e+00\n",
      "Epoch 230/500\n",
      "0s - loss: 171.7165 - acc: 0.0000e+00 - val_loss: 205.2128 - val_acc: 0.0000e+00\n",
      "Epoch 231/500\n",
      "0s - loss: 171.7153 - acc: 0.0000e+00 - val_loss: 205.2076 - val_acc: 0.0000e+00\n",
      "Epoch 232/500\n",
      "0s - loss: 171.7116 - acc: 0.0000e+00 - val_loss: 205.2005 - val_acc: 0.0000e+00\n",
      "Epoch 233/500\n",
      "0s - loss: 171.7087 - acc: 0.0000e+00 - val_loss: 205.1952 - val_acc: 0.0000e+00\n",
      "Epoch 234/500\n",
      "0s - loss: 171.7055 - acc: 0.0000e+00 - val_loss: 205.1889 - val_acc: 0.0000e+00\n",
      "Epoch 235/500\n",
      "0s - loss: 171.7028 - acc: 0.0000e+00 - val_loss: 205.1826 - val_acc: 0.0000e+00\n",
      "Epoch 236/500\n",
      "0s - loss: 171.7001 - acc: 0.0000e+00 - val_loss: 205.1750 - val_acc: 0.0000e+00\n",
      "Epoch 237/500\n",
      "0s - loss: 171.6967 - acc: 0.0000e+00 - val_loss: 205.1686 - val_acc: 0.0000e+00\n",
      "Epoch 238/500\n",
      "0s - loss: 171.6939 - acc: 0.0000e+00 - val_loss: 205.1635 - val_acc: 0.0000e+00\n",
      "Epoch 239/500\n",
      "0s - loss: 171.6930 - acc: 0.0000e+00 - val_loss: 205.1544 - val_acc: 0.0000e+00\n",
      "Epoch 240/500\n",
      "0s - loss: 171.6872 - acc: 0.0000e+00 - val_loss: 205.1467 - val_acc: 0.0000e+00\n",
      "Epoch 241/500\n",
      "0s - loss: 171.6851 - acc: 0.0000e+00 - val_loss: 205.1418 - val_acc: 0.0000e+00\n",
      "Epoch 242/500\n",
      "0s - loss: 171.6816 - acc: 0.0000e+00 - val_loss: 205.1346 - val_acc: 0.0000e+00\n",
      "Epoch 243/500\n",
      "0s - loss: 171.6784 - acc: 0.0000e+00 - val_loss: 205.1280 - val_acc: 0.0000e+00\n",
      "Epoch 244/500\n",
      "0s - loss: 171.6759 - acc: 0.0000e+00 - val_loss: 205.1196 - val_acc: 0.0000e+00\n",
      "Epoch 245/500\n",
      "0s - loss: 171.6716 - acc: 0.0000e+00 - val_loss: 205.1128 - val_acc: 0.0000e+00\n",
      "Epoch 246/500\n",
      "0s - loss: 171.6694 - acc: 0.0000e+00 - val_loss: 205.1040 - val_acc: 0.0000e+00\n",
      "Epoch 247/500\n",
      "0s - loss: 171.6660 - acc: 0.0000e+00 - val_loss: 205.0990 - val_acc: 0.0000e+00\n",
      "Epoch 248/500\n",
      "0s - loss: 171.6626 - acc: 0.0000e+00 - val_loss: 205.0915 - val_acc: 0.0000e+00\n",
      "Epoch 249/500\n",
      "0s - loss: 171.6596 - acc: 0.0000e+00 - val_loss: 205.0841 - val_acc: 0.0000e+00\n",
      "Epoch 250/500\n",
      "0s - loss: 171.6578 - acc: 0.0000e+00 - val_loss: 205.0757 - val_acc: 0.0000e+00\n",
      "Epoch 251/500\n",
      "0s - loss: 171.6527 - acc: 0.0000e+00 - val_loss: 205.0675 - val_acc: 0.0000e+00\n",
      "Epoch 252/500\n",
      "0s - loss: 171.6491 - acc: 0.0000e+00 - val_loss: 205.0605 - val_acc: 0.0000e+00\n",
      "Epoch 253/500\n",
      "0s - loss: 171.6456 - acc: 0.0000e+00 - val_loss: 205.0541 - val_acc: 0.0000e+00\n",
      "Epoch 254/500\n",
      "0s - loss: 171.6432 - acc: 0.0000e+00 - val_loss: 205.0483 - val_acc: 0.0000e+00\n",
      "Epoch 255/500\n",
      "0s - loss: 171.6398 - acc: 0.0000e+00 - val_loss: 205.0417 - val_acc: 0.0000e+00\n",
      "Epoch 256/500\n",
      "0s - loss: 171.6393 - acc: 0.0000e+00 - val_loss: 205.0380 - val_acc: 0.0000e+00\n",
      "Epoch 257/500\n",
      "0s - loss: 171.6352 - acc: 0.0000e+00 - val_loss: 205.0312 - val_acc: 0.0000e+00\n",
      "Epoch 258/500\n",
      "0s - loss: 171.6333 - acc: 0.0000e+00 - val_loss: 205.0266 - val_acc: 0.0000e+00\n",
      "Epoch 259/500\n",
      "0s - loss: 171.6301 - acc: 0.0000e+00 - val_loss: 205.0197 - val_acc: 0.0000e+00\n",
      "Epoch 260/500\n",
      "0s - loss: 171.6274 - acc: 0.0000e+00 - val_loss: 205.0120 - val_acc: 0.0000e+00\n",
      "Epoch 261/500\n",
      "0s - loss: 171.6242 - acc: 0.0000e+00 - val_loss: 205.0053 - val_acc: 0.0000e+00\n",
      "Epoch 262/500\n",
      "0s - loss: 171.6213 - acc: 0.0000e+00 - val_loss: 204.9959 - val_acc: 0.0000e+00\n",
      "Epoch 263/500\n",
      "0s - loss: 171.6167 - acc: 0.0000e+00 - val_loss: 204.9889 - val_acc: 0.0000e+00\n",
      "Epoch 264/500\n",
      "0s - loss: 171.6141 - acc: 0.0000e+00 - val_loss: 204.9831 - val_acc: 0.0000e+00\n",
      "Epoch 265/500\n",
      "0s - loss: 171.6119 - acc: 0.0000e+00 - val_loss: 204.9787 - val_acc: 0.0000e+00\n",
      "Epoch 266/500\n",
      "0s - loss: 171.6090 - acc: 0.0000e+00 - val_loss: 204.9711 - val_acc: 0.0000e+00\n",
      "Epoch 267/500\n",
      "0s - loss: 171.6059 - acc: 0.0000e+00 - val_loss: 204.9660 - val_acc: 0.0000e+00\n",
      "Epoch 268/500\n",
      "0s - loss: 171.6029 - acc: 0.0000e+00 - val_loss: 204.9594 - val_acc: 0.0000e+00\n",
      "Epoch 269/500\n",
      "0s - loss: 171.5999 - acc: 0.0000e+00 - val_loss: 204.9529 - val_acc: 0.0000e+00\n",
      "Epoch 270/500\n",
      "0s - loss: 171.5984 - acc: 0.0000e+00 - val_loss: 204.9482 - val_acc: 0.0000e+00\n",
      "Epoch 271/500\n",
      "0s - loss: 171.5949 - acc: 0.0000e+00 - val_loss: 204.9427 - val_acc: 0.0000e+00\n",
      "Epoch 272/500\n",
      "0s - loss: 171.5961 - acc: 0.0000e+00 - val_loss: 204.9388 - val_acc: 0.0000e+00\n",
      "Epoch 273/500\n",
      "0s - loss: 171.5909 - acc: 0.0000e+00 - val_loss: 204.9333 - val_acc: 0.0000e+00\n",
      "Epoch 274/500\n",
      "0s - loss: 171.5877 - acc: 0.0000e+00 - val_loss: 204.9268 - val_acc: 0.0000e+00\n",
      "Epoch 275/500\n",
      "0s - loss: 171.5855 - acc: 0.0000e+00 - val_loss: 204.9215 - val_acc: 0.0000e+00\n",
      "Epoch 276/500\n",
      "0s - loss: 171.5828 - acc: 0.0000e+00 - val_loss: 204.9162 - val_acc: 0.0000e+00\n",
      "Epoch 277/500\n",
      "0s - loss: 171.5833 - acc: 0.0000e+00 - val_loss: 204.9128 - val_acc: 0.0000e+00\n",
      "Epoch 278/500\n",
      "0s - loss: 171.5779 - acc: 0.0000e+00 - val_loss: 204.9057 - val_acc: 0.0000e+00\n",
      "Epoch 279/500\n",
      "0s - loss: 171.5750 - acc: 0.0000e+00 - val_loss: 204.8995 - val_acc: 0.0000e+00\n",
      "Epoch 280/500\n",
      "0s - loss: 171.5725 - acc: 0.0000e+00 - val_loss: 204.8934 - val_acc: 0.0000e+00\n",
      "Epoch 281/500\n",
      "0s - loss: 171.5695 - acc: 0.0000e+00 - val_loss: 204.8870 - val_acc: 0.0000e+00\n",
      "Epoch 282/500\n",
      "0s - loss: 171.5666 - acc: 0.0000e+00 - val_loss: 204.8801 - val_acc: 0.0000e+00\n",
      "Epoch 283/500\n",
      "0s - loss: 171.5636 - acc: 0.0000e+00 - val_loss: 204.8735 - val_acc: 0.0000e+00\n",
      "Epoch 284/500\n",
      "0s - loss: 171.5614 - acc: 0.0000e+00 - val_loss: 204.8669 - val_acc: 0.0000e+00\n",
      "Epoch 285/500\n",
      "0s - loss: 171.5574 - acc: 0.0000e+00 - val_loss: 204.8589 - val_acc: 0.0000e+00\n",
      "Epoch 286/500\n",
      "0s - loss: 171.5547 - acc: 0.0000e+00 - val_loss: 204.8532 - val_acc: 0.0000e+00\n",
      "Epoch 287/500\n",
      "0s - loss: 171.5521 - acc: 0.0000e+00 - val_loss: 204.8474 - val_acc: 0.0000e+00\n",
      "Epoch 288/500\n",
      "0s - loss: 171.5492 - acc: 0.0000e+00 - val_loss: 204.8414 - val_acc: 0.0000e+00\n",
      "Epoch 289/500\n",
      "0s - loss: 171.5462 - acc: 0.0000e+00 - val_loss: 204.8342 - val_acc: 0.0000e+00\n",
      "Epoch 290/500\n",
      "0s - loss: 171.5443 - acc: 0.0000e+00 - val_loss: 204.8295 - val_acc: 0.0000e+00\n",
      "Epoch 291/500\n",
      "0s - loss: 171.5411 - acc: 0.0000e+00 - val_loss: 204.8235 - val_acc: 0.0000e+00\n",
      "Epoch 292/500\n",
      "0s - loss: 171.5386 - acc: 0.0000e+00 - val_loss: 204.8172 - val_acc: 0.0000e+00\n",
      "Epoch 293/500\n",
      "0s - loss: 171.5362 - acc: 0.0000e+00 - val_loss: 204.8092 - val_acc: 0.0000e+00\n",
      "Epoch 294/500\n",
      "0s - loss: 171.5321 - acc: 0.0000e+00 - val_loss: 204.8041 - val_acc: 0.0000e+00\n",
      "Epoch 295/500\n",
      "0s - loss: 171.5297 - acc: 0.0000e+00 - val_loss: 204.7980 - val_acc: 0.0000e+00\n",
      "Epoch 296/500\n",
      "0s - loss: 171.5273 - acc: 0.0000e+00 - val_loss: 204.7917 - val_acc: 0.0000e+00\n",
      "Epoch 297/500\n",
      "0s - loss: 171.5244 - acc: 0.0000e+00 - val_loss: 204.7858 - val_acc: 0.0000e+00\n",
      "Epoch 298/500\n",
      "0s - loss: 171.5225 - acc: 0.0000e+00 - val_loss: 204.7786 - val_acc: 0.0000e+00\n",
      "Epoch 299/500\n",
      "0s - loss: 171.5189 - acc: 0.0000e+00 - val_loss: 204.7719 - val_acc: 0.0000e+00\n",
      "Epoch 300/500\n",
      "0s - loss: 171.5162 - acc: 0.0000e+00 - val_loss: 204.7652 - val_acc: 0.0000e+00\n",
      "Epoch 301/500\n",
      "0s - loss: 171.5151 - acc: 0.0000e+00 - val_loss: 204.7560 - val_acc: 0.0000e+00\n",
      "Epoch 302/500\n",
      "0s - loss: 171.5103 - acc: 0.0000e+00 - val_loss: 204.7516 - val_acc: 0.0000e+00\n",
      "Epoch 303/500\n",
      "0s - loss: 171.5072 - acc: 0.0000e+00 - val_loss: 204.7445 - val_acc: 0.0000e+00\n",
      "Epoch 304/500\n",
      "0s - loss: 171.5036 - acc: 0.0000e+00 - val_loss: 204.7390 - val_acc: 0.0000e+00\n",
      "Epoch 305/500\n",
      "0s - loss: 171.5011 - acc: 0.0000e+00 - val_loss: 204.7332 - val_acc: 0.0000e+00\n",
      "Epoch 306/500\n",
      "0s - loss: 171.5000 - acc: 0.0000e+00 - val_loss: 204.7283 - val_acc: 0.0000e+00\n",
      "Epoch 307/500\n",
      "0s - loss: 171.4980 - acc: 0.0000e+00 - val_loss: 204.7251 - val_acc: 0.0000e+00\n",
      "Epoch 308/500\n",
      "0s - loss: 171.4944 - acc: 0.0000e+00 - val_loss: 204.7183 - val_acc: 0.0000e+00\n",
      "Epoch 309/500\n",
      "0s - loss: 171.4913 - acc: 0.0000e+00 - val_loss: 204.7127 - val_acc: 0.0000e+00\n",
      "Epoch 310/500\n",
      "0s - loss: 171.4892 - acc: 0.0000e+00 - val_loss: 204.7047 - val_acc: 0.0000e+00\n",
      "Epoch 311/500\n",
      "0s - loss: 171.4859 - acc: 0.0000e+00 - val_loss: 204.6960 - val_acc: 0.0000e+00\n",
      "Epoch 312/500\n",
      "0s - loss: 171.4821 - acc: 0.0000e+00 - val_loss: 204.6902 - val_acc: 0.0000e+00\n",
      "Epoch 313/500\n",
      "0s - loss: 171.4801 - acc: 0.0000e+00 - val_loss: 204.6831 - val_acc: 0.0000e+00\n",
      "Epoch 314/500\n",
      "0s - loss: 171.4772 - acc: 0.0000e+00 - val_loss: 204.6778 - val_acc: 0.0000e+00\n",
      "Epoch 315/500\n",
      "0s - loss: 171.4739 - acc: 0.0000e+00 - val_loss: 204.6721 - val_acc: 0.0000e+00\n",
      "Epoch 316/500\n",
      "0s - loss: 171.4714 - acc: 0.0000e+00 - val_loss: 204.6662 - val_acc: 0.0000e+00\n",
      "Epoch 317/500\n",
      "0s - loss: 171.4702 - acc: 0.0000e+00 - val_loss: 204.6613 - val_acc: 0.0000e+00\n",
      "Epoch 318/500\n",
      "0s - loss: 171.4664 - acc: 0.0000e+00 - val_loss: 204.6563 - val_acc: 0.0000e+00\n",
      "Epoch 319/500\n",
      "0s - loss: 171.4654 - acc: 0.0000e+00 - val_loss: 204.6521 - val_acc: 0.0000e+00\n",
      "Epoch 320/500\n",
      "0s - loss: 171.4618 - acc: 0.0000e+00 - val_loss: 204.6461 - val_acc: 0.0000e+00\n",
      "Epoch 321/500\n",
      "0s - loss: 171.4598 - acc: 0.0000e+00 - val_loss: 204.6375 - val_acc: 0.0000e+00\n",
      "Epoch 322/500\n",
      "0s - loss: 171.4564 - acc: 0.0000e+00 - val_loss: 204.6328 - val_acc: 0.0000e+00\n",
      "Epoch 323/500\n",
      "0s - loss: 171.4551 - acc: 0.0000e+00 - val_loss: 204.6287 - val_acc: 0.0000e+00\n",
      "Epoch 324/500\n",
      "0s - loss: 171.4527 - acc: 0.0000e+00 - val_loss: 204.6203 - val_acc: 0.0000e+00\n",
      "Epoch 325/500\n",
      "0s - loss: 171.4480 - acc: 0.0000e+00 - val_loss: 204.6142 - val_acc: 0.0000e+00\n",
      "Epoch 326/500\n",
      "0s - loss: 171.4454 - acc: 0.0000e+00 - val_loss: 204.6095 - val_acc: 0.0000e+00\n",
      "Epoch 327/500\n",
      "0s - loss: 171.4431 - acc: 0.0000e+00 - val_loss: 204.6037 - val_acc: 0.0000e+00\n",
      "Epoch 328/500\n",
      "0s - loss: 171.4405 - acc: 0.0000e+00 - val_loss: 204.5979 - val_acc: 0.0000e+00\n",
      "Epoch 329/500\n",
      "0s - loss: 171.4380 - acc: 0.0000e+00 - val_loss: 204.5926 - val_acc: 0.0000e+00\n",
      "Epoch 330/500\n",
      "0s - loss: 171.4354 - acc: 0.0000e+00 - val_loss: 204.5859 - val_acc: 0.0000e+00\n",
      "Epoch 331/500\n",
      "0s - loss: 171.4334 - acc: 0.0000e+00 - val_loss: 204.5786 - val_acc: 0.0000e+00\n",
      "Epoch 332/500\n",
      "0s - loss: 171.4296 - acc: 0.0000e+00 - val_loss: 204.5721 - val_acc: 0.0000e+00\n",
      "Epoch 333/500\n",
      "0s - loss: 171.4284 - acc: 0.0000e+00 - val_loss: 204.5634 - val_acc: 0.0000e+00\n",
      "Epoch 334/500\n",
      "0s - loss: 171.4234 - acc: 0.0000e+00 - val_loss: 204.5580 - val_acc: 0.0000e+00\n",
      "Epoch 335/500\n",
      "0s - loss: 171.4218 - acc: 0.0000e+00 - val_loss: 204.5505 - val_acc: 0.0000e+00\n",
      "Epoch 336/500\n",
      "0s - loss: 171.4171 - acc: 0.0000e+00 - val_loss: 204.5423 - val_acc: 0.0000e+00\n",
      "Epoch 337/500\n",
      "0s - loss: 171.4147 - acc: 0.0000e+00 - val_loss: 204.5347 - val_acc: 0.0000e+00\n",
      "Epoch 338/500\n",
      "0s - loss: 171.4107 - acc: 0.0000e+00 - val_loss: 204.5276 - val_acc: 0.0000e+00\n",
      "Epoch 339/500\n",
      "0s - loss: 171.4083 - acc: 0.0000e+00 - val_loss: 204.5204 - val_acc: 0.0000e+00\n",
      "Epoch 340/500\n",
      "0s - loss: 171.4050 - acc: 0.0000e+00 - val_loss: 204.5143 - val_acc: 0.0000e+00\n",
      "Epoch 341/500\n",
      "0s - loss: 171.4028 - acc: 0.0000e+00 - val_loss: 204.5074 - val_acc: 0.0000e+00\n",
      "Epoch 342/500\n",
      "0s - loss: 171.3995 - acc: 0.0000e+00 - val_loss: 204.5001 - val_acc: 0.0000e+00\n",
      "Epoch 343/500\n",
      "0s - loss: 171.3960 - acc: 0.0000e+00 - val_loss: 204.4930 - val_acc: 0.0000e+00\n",
      "Epoch 344/500\n",
      "0s - loss: 171.3934 - acc: 0.0000e+00 - val_loss: 204.4877 - val_acc: 0.0000e+00\n",
      "Epoch 345/500\n",
      "0s - loss: 171.3908 - acc: 0.0000e+00 - val_loss: 204.4817 - val_acc: 0.0000e+00\n",
      "Epoch 346/500\n",
      "0s - loss: 171.3883 - acc: 0.0000e+00 - val_loss: 204.4763 - val_acc: 0.0000e+00\n",
      "Epoch 347/500\n",
      "0s - loss: 171.3857 - acc: 0.0000e+00 - val_loss: 204.4694 - val_acc: 0.0000e+00\n",
      "Epoch 348/500\n",
      "0s - loss: 171.3827 - acc: 0.0000e+00 - val_loss: 204.4623 - val_acc: 0.0000e+00\n",
      "Epoch 349/500\n",
      "0s - loss: 171.3797 - acc: 0.0000e+00 - val_loss: 204.4561 - val_acc: 0.0000e+00\n",
      "Epoch 350/500\n",
      "0s - loss: 171.3767 - acc: 0.0000e+00 - val_loss: 204.4501 - val_acc: 0.0000e+00\n",
      "Epoch 351/500\n",
      "0s - loss: 171.3741 - acc: 0.0000e+00 - val_loss: 204.4435 - val_acc: 0.0000e+00\n",
      "Epoch 352/500\n",
      "0s - loss: 171.3720 - acc: 0.0000e+00 - val_loss: 204.4365 - val_acc: 0.0000e+00\n",
      "Epoch 353/500\n",
      "0s - loss: 171.3741 - acc: 0.0000e+00 - val_loss: 204.4344 - val_acc: 0.0000e+00\n",
      "Epoch 354/500\n",
      "0s - loss: 171.3670 - acc: 0.0000e+00 - val_loss: 204.4291 - val_acc: 0.0000e+00\n",
      "Epoch 355/500\n",
      "0s - loss: 171.3652 - acc: 0.0000e+00 - val_loss: 204.4227 - val_acc: 0.0000e+00\n",
      "Epoch 356/500\n",
      "0s - loss: 171.3633 - acc: 0.0000e+00 - val_loss: 204.4161 - val_acc: 0.0000e+00\n",
      "Epoch 357/500\n",
      "0s - loss: 171.3593 - acc: 0.0000e+00 - val_loss: 204.4099 - val_acc: 0.0000e+00\n",
      "Epoch 358/500\n",
      "0s - loss: 171.3567 - acc: 0.0000e+00 - val_loss: 204.4045 - val_acc: 0.0000e+00\n",
      "Epoch 359/500\n",
      "0s - loss: 171.3551 - acc: 0.0000e+00 - val_loss: 204.4005 - val_acc: 0.0000e+00\n",
      "Epoch 360/500\n",
      "0s - loss: 171.3525 - acc: 0.0000e+00 - val_loss: 204.3954 - val_acc: 0.0000e+00\n",
      "Epoch 361/500\n",
      "0s - loss: 171.3498 - acc: 0.0000e+00 - val_loss: 204.3901 - val_acc: 0.0000e+00\n",
      "Epoch 362/500\n",
      "0s - loss: 171.3476 - acc: 0.0000e+00 - val_loss: 204.3833 - val_acc: 0.0000e+00\n",
      "Epoch 363/500\n",
      "0s - loss: 171.3447 - acc: 0.0000e+00 - val_loss: 204.3767 - val_acc: 0.0000e+00\n",
      "Epoch 364/500\n",
      "0s - loss: 171.3426 - acc: 0.0000e+00 - val_loss: 204.3696 - val_acc: 0.0000e+00\n",
      "Epoch 365/500\n",
      "0s - loss: 171.3388 - acc: 0.0000e+00 - val_loss: 204.3636 - val_acc: 0.0000e+00\n",
      "Epoch 366/500\n",
      "0s - loss: 171.3362 - acc: 0.0000e+00 - val_loss: 204.3586 - val_acc: 0.0000e+00\n",
      "Epoch 367/500\n",
      "0s - loss: 171.3361 - acc: 0.0000e+00 - val_loss: 204.3553 - val_acc: 0.0000e+00\n",
      "Epoch 368/500\n",
      "0s - loss: 171.3331 - acc: 0.0000e+00 - val_loss: 204.3479 - val_acc: 0.0000e+00\n",
      "Epoch 369/500\n",
      "0s - loss: 171.3291 - acc: 0.0000e+00 - val_loss: 204.3428 - val_acc: 0.0000e+00\n",
      "Epoch 370/500\n",
      "0s - loss: 171.3268 - acc: 0.0000e+00 - val_loss: 204.3380 - val_acc: 0.0000e+00\n",
      "Epoch 371/500\n",
      "0s - loss: 171.3248 - acc: 0.0000e+00 - val_loss: 204.3326 - val_acc: 0.0000e+00\n",
      "Epoch 372/500\n",
      "0s - loss: 171.3221 - acc: 0.0000e+00 - val_loss: 204.3270 - val_acc: 0.0000e+00\n",
      "Epoch 373/500\n",
      "0s - loss: 171.3196 - acc: 0.0000e+00 - val_loss: 204.3201 - val_acc: 0.0000e+00\n",
      "Epoch 374/500\n",
      "0s - loss: 171.3167 - acc: 0.0000e+00 - val_loss: 204.3145 - val_acc: 0.0000e+00\n",
      "Epoch 375/500\n",
      "0s - loss: 171.3159 - acc: 0.0000e+00 - val_loss: 204.3103 - val_acc: 0.0000e+00\n",
      "Epoch 376/500\n",
      "0s - loss: 171.3126 - acc: 0.0000e+00 - val_loss: 204.3061 - val_acc: 0.0000e+00\n",
      "Epoch 377/500\n",
      "0s - loss: 171.3104 - acc: 0.0000e+00 - val_loss: 204.3008 - val_acc: 0.0000e+00\n",
      "Epoch 378/500\n",
      "0s - loss: 171.3080 - acc: 0.0000e+00 - val_loss: 204.2951 - val_acc: 0.0000e+00\n",
      "Epoch 379/500\n",
      "0s - loss: 171.3054 - acc: 0.0000e+00 - val_loss: 204.2899 - val_acc: 0.0000e+00\n",
      "Epoch 380/500\n",
      "0s - loss: 171.3047 - acc: 0.0000e+00 - val_loss: 204.2818 - val_acc: 0.0000e+00\n",
      "Epoch 381/500\n",
      "0s - loss: 171.3005 - acc: 0.0000e+00 - val_loss: 204.2782 - val_acc: 0.0000e+00\n",
      "Epoch 382/500\n",
      "0s - loss: 171.3004 - acc: 0.0000e+00 - val_loss: 204.2695 - val_acc: 0.0000e+00\n",
      "Epoch 383/500\n",
      "0s - loss: 171.2949 - acc: 0.0000e+00 - val_loss: 204.2642 - val_acc: 0.0000e+00\n",
      "Epoch 384/500\n",
      "0s - loss: 171.2926 - acc: 0.0000e+00 - val_loss: 204.2599 - val_acc: 0.0000e+00\n",
      "Epoch 385/500\n",
      "0s - loss: 171.2903 - acc: 0.0000e+00 - val_loss: 204.2540 - val_acc: 0.0000e+00\n",
      "Epoch 386/500\n",
      "0s - loss: 171.2880 - acc: 0.0000e+00 - val_loss: 204.2507 - val_acc: 0.0000e+00\n",
      "Epoch 387/500\n",
      "0s - loss: 171.2859 - acc: 0.0000e+00 - val_loss: 204.2458 - val_acc: 0.0000e+00\n",
      "Epoch 388/500\n",
      "0s - loss: 171.2843 - acc: 0.0000e+00 - val_loss: 204.2388 - val_acc: 0.0000e+00\n",
      "Epoch 389/500\n",
      "0s - loss: 171.2827 - acc: 0.0000e+00 - val_loss: 204.2313 - val_acc: 0.0000e+00\n",
      "Epoch 390/500\n",
      "0s - loss: 171.2775 - acc: 0.0000e+00 - val_loss: 204.2259 - val_acc: 0.0000e+00\n",
      "Epoch 391/500\n",
      "0s - loss: 171.2759 - acc: 0.0000e+00 - val_loss: 204.2195 - val_acc: 0.0000e+00\n",
      "Epoch 392/500\n",
      "0s - loss: 171.2729 - acc: 0.0000e+00 - val_loss: 204.2115 - val_acc: 0.0000e+00\n",
      "Epoch 393/500\n",
      "0s - loss: 171.2699 - acc: 0.0000e+00 - val_loss: 204.2052 - val_acc: 0.0000e+00\n",
      "Epoch 394/500\n",
      "0s - loss: 171.2664 - acc: 0.0000e+00 - val_loss: 204.2001 - val_acc: 0.0000e+00\n",
      "Epoch 395/500\n",
      "0s - loss: 171.2641 - acc: 0.0000e+00 - val_loss: 204.1945 - val_acc: 0.0000e+00\n",
      "Epoch 396/500\n",
      "0s - loss: 171.2615 - acc: 0.0000e+00 - val_loss: 204.1887 - val_acc: 0.0000e+00\n",
      "Epoch 397/500\n",
      "0s - loss: 171.2592 - acc: 0.0000e+00 - val_loss: 204.1826 - val_acc: 0.0000e+00\n",
      "Epoch 398/500\n",
      "0s - loss: 171.2571 - acc: 0.0000e+00 - val_loss: 204.1775 - val_acc: 0.0000e+00\n",
      "Epoch 399/500\n",
      "0s - loss: 171.2544 - acc: 0.0000e+00 - val_loss: 204.1724 - val_acc: 0.0000e+00\n",
      "Epoch 400/500\n",
      "0s - loss: 171.2522 - acc: 0.0000e+00 - val_loss: 204.1664 - val_acc: 0.0000e+00\n",
      "Epoch 401/500\n",
      "0s - loss: 171.2509 - acc: 0.0000e+00 - val_loss: 204.1591 - val_acc: 0.0000e+00\n",
      "Epoch 402/500\n",
      "0s - loss: 171.2472 - acc: 0.0000e+00 - val_loss: 204.1544 - val_acc: 0.0000e+00\n",
      "Epoch 403/500\n",
      "0s - loss: 171.2444 - acc: 0.0000e+00 - val_loss: 204.1488 - val_acc: 0.0000e+00\n",
      "Epoch 404/500\n",
      "0s - loss: 171.2429 - acc: 0.0000e+00 - val_loss: 204.1408 - val_acc: 0.0000e+00\n",
      "Epoch 405/500\n",
      "0s - loss: 171.2390 - acc: 0.0000e+00 - val_loss: 204.1349 - val_acc: 0.0000e+00\n",
      "Epoch 406/500\n",
      "0s - loss: 171.2362 - acc: 0.0000e+00 - val_loss: 204.1281 - val_acc: 0.0000e+00\n",
      "Epoch 407/500\n",
      "0s - loss: 171.2338 - acc: 0.0000e+00 - val_loss: 204.1222 - val_acc: 0.0000e+00\n",
      "Epoch 408/500\n",
      "0s - loss: 171.2338 - acc: 0.0000e+00 - val_loss: 204.1189 - val_acc: 0.0000e+00\n",
      "Epoch 409/500\n",
      "0s - loss: 171.2299 - acc: 0.0000e+00 - val_loss: 204.1150 - val_acc: 0.0000e+00\n",
      "Epoch 410/500\n",
      "0s - loss: 171.2291 - acc: 0.0000e+00 - val_loss: 204.1075 - val_acc: 0.0000e+00\n",
      "Epoch 411/500\n",
      "0s - loss: 171.2248 - acc: 0.0000e+00 - val_loss: 204.1012 - val_acc: 0.0000e+00\n",
      "Epoch 412/500\n",
      "0s - loss: 171.2228 - acc: 0.0000e+00 - val_loss: 204.0979 - val_acc: 0.0000e+00\n",
      "Epoch 413/500\n",
      "0s - loss: 171.2219 - acc: 0.0000e+00 - val_loss: 204.0947 - val_acc: 0.0000e+00\n",
      "Epoch 414/500\n",
      "0s - loss: 171.2183 - acc: 0.0000e+00 - val_loss: 204.0887 - val_acc: 0.0000e+00\n",
      "Epoch 415/500\n",
      "0s - loss: 171.2166 - acc: 0.0000e+00 - val_loss: 204.0845 - val_acc: 0.0000e+00\n",
      "Epoch 416/500\n",
      "0s - loss: 171.2141 - acc: 0.0000e+00 - val_loss: 204.0774 - val_acc: 0.0000e+00\n",
      "Epoch 417/500\n",
      "0s - loss: 171.2112 - acc: 0.0000e+00 - val_loss: 204.0724 - val_acc: 0.0000e+00\n",
      "Epoch 418/500\n",
      "0s - loss: 171.2088 - acc: 0.0000e+00 - val_loss: 204.0669 - val_acc: 0.0000e+00\n",
      "Epoch 419/500\n",
      "0s - loss: 171.2067 - acc: 0.0000e+00 - val_loss: 204.0623 - val_acc: 0.0000e+00\n",
      "Epoch 420/500\n",
      "0s - loss: 171.2042 - acc: 0.0000e+00 - val_loss: 204.0572 - val_acc: 0.0000e+00\n",
      "Epoch 421/500\n",
      "0s - loss: 171.2027 - acc: 0.0000e+00 - val_loss: 204.0525 - val_acc: 0.0000e+00\n",
      "Epoch 422/500\n",
      "0s - loss: 171.2002 - acc: 0.0000e+00 - val_loss: 204.0484 - val_acc: 0.0000e+00\n",
      "Epoch 423/500\n",
      "0s - loss: 171.1982 - acc: 0.0000e+00 - val_loss: 204.0419 - val_acc: 0.0000e+00\n",
      "Epoch 424/500\n",
      "0s - loss: 171.1955 - acc: 0.0000e+00 - val_loss: 204.0360 - val_acc: 0.0000e+00\n",
      "Epoch 425/500\n",
      "0s - loss: 171.1932 - acc: 0.0000e+00 - val_loss: 204.0293 - val_acc: 0.0000e+00\n",
      "Epoch 426/500\n",
      "0s - loss: 171.1902 - acc: 0.0000e+00 - val_loss: 204.0238 - val_acc: 0.0000e+00\n",
      "Epoch 427/500\n",
      "0s - loss: 171.1877 - acc: 0.0000e+00 - val_loss: 204.0177 - val_acc: 0.0000e+00\n",
      "Epoch 428/500\n",
      "0s - loss: 171.1853 - acc: 0.0000e+00 - val_loss: 204.0116 - val_acc: 0.0000e+00\n",
      "Epoch 429/500\n",
      "0s - loss: 171.1826 - acc: 0.0000e+00 - val_loss: 204.0063 - val_acc: 0.0000e+00\n",
      "Epoch 430/500\n",
      "0s - loss: 171.1803 - acc: 0.0000e+00 - val_loss: 204.0001 - val_acc: 0.0000e+00\n",
      "Epoch 431/500\n",
      "0s - loss: 171.1775 - acc: 0.0000e+00 - val_loss: 203.9946 - val_acc: 0.0000e+00\n",
      "Epoch 432/500\n",
      "0s - loss: 171.1752 - acc: 0.0000e+00 - val_loss: 203.9891 - val_acc: 0.0000e+00\n",
      "Epoch 433/500\n",
      "0s - loss: 171.1729 - acc: 0.0000e+00 - val_loss: 203.9826 - val_acc: 0.0000e+00\n",
      "Epoch 434/500\n",
      "0s - loss: 171.1719 - acc: 0.0000e+00 - val_loss: 203.9756 - val_acc: 0.0000e+00\n",
      "Epoch 435/500\n",
      "0s - loss: 171.1673 - acc: 0.0000e+00 - val_loss: 203.9693 - val_acc: 0.0000e+00\n",
      "Epoch 436/500\n",
      "0s - loss: 171.1645 - acc: 0.0000e+00 - val_loss: 203.9644 - val_acc: 0.0000e+00\n",
      "Epoch 437/500\n",
      "0s - loss: 171.1626 - acc: 0.0000e+00 - val_loss: 203.9609 - val_acc: 0.0000e+00\n",
      "Epoch 438/500\n",
      "0s - loss: 171.1613 - acc: 0.0000e+00 - val_loss: 203.9561 - val_acc: 0.0000e+00\n",
      "Epoch 439/500\n",
      "0s - loss: 171.1587 - acc: 0.0000e+00 - val_loss: 203.9499 - val_acc: 0.0000e+00\n",
      "Epoch 440/500\n",
      "0s - loss: 171.1562 - acc: 0.0000e+00 - val_loss: 203.9457 - val_acc: 0.0000e+00\n",
      "Epoch 441/500\n",
      "0s - loss: 171.1542 - acc: 0.0000e+00 - val_loss: 203.9407 - val_acc: 0.0000e+00\n",
      "Epoch 442/500\n",
      "0s - loss: 171.1523 - acc: 0.0000e+00 - val_loss: 203.9359 - val_acc: 0.0000e+00\n",
      "Epoch 443/500\n",
      "0s - loss: 171.1501 - acc: 0.0000e+00 - val_loss: 203.9317 - val_acc: 0.0000e+00\n",
      "Epoch 444/500\n",
      "0s - loss: 171.1480 - acc: 0.0000e+00 - val_loss: 203.9273 - val_acc: 0.0000e+00\n",
      "Epoch 445/500\n",
      "0s - loss: 171.1462 - acc: 0.0000e+00 - val_loss: 203.9200 - val_acc: 0.0000e+00\n",
      "Epoch 446/500\n",
      "0s - loss: 171.1436 - acc: 0.0000e+00 - val_loss: 203.9139 - val_acc: 0.0000e+00\n",
      "Epoch 447/500\n",
      "0s - loss: 171.1404 - acc: 0.0000e+00 - val_loss: 203.9094 - val_acc: 0.0000e+00\n",
      "Epoch 448/500\n",
      "0s - loss: 171.1386 - acc: 0.0000e+00 - val_loss: 203.9043 - val_acc: 0.0000e+00\n",
      "Epoch 449/500\n",
      "0s - loss: 171.1368 - acc: 0.0000e+00 - val_loss: 203.8999 - val_acc: 0.0000e+00\n",
      "Epoch 450/500\n",
      "0s - loss: 171.1345 - acc: 0.0000e+00 - val_loss: 203.8935 - val_acc: 0.0000e+00\n",
      "Epoch 451/500\n",
      "0s - loss: 171.1313 - acc: 0.0000e+00 - val_loss: 203.8880 - val_acc: 0.0000e+00\n",
      "Epoch 452/500\n",
      "0s - loss: 171.1292 - acc: 0.0000e+00 - val_loss: 203.8844 - val_acc: 0.0000e+00\n",
      "Epoch 453/500\n",
      "0s - loss: 171.1269 - acc: 0.0000e+00 - val_loss: 203.8786 - val_acc: 0.0000e+00\n",
      "Epoch 454/500\n",
      "0s - loss: 171.1251 - acc: 0.0000e+00 - val_loss: 203.8737 - val_acc: 0.0000e+00\n",
      "Epoch 455/500\n",
      "0s - loss: 171.1232 - acc: 0.0000e+00 - val_loss: 203.8690 - val_acc: 0.0000e+00\n",
      "Epoch 456/500\n",
      "0s - loss: 171.1234 - acc: 0.0000e+00 - val_loss: 203.8658 - val_acc: 0.0000e+00\n",
      "Epoch 457/500\n",
      "0s - loss: 171.1194 - acc: 0.0000e+00 - val_loss: 203.8597 - val_acc: 0.0000e+00\n",
      "Epoch 458/500\n",
      "0s - loss: 171.1167 - acc: 0.0000e+00 - val_loss: 203.8553 - val_acc: 0.0000e+00\n",
      "Epoch 459/500\n",
      "0s - loss: 171.1148 - acc: 0.0000e+00 - val_loss: 203.8483 - val_acc: 0.0000e+00\n",
      "Epoch 460/500\n",
      "0s - loss: 171.1119 - acc: 0.0000e+00 - val_loss: 203.8432 - val_acc: 0.0000e+00\n",
      "Epoch 461/500\n",
      "0s - loss: 171.1103 - acc: 0.0000e+00 - val_loss: 203.8394 - val_acc: 0.0000e+00\n",
      "Epoch 462/500\n",
      "0s - loss: 171.1075 - acc: 0.0000e+00 - val_loss: 203.8333 - val_acc: 0.0000e+00\n",
      "Epoch 463/500\n",
      "0s - loss: 171.1080 - acc: 0.0000e+00 - val_loss: 203.8301 - val_acc: 0.0000e+00\n",
      "Epoch 464/500\n",
      "0s - loss: 171.1040 - acc: 0.0000e+00 - val_loss: 203.8240 - val_acc: 0.0000e+00\n",
      "Epoch 465/500\n",
      "0s - loss: 171.1013 - acc: 0.0000e+00 - val_loss: 203.8190 - val_acc: 0.0000e+00\n",
      "Epoch 466/500\n",
      "0s - loss: 171.0991 - acc: 0.0000e+00 - val_loss: 203.8147 - val_acc: 0.0000e+00\n",
      "Epoch 467/500\n",
      "0s - loss: 171.0969 - acc: 0.0000e+00 - val_loss: 203.8100 - val_acc: 0.0000e+00\n",
      "Epoch 468/500\n",
      "0s - loss: 171.0949 - acc: 0.0000e+00 - val_loss: 203.8037 - val_acc: 0.0000e+00\n",
      "Epoch 469/500\n",
      "0s - loss: 171.0926 - acc: 0.0000e+00 - val_loss: 203.7978 - val_acc: 0.0000e+00\n",
      "Epoch 470/500\n",
      "0s - loss: 171.0904 - acc: 0.0000e+00 - val_loss: 203.7926 - val_acc: 0.0000e+00\n",
      "Epoch 471/500\n",
      "0s - loss: 171.0881 - acc: 0.0000e+00 - val_loss: 203.7875 - val_acc: 0.0000e+00\n",
      "Epoch 472/500\n",
      "0s - loss: 171.0862 - acc: 0.0000e+00 - val_loss: 203.7838 - val_acc: 0.0000e+00\n",
      "Epoch 473/500\n",
      "0s - loss: 171.0838 - acc: 0.0000e+00 - val_loss: 203.7782 - val_acc: 0.0000e+00\n",
      "Epoch 474/500\n",
      "0s - loss: 171.0818 - acc: 0.0000e+00 - val_loss: 203.7726 - val_acc: 0.0000e+00\n",
      "Epoch 475/500\n",
      "0s - loss: 171.0813 - acc: 0.0000e+00 - val_loss: 203.7706 - val_acc: 0.0000e+00\n",
      "Epoch 476/500\n",
      "0s - loss: 171.0777 - acc: 0.0000e+00 - val_loss: 203.7643 - val_acc: 0.0000e+00\n",
      "Epoch 477/500\n",
      "0s - loss: 171.0753 - acc: 0.0000e+00 - val_loss: 203.7589 - val_acc: 0.0000e+00\n",
      "Epoch 478/500\n",
      "0s - loss: 171.0730 - acc: 0.0000e+00 - val_loss: 203.7530 - val_acc: 0.0000e+00\n",
      "Epoch 479/500\n",
      "0s - loss: 171.0717 - acc: 0.0000e+00 - val_loss: 203.7497 - val_acc: 0.0000e+00\n",
      "Epoch 480/500\n",
      "0s - loss: 171.0690 - acc: 0.0000e+00 - val_loss: 203.7432 - val_acc: 0.0000e+00\n",
      "Epoch 481/500\n",
      "0s - loss: 171.0664 - acc: 0.0000e+00 - val_loss: 203.7379 - val_acc: 0.0000e+00\n",
      "Epoch 482/500\n",
      "0s - loss: 171.0641 - acc: 0.0000e+00 - val_loss: 203.7315 - val_acc: 0.0000e+00\n",
      "Epoch 483/500\n",
      "0s - loss: 171.0617 - acc: 0.0000e+00 - val_loss: 203.7270 - val_acc: 0.0000e+00\n",
      "Epoch 484/500\n",
      "0s - loss: 171.0592 - acc: 0.0000e+00 - val_loss: 203.7223 - val_acc: 0.0000e+00\n",
      "Epoch 485/500\n",
      "0s - loss: 171.0573 - acc: 0.0000e+00 - val_loss: 203.7160 - val_acc: 0.0000e+00\n",
      "Epoch 486/500\n",
      "0s - loss: 171.0549 - acc: 0.0000e+00 - val_loss: 203.7117 - val_acc: 0.0000e+00\n",
      "Epoch 487/500\n",
      "0s - loss: 171.0572 - acc: 0.0000e+00 - val_loss: 203.7017 - val_acc: 0.0000e+00\n",
      "Epoch 488/500\n",
      "0s - loss: 171.0490 - acc: 0.0000e+00 - val_loss: 203.6966 - val_acc: 0.0000e+00\n",
      "Epoch 489/500\n",
      "0s - loss: 171.0467 - acc: 0.0000e+00 - val_loss: 203.6912 - val_acc: 0.0000e+00\n",
      "Epoch 490/500\n",
      "0s - loss: 171.0448 - acc: 0.0000e+00 - val_loss: 203.6859 - val_acc: 0.0000e+00\n",
      "Epoch 491/500\n",
      "0s - loss: 171.0443 - acc: 0.0000e+00 - val_loss: 203.6795 - val_acc: 0.0000e+00\n",
      "Epoch 492/500\n",
      "0s - loss: 171.0397 - acc: 0.0000e+00 - val_loss: 203.6753 - val_acc: 0.0000e+00\n",
      "Epoch 493/500\n",
      "0s - loss: 171.0379 - acc: 0.0000e+00 - val_loss: 203.6703 - val_acc: 0.0000e+00\n",
      "Epoch 494/500\n",
      "0s - loss: 171.0358 - acc: 0.0000e+00 - val_loss: 203.6650 - val_acc: 0.0000e+00\n",
      "Epoch 495/500\n",
      "0s - loss: 171.0335 - acc: 0.0000e+00 - val_loss: 203.6594 - val_acc: 0.0000e+00\n",
      "Epoch 496/500\n",
      "0s - loss: 171.0319 - acc: 0.0000e+00 - val_loss: 203.6553 - val_acc: 0.0000e+00\n",
      "Epoch 497/500\n",
      "0s - loss: 171.0305 - acc: 0.0000e+00 - val_loss: 203.6471 - val_acc: 0.0000e+00\n",
      "Epoch 498/500\n",
      "0s - loss: 171.0261 - acc: 0.0000e+00 - val_loss: 203.6429 - val_acc: 0.0000e+00\n",
      "Epoch 499/500\n",
      "0s - loss: 171.0242 - acc: 0.0000e+00 - val_loss: 203.6382 - val_acc: 0.0000e+00\n",
      "Epoch 500/500\n",
      "0s - loss: 171.0220 - acc: 0.0000e+00 - val_loss: 203.6313 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11c261588>"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelDred.fit(x_train,y_train, batch_size=57, nb_epoch=500, verbose=2, \n",
    "          callbacks=[], validation_split=0.1, validation_data=[x_val,y_val],\n",
    "              shuffle=True, class_weight=None)#cl_w_ing, sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#descriptive names\n",
    "df=pd.read_csv('20130401.export.CSV',delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "header1=pd.read_csv('CSV.header.dailyupdates.txt',delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.columns=list(header1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GLOBALEVENTID</th>\n",
       "      <th>SQLDATE</th>\n",
       "      <th>MonthYear</th>\n",
       "      <th>Year</th>\n",
       "      <th>FractionDate</th>\n",
       "      <th>Actor1Code</th>\n",
       "      <th>Actor1Name</th>\n",
       "      <th>Actor1CountryCode</th>\n",
       "      <th>Actor1KnownGroupCode</th>\n",
       "      <th>Actor1EthnicCode</th>\n",
       "      <th>...</th>\n",
       "      <th>Actor2Geo_FeatureID</th>\n",
       "      <th>ActionGeo_Type</th>\n",
       "      <th>ActionGeo_FullName</th>\n",
       "      <th>ActionGeo_CountryCode</th>\n",
       "      <th>ActionGeo_ADM1Code</th>\n",
       "      <th>ActionGeo_Lat</th>\n",
       "      <th>ActionGeo_Long</th>\n",
       "      <th>ActionGeo_FeatureID</th>\n",
       "      <th>DATEADDED</th>\n",
       "      <th>SOURCEURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>253461012</td>\n",
       "      <td>20030404</td>\n",
       "      <td>200304</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003.2575</td>\n",
       "      <td>AUS</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>AUS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>AS</td>\n",
       "      <td>1</td>\n",
       "      <td>Australia</td>\n",
       "      <td>AS</td>\n",
       "      <td>AS</td>\n",
       "      <td>-27.0000</td>\n",
       "      <td>133.000</td>\n",
       "      <td>AS</td>\n",
       "      <td>20130401</td>\n",
       "      <td>http://www.bangkokpost.com/breakingnews/343522...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>253461013</td>\n",
       "      <td>20030404</td>\n",
       "      <td>200304</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003.2575</td>\n",
       "      <td>BUS</td>\n",
       "      <td>SHOP OWNER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-1354145</td>\n",
       "      <td>4</td>\n",
       "      <td>Tai Hang, Hong Kong (general), Hong Kong</td>\n",
       "      <td>HK</td>\n",
       "      <td>HK00</td>\n",
       "      <td>22.4667</td>\n",
       "      <td>114.150</td>\n",
       "      <td>-1354145</td>\n",
       "      <td>20130401</td>\n",
       "      <td>http://www.bloomberg.com/news/2013-04-01/hong-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>253461014</td>\n",
       "      <td>20030404</td>\n",
       "      <td>200304</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003.2575</td>\n",
       "      <td>BUS</td>\n",
       "      <td>SHOP OWNER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-1354454</td>\n",
       "      <td>4</td>\n",
       "      <td>Tai Hang, Hong Kong (general), Hong Kong</td>\n",
       "      <td>HK</td>\n",
       "      <td>HK00</td>\n",
       "      <td>22.4667</td>\n",
       "      <td>114.150</td>\n",
       "      <td>-1354145</td>\n",
       "      <td>20130401</td>\n",
       "      <td>http://www.bloomberg.com/news/2013-04-01/hong-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>253461015</td>\n",
       "      <td>20030404</td>\n",
       "      <td>200304</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003.2575</td>\n",
       "      <td>CVL</td>\n",
       "      <td>MIGRANT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>AS</td>\n",
       "      <td>1</td>\n",
       "      <td>Australia</td>\n",
       "      <td>AS</td>\n",
       "      <td>AS</td>\n",
       "      <td>-27.0000</td>\n",
       "      <td>133.000</td>\n",
       "      <td>AS</td>\n",
       "      <td>20130401</td>\n",
       "      <td>http://www.bangkokpost.com/breakingnews/343522...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>253461016</td>\n",
       "      <td>20030404</td>\n",
       "      <td>200304</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003.2575</td>\n",
       "      <td>HLH</td>\n",
       "      <td>DOCTOR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>Nevada, United States</td>\n",
       "      <td>US</td>\n",
       "      <td>USNV</td>\n",
       "      <td>38.4199</td>\n",
       "      <td>-117.122</td>\n",
       "      <td>NV</td>\n",
       "      <td>20130401</td>\n",
       "      <td>http://www.startribune.com/nation/200818961.html</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   GLOBALEVENTID   SQLDATE  MonthYear  Year  FractionDate Actor1Code  \\\n",
       "0      253461012  20030404     200304  2003     2003.2575        AUS   \n",
       "1      253461013  20030404     200304  2003     2003.2575        BUS   \n",
       "2      253461014  20030404     200304  2003     2003.2575        BUS   \n",
       "3      253461015  20030404     200304  2003     2003.2575        CVL   \n",
       "4      253461016  20030404     200304  2003     2003.2575        HLH   \n",
       "\n",
       "   Actor1Name Actor1CountryCode Actor1KnownGroupCode Actor1EthnicCode  \\\n",
       "0   AUSTRALIA               AUS                  NaN              NaN   \n",
       "1  SHOP OWNER               NaN                  NaN              NaN   \n",
       "2  SHOP OWNER               NaN                  NaN              NaN   \n",
       "3     MIGRANT               NaN                  NaN              NaN   \n",
       "4      DOCTOR               NaN                  NaN              NaN   \n",
       "\n",
       "                         ...                         Actor2Geo_FeatureID  \\\n",
       "0                        ...                                          AS   \n",
       "1                        ...                                    -1354145   \n",
       "2                        ...                                    -1354454   \n",
       "3                        ...                                          AS   \n",
       "4                        ...                                         NaN   \n",
       "\n",
       "  ActionGeo_Type                        ActionGeo_FullName  \\\n",
       "0              1                                 Australia   \n",
       "1              4  Tai Hang, Hong Kong (general), Hong Kong   \n",
       "2              4  Tai Hang, Hong Kong (general), Hong Kong   \n",
       "3              1                                 Australia   \n",
       "4              2                     Nevada, United States   \n",
       "\n",
       "  ActionGeo_CountryCode ActionGeo_ADM1Code ActionGeo_Lat ActionGeo_Long  \\\n",
       "0                    AS                 AS      -27.0000        133.000   \n",
       "1                    HK               HK00       22.4667        114.150   \n",
       "2                    HK               HK00       22.4667        114.150   \n",
       "3                    AS                 AS      -27.0000        133.000   \n",
       "4                    US               USNV       38.4199       -117.122   \n",
       "\n",
       "  ActionGeo_FeatureID DATEADDED  \\\n",
       "0                  AS  20130401   \n",
       "1            -1354145  20130401   \n",
       "2            -1354145  20130401   \n",
       "3                  AS  20130401   \n",
       "4                  NV  20130401   \n",
       "\n",
       "                                           SOURCEURL  \n",
       "0  http://www.bangkokpost.com/breakingnews/343522...  \n",
       "1  http://www.bloomberg.com/news/2013-04-01/hong-...  \n",
       "2  http://www.bloomberg.com/news/2013-04-01/hong-...  \n",
       "3  http://www.bangkokpost.com/breakingnews/343522...  \n",
       "4   http://www.startribune.com/nation/200818961.html  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FractionDate</th>\n",
       "      <th>GoldsteinScale</th>\n",
       "      <th>NumMentions</th>\n",
       "      <th>AvgTone</th>\n",
       "      <th>Actor1Code_AFG</th>\n",
       "      <th>Actor1Code_AFGBUS</th>\n",
       "      <th>Actor1Code_AFGCOP</th>\n",
       "      <th>Actor1Code_AFGCVL</th>\n",
       "      <th>Actor1Code_AFGGOV</th>\n",
       "      <th>Actor1Code_AFGGOVEDU</th>\n",
       "      <th>...</th>\n",
       "      <th>EventCode_1723</th>\n",
       "      <th>EventCode_1724</th>\n",
       "      <th>EventCode_1821</th>\n",
       "      <th>EventCode_1822</th>\n",
       "      <th>EventCode_1823</th>\n",
       "      <th>EventCode_1831</th>\n",
       "      <th>QuadClass_1</th>\n",
       "      <th>QuadClass_2</th>\n",
       "      <th>QuadClass_3</th>\n",
       "      <th>QuadClass_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003.2575</td>\n",
       "      <td>2.8</td>\n",
       "      <td>10</td>\n",
       "      <td>2.222222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003.2575</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>8</td>\n",
       "      <td>2.167369</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003.2575</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.167369</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003.2575</td>\n",
       "      <td>1.9</td>\n",
       "      <td>10</td>\n",
       "      <td>2.222222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2003.2575</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>10</td>\n",
       "      <td>1.843318</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5345 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   FractionDate  GoldsteinScale  NumMentions   AvgTone  Actor1Code_AFG  \\\n",
       "0     2003.2575             2.8           10  2.222222             0.0   \n",
       "1     2003.2575            -5.0            8  2.167369             0.0   \n",
       "2     2003.2575            -5.0            2  2.167369             0.0   \n",
       "3     2003.2575             1.9           10  2.222222             0.0   \n",
       "4     2003.2575            -0.4           10  1.843318             0.0   \n",
       "\n",
       "   Actor1Code_AFGBUS  Actor1Code_AFGCOP  Actor1Code_AFGCVL  Actor1Code_AFGGOV  \\\n",
       "0                0.0                0.0                0.0                0.0   \n",
       "1                0.0                0.0                0.0                0.0   \n",
       "2                0.0                0.0                0.0                0.0   \n",
       "3                0.0                0.0                0.0                0.0   \n",
       "4                0.0                0.0                0.0                0.0   \n",
       "\n",
       "   Actor1Code_AFGGOVEDU     ...       EventCode_1723  EventCode_1724  \\\n",
       "0                   0.0     ...                  0.0             0.0   \n",
       "1                   0.0     ...                  0.0             0.0   \n",
       "2                   0.0     ...                  0.0             0.0   \n",
       "3                   0.0     ...                  0.0             0.0   \n",
       "4                   0.0     ...                  0.0             0.0   \n",
       "\n",
       "   EventCode_1821  EventCode_1822  EventCode_1823  EventCode_1831  \\\n",
       "0             0.0             0.0             0.0             0.0   \n",
       "1             0.0             0.0             0.0             0.0   \n",
       "2             0.0             0.0             0.0             0.0   \n",
       "3             0.0             0.0             0.0             0.0   \n",
       "4             0.0             0.0             0.0             0.0   \n",
       "\n",
       "   QuadClass_1  QuadClass_2  QuadClass_3  QuadClass_4  \n",
       "0          1.0          0.0          0.0          0.0  \n",
       "1          0.0          0.0          0.0          1.0  \n",
       "2          0.0          0.0          0.0          1.0  \n",
       "3          1.0          0.0          0.0          0.0  \n",
       "4          1.0          0.0          0.0          0.0  \n",
       "\n",
       "[5 rows x 5345 columns]"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_dummies = pd.get_dummies(df[feat_columns], columns = cat_columns )\n",
    "df_with_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=1,tokenizer=my_tokenizer)\n",
    "corpus=[df.iloc[i,-1] for i in range(len(df))]\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "Y=X.toarray()\n",
    "for i,col in enumerate(vectorizer.get_feature_names()):\n",
    "    df_with_dummies[col]=pd.DataFrame(Y[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizertfidf = TfidfVectorizer(min_df=1,tokenizer=my_tokenizer)\n",
    "Xtfidf = vectorizertfidf.fit_transform(corpus)\n",
    "Ytfidf=Xtfidf.toarray()\n",
    "for i,col in enumerate(vectorizertfidf.get_feature_names()):\n",
    "    df_with_dummies['tfidf'+col]=pd.DataFrame(Ytfidf[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feat_df=df_with_dummies.iloc[:,0:10401]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FractionDate</th>\n",
       "      <th>GoldsteinScale</th>\n",
       "      <th>NumMentions</th>\n",
       "      <th>AvgTone</th>\n",
       "      <th>Actor1Code_AFG</th>\n",
       "      <th>Actor1Code_AFGBUS</th>\n",
       "      <th>Actor1Code_AFGCOP</th>\n",
       "      <th>Actor1Code_AFGCVL</th>\n",
       "      <th>Actor1Code_AFGGOV</th>\n",
       "      <th>Actor1Code_AFGGOVEDU</th>\n",
       "      <th>...</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zealotri</th>\n",
       "      <th>zeidan</th>\n",
       "      <th>zelda</th>\n",
       "      <th>zhiggkoea</th>\n",
       "      <th>ziivhmez_uxlgpnlo</th>\n",
       "      <th>zikir</th>\n",
       "      <th>zipwir</th>\n",
       "      <th>zmnmbcosjccynudfnuig</th>\n",
       "      <th>zoo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003.2575</td>\n",
       "      <td>2.8</td>\n",
       "      <td>10</td>\n",
       "      <td>2.222222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003.2575</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>8</td>\n",
       "      <td>2.167369</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003.2575</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.167369</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003.2575</td>\n",
       "      <td>1.9</td>\n",
       "      <td>10</td>\n",
       "      <td>2.222222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2003.2575</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>10</td>\n",
       "      <td>1.843318</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 10401 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   FractionDate  GoldsteinScale  NumMentions   AvgTone  Actor1Code_AFG  \\\n",
       "0     2003.2575             2.8           10  2.222222             0.0   \n",
       "1     2003.2575            -5.0            8  2.167369             0.0   \n",
       "2     2003.2575            -5.0            2  2.167369             0.0   \n",
       "3     2003.2575             1.9           10  2.222222             0.0   \n",
       "4     2003.2575            -0.4           10  1.843318             0.0   \n",
       "\n",
       "   Actor1Code_AFGBUS  Actor1Code_AFGCOP  Actor1Code_AFGCVL  Actor1Code_AFGGOV  \\\n",
       "0                0.0                0.0                0.0                0.0   \n",
       "1                0.0                0.0                0.0                0.0   \n",
       "2                0.0                0.0                0.0                0.0   \n",
       "3                0.0                0.0                0.0                0.0   \n",
       "4                0.0                0.0                0.0                0.0   \n",
       "\n",
       "   Actor1Code_AFGGOVEDU ...   zealand  zealotri  zeidan  zelda  zhiggkoea  \\\n",
       "0                   0.0 ...         0         0       0      0          0   \n",
       "1                   0.0 ...         0         0       0      0          0   \n",
       "2                   0.0 ...         0         0       0      0          0   \n",
       "3                   0.0 ...         0         0       0      0          0   \n",
       "4                   0.0 ...         0         0       0      0          0   \n",
       "\n",
       "   ziivhmez_uxlgpnlo  zikir  zipwir  zmnmbcosjccynudfnuig  zoo  \n",
       "0                  0      0       0                     0    0  \n",
       "1                  0      0       0                     0    0  \n",
       "2                  0      0       0                     0    0  \n",
       "3                  0      0       0                     0    0  \n",
       "4                  0      0       0                     0    0  \n",
       "\n",
       "[5 rows x 10401 columns]"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feattfidf_df=df_with_dummies.iloc[:,list(range(0,5345))+list(range(10401,15457))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FractionDate</th>\n",
       "      <th>GoldsteinScale</th>\n",
       "      <th>NumMentions</th>\n",
       "      <th>AvgTone</th>\n",
       "      <th>Actor1Code_AFG</th>\n",
       "      <th>Actor1Code_AFGBUS</th>\n",
       "      <th>Actor1Code_AFGCOP</th>\n",
       "      <th>Actor1Code_AFGCVL</th>\n",
       "      <th>Actor1Code_AFGGOV</th>\n",
       "      <th>Actor1Code_AFGGOVEDU</th>\n",
       "      <th>...</th>\n",
       "      <th>tfidfzealand</th>\n",
       "      <th>tfidfzealotri</th>\n",
       "      <th>tfidfzeidan</th>\n",
       "      <th>tfidfzelda</th>\n",
       "      <th>tfidfzhiggkoea</th>\n",
       "      <th>tfidfziivhmez_uxlgpnlo</th>\n",
       "      <th>tfidfzikir</th>\n",
       "      <th>tfidfzipwir</th>\n",
       "      <th>tfidfzmnmbcosjccynudfnuig</th>\n",
       "      <th>tfidfzoo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003.2575</td>\n",
       "      <td>2.8</td>\n",
       "      <td>10</td>\n",
       "      <td>2.222222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003.2575</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>8</td>\n",
       "      <td>2.167369</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003.2575</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.167369</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003.2575</td>\n",
       "      <td>1.9</td>\n",
       "      <td>10</td>\n",
       "      <td>2.222222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2003.2575</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>10</td>\n",
       "      <td>1.843318</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 10401 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   FractionDate  GoldsteinScale  NumMentions   AvgTone  Actor1Code_AFG  \\\n",
       "0     2003.2575             2.8           10  2.222222             0.0   \n",
       "1     2003.2575            -5.0            8  2.167369             0.0   \n",
       "2     2003.2575            -5.0            2  2.167369             0.0   \n",
       "3     2003.2575             1.9           10  2.222222             0.0   \n",
       "4     2003.2575            -0.4           10  1.843318             0.0   \n",
       "\n",
       "   Actor1Code_AFGBUS  Actor1Code_AFGCOP  Actor1Code_AFGCVL  Actor1Code_AFGGOV  \\\n",
       "0                0.0                0.0                0.0                0.0   \n",
       "1                0.0                0.0                0.0                0.0   \n",
       "2                0.0                0.0                0.0                0.0   \n",
       "3                0.0                0.0                0.0                0.0   \n",
       "4                0.0                0.0                0.0                0.0   \n",
       "\n",
       "   Actor1Code_AFGGOVEDU    ...     tfidfzealand  tfidfzealotri  tfidfzeidan  \\\n",
       "0                   0.0    ...              0.0            0.0          0.0   \n",
       "1                   0.0    ...              0.0            0.0          0.0   \n",
       "2                   0.0    ...              0.0            0.0          0.0   \n",
       "3                   0.0    ...              0.0            0.0          0.0   \n",
       "4                   0.0    ...              0.0            0.0          0.0   \n",
       "\n",
       "   tfidfzelda  tfidfzhiggkoea  tfidfziivhmez_uxlgpnlo  tfidfzikir  \\\n",
       "0         0.0             0.0                     0.0         0.0   \n",
       "1         0.0             0.0                     0.0         0.0   \n",
       "2         0.0             0.0                     0.0         0.0   \n",
       "3         0.0             0.0                     0.0         0.0   \n",
       "4         0.0             0.0                     0.0         0.0   \n",
       "\n",
       "   tfidfzipwir  tfidfzmnmbcosjccynudfnuig  tfidfzoo  \n",
       "0          0.0                        0.0       0.0  \n",
       "1          0.0                        0.0       0.0  \n",
       "2          0.0                        0.0       0.0  \n",
       "3          0.0                        0.0       0.0  \n",
       "4          0.0                        0.0       0.0  \n",
       "\n",
       "[5 rows x 10401 columns]"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feattfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1040100"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10401*len(feattfidfRED_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feattfidfRED_df=(feattfidf_df.sort_values('NumMentions', ascending=False))[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FractionDate</th>\n",
       "      <th>GoldsteinScale</th>\n",
       "      <th>NumMentions</th>\n",
       "      <th>AvgTone</th>\n",
       "      <th>Actor1Code_AFG</th>\n",
       "      <th>Actor1Code_AFGBUS</th>\n",
       "      <th>Actor1Code_AFGCOP</th>\n",
       "      <th>Actor1Code_AFGCVL</th>\n",
       "      <th>Actor1Code_AFGGOV</th>\n",
       "      <th>Actor1Code_AFGGOVEDU</th>\n",
       "      <th>...</th>\n",
       "      <th>tfidfzealand</th>\n",
       "      <th>tfidfzealotri</th>\n",
       "      <th>tfidfzeidan</th>\n",
       "      <th>tfidfzelda</th>\n",
       "      <th>tfidfzhiggkoea</th>\n",
       "      <th>tfidfziivhmez_uxlgpnlo</th>\n",
       "      <th>tfidfzikir</th>\n",
       "      <th>tfidfzipwir</th>\n",
       "      <th>tfidfzmnmbcosjccynudfnuig</th>\n",
       "      <th>tfidfzoo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3196</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>643</td>\n",
       "      <td>3.227707</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18436</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>3.0</td>\n",
       "      <td>384</td>\n",
       "      <td>4.111273</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12787</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>372</td>\n",
       "      <td>3.989697</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6969</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>3.0</td>\n",
       "      <td>299</td>\n",
       "      <td>1.849065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2468</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>-7.2</td>\n",
       "      <td>290</td>\n",
       "      <td>1.656668</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6971</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>280</td>\n",
       "      <td>1.674708</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24163</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>275</td>\n",
       "      <td>1.254327</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>270</td>\n",
       "      <td>3.960111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15388</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>256</td>\n",
       "      <td>1.609864</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6882</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>246</td>\n",
       "      <td>3.980200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25982</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>185</td>\n",
       "      <td>1.270816</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21765</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>7.0</td>\n",
       "      <td>180</td>\n",
       "      <td>2.596207</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15966</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>179</td>\n",
       "      <td>1.054735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21761</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>3.4</td>\n",
       "      <td>168</td>\n",
       "      <td>2.717686</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25328</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>8.3</td>\n",
       "      <td>165</td>\n",
       "      <td>1.419494</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21779</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>164</td>\n",
       "      <td>2.562969</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15610</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>162</td>\n",
       "      <td>2.194546</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17320</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>160</td>\n",
       "      <td>2.020098</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20178</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>2.8</td>\n",
       "      <td>160</td>\n",
       "      <td>2.229886</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15178</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>160</td>\n",
       "      <td>2.295452</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19463</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>7.0</td>\n",
       "      <td>160</td>\n",
       "      <td>2.565543</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18370</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>152</td>\n",
       "      <td>1.082084</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8507</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>152</td>\n",
       "      <td>1.233950</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21792</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>150</td>\n",
       "      <td>2.516193</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2880</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>4.0</td>\n",
       "      <td>150</td>\n",
       "      <td>2.405127</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20077</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>4.0</td>\n",
       "      <td>150</td>\n",
       "      <td>2.405127</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20886</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>3.4</td>\n",
       "      <td>150</td>\n",
       "      <td>2.516193</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24515</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>8.3</td>\n",
       "      <td>142</td>\n",
       "      <td>1.425969</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8077</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>1.9</td>\n",
       "      <td>141</td>\n",
       "      <td>2.145737</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15428</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>141</td>\n",
       "      <td>2.254096</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16269</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>7.0</td>\n",
       "      <td>110</td>\n",
       "      <td>2.245010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11312</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>3.0</td>\n",
       "      <td>110</td>\n",
       "      <td>2.303969</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13184</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>110</td>\n",
       "      <td>2.255151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7157</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>7.0</td>\n",
       "      <td>107</td>\n",
       "      <td>2.558647</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5666</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>3.0</td>\n",
       "      <td>106</td>\n",
       "      <td>4.027624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4232</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>1.9</td>\n",
       "      <td>106</td>\n",
       "      <td>0.844023</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24251</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>1.9</td>\n",
       "      <td>106</td>\n",
       "      <td>0.844023</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24260</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>2.8</td>\n",
       "      <td>106</td>\n",
       "      <td>0.844023</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11398</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>4.0</td>\n",
       "      <td>106</td>\n",
       "      <td>1.212151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4242</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>2.8</td>\n",
       "      <td>106</td>\n",
       "      <td>0.844023</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5663</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>3.0</td>\n",
       "      <td>105</td>\n",
       "      <td>3.280937</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7142</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>6.4</td>\n",
       "      <td>104</td>\n",
       "      <td>2.660422</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7169</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>1.9</td>\n",
       "      <td>104</td>\n",
       "      <td>2.660422</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7173</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>6.4</td>\n",
       "      <td>104</td>\n",
       "      <td>2.660422</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18825</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>104</td>\n",
       "      <td>4.015640</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19574</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>-6.5</td>\n",
       "      <td>104</td>\n",
       "      <td>2.660422</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23840</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>5.0</td>\n",
       "      <td>104</td>\n",
       "      <td>2.209311</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13552</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>2.8</td>\n",
       "      <td>104</td>\n",
       "      <td>2.660422</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1346</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>4.0</td>\n",
       "      <td>104</td>\n",
       "      <td>1.869357</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23932</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>103</td>\n",
       "      <td>2.288431</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6154</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102</td>\n",
       "      <td>2.658076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20072</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>4.0</td>\n",
       "      <td>100</td>\n",
       "      <td>2.246146</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7857</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>100</td>\n",
       "      <td>1.756872</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16830</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>2.032834</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7851</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>100</td>\n",
       "      <td>0.722215</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1586</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>2.8</td>\n",
       "      <td>100</td>\n",
       "      <td>1.756872</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11600</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>100</td>\n",
       "      <td>1.289627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18033</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>2.8</td>\n",
       "      <td>100</td>\n",
       "      <td>2.564332</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11333</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>3.0</td>\n",
       "      <td>100</td>\n",
       "      <td>2.494546</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8387</th>\n",
       "      <td>2013.2493</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>100</td>\n",
       "      <td>1.294421</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 10401 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       FractionDate  GoldsteinScale  NumMentions   AvgTone  Actor1Code_AFG  \\\n",
       "3196      2013.2493             0.0          643  3.227707             0.0   \n",
       "18436     2013.2493             3.0          384  4.111273             0.0   \n",
       "12787     2013.2493            -4.0          372  3.989697             0.0   \n",
       "6969      2013.2493             3.0          299  1.849065             0.0   \n",
       "2468      2013.2493            -7.2          290  1.656668             0.0   \n",
       "6971      2013.2493            -0.3          280  1.674708             0.0   \n",
       "24163     2013.2493           -10.0          275  1.254327             0.0   \n",
       "1489      2013.2493           -10.0          270  3.960111             0.0   \n",
       "15388     2013.2493             0.0          256  1.609864             0.0   \n",
       "6882      2013.2493           -10.0          246  3.980200             0.0   \n",
       "25982     2013.2493           -10.0          185  1.270816             0.0   \n",
       "21765     2013.2493             7.0          180  2.596207             0.0   \n",
       "15966     2013.2493            -5.0          179  1.054735             0.0   \n",
       "21761     2013.2493             3.4          168  2.717686             0.0   \n",
       "25328     2013.2493             8.3          165  1.419494             0.0   \n",
       "21779     2013.2493            -2.0          164  2.562969             0.0   \n",
       "15610     2013.2493           -10.0          162  2.194546             0.0   \n",
       "17320     2013.2493             0.0          160  2.020098             0.0   \n",
       "20178     2013.2493             2.8          160  2.229886             0.0   \n",
       "15178     2013.2493            -2.0          160  2.295452             0.0   \n",
       "19463     2013.2493             7.0          160  2.565543             0.0   \n",
       "18370     2013.2493           -10.0          152  1.082084             0.0   \n",
       "8507      2013.2493           -10.0          152  1.233950             0.0   \n",
       "21792     2013.2493            -2.0          150  2.516193             0.0   \n",
       "2880      2013.2493             4.0          150  2.405127             0.0   \n",
       "20077     2013.2493             4.0          150  2.405127             0.0   \n",
       "20886     2013.2493             3.4          150  2.516193             0.0   \n",
       "24515     2013.2493             8.3          142  1.425969             0.0   \n",
       "8077      2013.2493             1.9          141  2.145737             0.0   \n",
       "15428     2013.2493             0.0          141  2.254096             0.0   \n",
       "...             ...             ...          ...       ...             ...   \n",
       "16269     2013.2493             7.0          110  2.245010             0.0   \n",
       "11312     2013.2493             3.0          110  2.303969             0.0   \n",
       "13184     2013.2493            -5.0          110  2.255151             0.0   \n",
       "7157      2013.2493             7.0          107  2.558647             0.0   \n",
       "5666      2013.2493             3.0          106  4.027624             0.0   \n",
       "4232      2013.2493             1.9          106  0.844023             1.0   \n",
       "24251     2013.2493             1.9          106  0.844023             0.0   \n",
       "24260     2013.2493             2.8          106  0.844023             0.0   \n",
       "11398     2013.2493             4.0          106  1.212151             0.0   \n",
       "4242      2013.2493             2.8          106  0.844023             1.0   \n",
       "5663      2013.2493             3.0          105  3.280937             0.0   \n",
       "7142      2013.2493             6.4          104  2.660422             0.0   \n",
       "7169      2013.2493             1.9          104  2.660422             0.0   \n",
       "7173      2013.2493             6.4          104  2.660422             0.0   \n",
       "18825     2013.2493             0.0          104  4.015640             0.0   \n",
       "19574     2013.2493            -6.5          104  2.660422             0.0   \n",
       "23840     2013.2493             5.0          104  2.209311             0.0   \n",
       "13552     2013.2493             2.8          104  2.660422             0.0   \n",
       "1346      2013.2493             4.0          104  1.869357             0.0   \n",
       "23932     2013.2493            -2.0          103  2.288431             0.0   \n",
       "6154      2013.2493             0.0          102  2.658076             0.0   \n",
       "20072     2013.2493             4.0          100  2.246146             0.0   \n",
       "7857      2013.2493           -10.0          100  1.756872             0.0   \n",
       "16830     2013.2493             0.0          100  2.032834             0.0   \n",
       "7851      2013.2493           -10.0          100  0.722215             0.0   \n",
       "1586      2013.2493             2.8          100  1.756872             0.0   \n",
       "11600     2013.2493            -2.0          100  1.289627             0.0   \n",
       "18033     2013.2493             2.8          100  2.564332             0.0   \n",
       "11333     2013.2493             3.0          100  2.494546             0.0   \n",
       "8387      2013.2493            -9.0          100  1.294421             0.0   \n",
       "\n",
       "       Actor1Code_AFGBUS  Actor1Code_AFGCOP  Actor1Code_AFGCVL  \\\n",
       "3196                 0.0                0.0                0.0   \n",
       "18436                0.0                0.0                0.0   \n",
       "12787                0.0                0.0                0.0   \n",
       "6969                 0.0                0.0                0.0   \n",
       "2468                 0.0                0.0                0.0   \n",
       "6971                 0.0                0.0                0.0   \n",
       "24163                0.0                0.0                0.0   \n",
       "1489                 0.0                0.0                0.0   \n",
       "15388                0.0                0.0                0.0   \n",
       "6882                 0.0                0.0                0.0   \n",
       "25982                0.0                0.0                0.0   \n",
       "21765                0.0                0.0                0.0   \n",
       "15966                0.0                0.0                0.0   \n",
       "21761                0.0                0.0                0.0   \n",
       "25328                0.0                0.0                0.0   \n",
       "21779                0.0                0.0                0.0   \n",
       "15610                0.0                0.0                0.0   \n",
       "17320                0.0                0.0                0.0   \n",
       "20178                0.0                0.0                0.0   \n",
       "15178                0.0                0.0                0.0   \n",
       "19463                0.0                0.0                0.0   \n",
       "18370                0.0                0.0                0.0   \n",
       "8507                 0.0                0.0                0.0   \n",
       "21792                0.0                0.0                0.0   \n",
       "2880                 0.0                0.0                0.0   \n",
       "20077                0.0                0.0                0.0   \n",
       "20886                0.0                0.0                0.0   \n",
       "24515                0.0                0.0                0.0   \n",
       "8077                 0.0                0.0                0.0   \n",
       "15428                0.0                0.0                0.0   \n",
       "...                  ...                ...                ...   \n",
       "16269                0.0                0.0                0.0   \n",
       "11312                0.0                0.0                0.0   \n",
       "13184                0.0                0.0                0.0   \n",
       "7157                 0.0                0.0                0.0   \n",
       "5666                 0.0                0.0                0.0   \n",
       "4232                 0.0                0.0                0.0   \n",
       "24251                0.0                0.0                0.0   \n",
       "24260                0.0                0.0                0.0   \n",
       "11398                0.0                0.0                0.0   \n",
       "4242                 0.0                0.0                0.0   \n",
       "5663                 0.0                0.0                0.0   \n",
       "7142                 0.0                0.0                0.0   \n",
       "7169                 0.0                0.0                0.0   \n",
       "7173                 0.0                0.0                0.0   \n",
       "18825                0.0                0.0                0.0   \n",
       "19574                0.0                0.0                0.0   \n",
       "23840                0.0                0.0                0.0   \n",
       "13552                0.0                0.0                0.0   \n",
       "1346                 0.0                0.0                0.0   \n",
       "23932                0.0                0.0                0.0   \n",
       "6154                 0.0                0.0                0.0   \n",
       "20072                0.0                0.0                0.0   \n",
       "7857                 0.0                0.0                0.0   \n",
       "16830                0.0                0.0                0.0   \n",
       "7851                 0.0                0.0                0.0   \n",
       "1586                 0.0                0.0                0.0   \n",
       "11600                0.0                0.0                0.0   \n",
       "18033                0.0                0.0                0.0   \n",
       "11333                0.0                0.0                0.0   \n",
       "8387                 0.0                0.0                0.0   \n",
       "\n",
       "       Actor1Code_AFGGOV  Actor1Code_AFGGOVEDU    ...     tfidfzealand  \\\n",
       "3196                 0.0                   0.0    ...              0.0   \n",
       "18436                0.0                   0.0    ...              0.0   \n",
       "12787                0.0                   0.0    ...              0.0   \n",
       "6969                 0.0                   0.0    ...              0.0   \n",
       "2468                 0.0                   0.0    ...              0.0   \n",
       "6971                 0.0                   0.0    ...              0.0   \n",
       "24163                0.0                   0.0    ...              0.0   \n",
       "1489                 0.0                   0.0    ...              0.0   \n",
       "15388                0.0                   0.0    ...              0.0   \n",
       "6882                 0.0                   0.0    ...              0.0   \n",
       "25982                0.0                   0.0    ...              0.0   \n",
       "21765                0.0                   0.0    ...              0.0   \n",
       "15966                0.0                   0.0    ...              0.0   \n",
       "21761                0.0                   0.0    ...              0.0   \n",
       "25328                0.0                   0.0    ...              0.0   \n",
       "21779                0.0                   0.0    ...              0.0   \n",
       "15610                0.0                   0.0    ...              0.0   \n",
       "17320                0.0                   0.0    ...              0.0   \n",
       "20178                0.0                   0.0    ...              0.0   \n",
       "15178                0.0                   0.0    ...              0.0   \n",
       "19463                0.0                   0.0    ...              0.0   \n",
       "18370                0.0                   0.0    ...              0.0   \n",
       "8507                 0.0                   0.0    ...              0.0   \n",
       "21792                0.0                   0.0    ...              0.0   \n",
       "2880                 0.0                   0.0    ...              0.0   \n",
       "20077                0.0                   0.0    ...              0.0   \n",
       "20886                0.0                   0.0    ...              0.0   \n",
       "24515                0.0                   0.0    ...              0.0   \n",
       "8077                 0.0                   0.0    ...              0.0   \n",
       "15428                0.0                   0.0    ...              0.0   \n",
       "...                  ...                   ...    ...              ...   \n",
       "16269                0.0                   0.0    ...              0.0   \n",
       "11312                0.0                   0.0    ...              0.0   \n",
       "13184                0.0                   0.0    ...              0.0   \n",
       "7157                 0.0                   0.0    ...              0.0   \n",
       "5666                 0.0                   0.0    ...              0.0   \n",
       "4232                 0.0                   0.0    ...              0.0   \n",
       "24251                0.0                   0.0    ...              0.0   \n",
       "24260                0.0                   0.0    ...              0.0   \n",
       "11398                0.0                   0.0    ...              0.0   \n",
       "4242                 0.0                   0.0    ...              0.0   \n",
       "5663                 0.0                   0.0    ...              0.0   \n",
       "7142                 0.0                   0.0    ...              0.0   \n",
       "7169                 0.0                   0.0    ...              0.0   \n",
       "7173                 0.0                   0.0    ...              0.0   \n",
       "18825                0.0                   0.0    ...              0.0   \n",
       "19574                0.0                   0.0    ...              0.0   \n",
       "23840                0.0                   0.0    ...              0.0   \n",
       "13552                0.0                   0.0    ...              0.0   \n",
       "1346                 0.0                   0.0    ...              0.0   \n",
       "23932                0.0                   0.0    ...              0.0   \n",
       "6154                 0.0                   0.0    ...              0.0   \n",
       "20072                0.0                   0.0    ...              0.0   \n",
       "7857                 0.0                   0.0    ...              0.0   \n",
       "16830                0.0                   0.0    ...              0.0   \n",
       "7851                 0.0                   0.0    ...              0.0   \n",
       "1586                 0.0                   0.0    ...              0.0   \n",
       "11600                0.0                   0.0    ...              0.0   \n",
       "18033                0.0                   0.0    ...              0.0   \n",
       "11333                0.0                   0.0    ...              0.0   \n",
       "8387                 0.0                   0.0    ...              0.0   \n",
       "\n",
       "       tfidfzealotri  tfidfzeidan  tfidfzelda  tfidfzhiggkoea  \\\n",
       "3196             0.0          0.0         0.0             0.0   \n",
       "18436            0.0          0.0         0.0             0.0   \n",
       "12787            0.0          0.0         0.0             0.0   \n",
       "6969             0.0          0.0         0.0             0.0   \n",
       "2468             0.0          0.0         0.0             0.0   \n",
       "6971             0.0          0.0         0.0             0.0   \n",
       "24163            0.0          0.0         0.0             0.0   \n",
       "1489             0.0          0.0         0.0             0.0   \n",
       "15388            0.0          0.0         0.0             0.0   \n",
       "6882             0.0          0.0         0.0             0.0   \n",
       "25982            0.0          0.0         0.0             0.0   \n",
       "21765            0.0          0.0         0.0             0.0   \n",
       "15966            0.0          0.0         0.0             0.0   \n",
       "21761            0.0          0.0         0.0             0.0   \n",
       "25328            0.0          0.0         0.0             0.0   \n",
       "21779            0.0          0.0         0.0             0.0   \n",
       "15610            0.0          0.0         0.0             0.0   \n",
       "17320            0.0          0.0         0.0             0.0   \n",
       "20178            0.0          0.0         0.0             0.0   \n",
       "15178            0.0          0.0         0.0             0.0   \n",
       "19463            0.0          0.0         0.0             0.0   \n",
       "18370            0.0          0.0         0.0             0.0   \n",
       "8507             0.0          0.0         0.0             0.0   \n",
       "21792            0.0          0.0         0.0             0.0   \n",
       "2880             0.0          0.0         0.0             0.0   \n",
       "20077            0.0          0.0         0.0             0.0   \n",
       "20886            0.0          0.0         0.0             0.0   \n",
       "24515            0.0          0.0         0.0             0.0   \n",
       "8077             0.0          0.0         0.0             0.0   \n",
       "15428            0.0          0.0         0.0             0.0   \n",
       "...              ...          ...         ...             ...   \n",
       "16269            0.0          0.0         0.0             0.0   \n",
       "11312            0.0          0.0         0.0             0.0   \n",
       "13184            0.0          0.0         0.0             0.0   \n",
       "7157             0.0          0.0         0.0             0.0   \n",
       "5666             0.0          0.0         0.0             0.0   \n",
       "4232             0.0          0.0         0.0             0.0   \n",
       "24251            0.0          0.0         0.0             0.0   \n",
       "24260            0.0          0.0         0.0             0.0   \n",
       "11398            0.0          0.0         0.0             0.0   \n",
       "4242             0.0          0.0         0.0             0.0   \n",
       "5663             0.0          0.0         0.0             0.0   \n",
       "7142             0.0          0.0         0.0             0.0   \n",
       "7169             0.0          0.0         0.0             0.0   \n",
       "7173             0.0          0.0         0.0             0.0   \n",
       "18825            0.0          0.0         0.0             0.0   \n",
       "19574            0.0          0.0         0.0             0.0   \n",
       "23840            0.0          0.0         0.0             0.0   \n",
       "13552            0.0          0.0         0.0             0.0   \n",
       "1346             0.0          0.0         0.0             0.0   \n",
       "23932            0.0          0.0         0.0             0.0   \n",
       "6154             0.0          0.0         0.0             0.0   \n",
       "20072            0.0          0.0         0.0             0.0   \n",
       "7857             0.0          0.0         0.0             0.0   \n",
       "16830            0.0          0.0         0.0             0.0   \n",
       "7851             0.0          0.0         0.0             0.0   \n",
       "1586             0.0          0.0         0.0             0.0   \n",
       "11600            0.0          0.0         0.0             0.0   \n",
       "18033            0.0          0.0         0.0             0.0   \n",
       "11333            0.0          0.0         0.0             0.0   \n",
       "8387             0.0          0.0         0.0             0.0   \n",
       "\n",
       "       tfidfziivhmez_uxlgpnlo  tfidfzikir  tfidfzipwir  \\\n",
       "3196                      0.0         0.0          0.0   \n",
       "18436                     0.0         0.0          0.0   \n",
       "12787                     0.0         0.0          0.0   \n",
       "6969                      0.0         0.0          0.0   \n",
       "2468                      0.0         0.0          0.0   \n",
       "6971                      0.0         0.0          0.0   \n",
       "24163                     0.0         0.0          0.0   \n",
       "1489                      0.0         0.0          0.0   \n",
       "15388                     0.0         0.0          0.0   \n",
       "6882                      0.0         0.0          0.0   \n",
       "25982                     0.0         0.0          0.0   \n",
       "21765                     0.0         0.0          0.0   \n",
       "15966                     0.0         0.0          0.0   \n",
       "21761                     0.0         0.0          0.0   \n",
       "25328                     0.0         0.0          0.0   \n",
       "21779                     0.0         0.0          0.0   \n",
       "15610                     0.0         0.0          0.0   \n",
       "17320                     0.0         0.0          0.0   \n",
       "20178                     0.0         0.0          0.0   \n",
       "15178                     0.0         0.0          0.0   \n",
       "19463                     0.0         0.0          0.0   \n",
       "18370                     0.0         0.0          0.0   \n",
       "8507                      0.0         0.0          0.0   \n",
       "21792                     0.0         0.0          0.0   \n",
       "2880                      0.0         0.0          0.0   \n",
       "20077                     0.0         0.0          0.0   \n",
       "20886                     0.0         0.0          0.0   \n",
       "24515                     0.0         0.0          0.0   \n",
       "8077                      0.0         0.0          0.0   \n",
       "15428                     0.0         0.0          0.0   \n",
       "...                       ...         ...          ...   \n",
       "16269                     0.0         0.0          0.0   \n",
       "11312                     0.0         0.0          0.0   \n",
       "13184                     0.0         0.0          0.0   \n",
       "7157                      0.0         0.0          0.0   \n",
       "5666                      0.0         0.0          0.0   \n",
       "4232                      0.0         0.0          0.0   \n",
       "24251                     0.0         0.0          0.0   \n",
       "24260                     0.0         0.0          0.0   \n",
       "11398                     0.0         0.0          0.0   \n",
       "4242                      0.0         0.0          0.0   \n",
       "5663                      0.0         0.0          0.0   \n",
       "7142                      0.0         0.0          0.0   \n",
       "7169                      0.0         0.0          0.0   \n",
       "7173                      0.0         0.0          0.0   \n",
       "18825                     0.0         0.0          0.0   \n",
       "19574                     0.0         0.0          0.0   \n",
       "23840                     0.0         0.0          0.0   \n",
       "13552                     0.0         0.0          0.0   \n",
       "1346                      0.0         0.0          0.0   \n",
       "23932                     0.0         0.0          0.0   \n",
       "6154                      0.0         0.0          0.0   \n",
       "20072                     0.0         0.0          0.0   \n",
       "7857                      0.0         0.0          0.0   \n",
       "16830                     0.0         0.0          0.0   \n",
       "7851                      0.0         0.0          0.0   \n",
       "1586                      0.0         0.0          0.0   \n",
       "11600                     0.0         0.0          0.0   \n",
       "18033                     0.0         0.0          0.0   \n",
       "11333                     0.0         0.0          0.0   \n",
       "8387                      0.0         0.0          0.0   \n",
       "\n",
       "       tfidfzmnmbcosjccynudfnuig  tfidfzoo  \n",
       "3196                         0.0       0.0  \n",
       "18436                        0.0       0.0  \n",
       "12787                        0.0       0.0  \n",
       "6969                         0.0       0.0  \n",
       "2468                         0.0       0.0  \n",
       "6971                         0.0       0.0  \n",
       "24163                        0.0       0.0  \n",
       "1489                         0.0       0.0  \n",
       "15388                        0.0       0.0  \n",
       "6882                         0.0       0.0  \n",
       "25982                        0.0       0.0  \n",
       "21765                        0.0       0.0  \n",
       "15966                        0.0       0.0  \n",
       "21761                        0.0       0.0  \n",
       "25328                        0.0       0.0  \n",
       "21779                        0.0       0.0  \n",
       "15610                        0.0       0.0  \n",
       "17320                        0.0       0.0  \n",
       "20178                        0.0       0.0  \n",
       "15178                        0.0       0.0  \n",
       "19463                        0.0       0.0  \n",
       "18370                        0.0       0.0  \n",
       "8507                         0.0       0.0  \n",
       "21792                        0.0       0.0  \n",
       "2880                         0.0       0.0  \n",
       "20077                        0.0       0.0  \n",
       "20886                        0.0       0.0  \n",
       "24515                        0.0       0.0  \n",
       "8077                         0.0       0.0  \n",
       "15428                        0.0       0.0  \n",
       "...                          ...       ...  \n",
       "16269                        0.0       0.0  \n",
       "11312                        0.0       0.0  \n",
       "13184                        0.0       0.0  \n",
       "7157                         0.0       0.0  \n",
       "5666                         0.0       0.0  \n",
       "4232                         0.0       0.0  \n",
       "24251                        0.0       0.0  \n",
       "24260                        0.0       0.0  \n",
       "11398                        0.0       0.0  \n",
       "4242                         0.0       0.0  \n",
       "5663                         0.0       0.0  \n",
       "7142                         0.0       0.0  \n",
       "7169                         0.0       0.0  \n",
       "7173                         0.0       0.0  \n",
       "18825                        0.0       0.0  \n",
       "19574                        0.0       0.0  \n",
       "23840                        0.0       0.0  \n",
       "13552                        0.0       0.0  \n",
       "1346                         0.0       0.0  \n",
       "23932                        0.0       0.0  \n",
       "6154                         0.0       0.0  \n",
       "20072                        0.0       0.0  \n",
       "7857                         0.0       0.0  \n",
       "16830                        0.0       0.0  \n",
       "7851                         0.0       0.0  \n",
       "1586                         0.0       0.0  \n",
       "11600                        0.0       0.0  \n",
       "18033                        0.0       0.0  \n",
       "11333                        0.0       0.0  \n",
       "8387                         0.0       0.0  \n",
       "\n",
       "[100 rows x 10401 columns]"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feattfidfRED_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'Adj Close'), (961, '1570.25')]"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1570.25"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(sp500[961][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1553.689941"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(sp500[960][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "australia\n",
      "peopl\n",
      "smuggl\n",
      "rise\n"
     ]
    }
   ],
   "source": [
    "my_tokenizer(df.iloc[3,-1]):\n",
    "    print(porter.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FractionDate</th>\n",
       "      <th>Actor1Code</th>\n",
       "      <th>Actor1Name</th>\n",
       "      <th>Actor1CountryCode</th>\n",
       "      <th>Actor1Type1Code</th>\n",
       "      <th>Actor2Code</th>\n",
       "      <th>Actor2Name</th>\n",
       "      <th>Actor2CountryCode</th>\n",
       "      <th>Actor2Type1Code</th>\n",
       "      <th>EventCode</th>\n",
       "      <th>QuadClass</th>\n",
       "      <th>GoldsteinScale</th>\n",
       "      <th>NumMentions</th>\n",
       "      <th>AvgTone</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003.2575</td>\n",
       "      <td>AUS</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>AUS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CVL</td>\n",
       "      <td>MIGRANT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CVL</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>2.8</td>\n",
       "      <td>10</td>\n",
       "      <td>2.222222</td>\n",
       "      <td>[australia, people, smuggling, rising]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003.2575</td>\n",
       "      <td>BUS</td>\n",
       "      <td>SHOP OWNER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BUS</td>\n",
       "      <td>CVL</td>\n",
       "      <td>NEIGHBORHOOD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CVL</td>\n",
       "      <td>172</td>\n",
       "      <td>4</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>8</td>\n",
       "      <td>2.167369</td>\n",
       "      <td>[hong, kong, businesses, vanish, rents, soar, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003.2575</td>\n",
       "      <td>BUS</td>\n",
       "      <td>SHOP OWNER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BUS</td>\n",
       "      <td>CVL</td>\n",
       "      <td>NEIGHBORHOOD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CVL</td>\n",
       "      <td>172</td>\n",
       "      <td>4</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.167369</td>\n",
       "      <td>[hong, kong, businesses, vanish, rents, soar, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003.2575</td>\n",
       "      <td>CVL</td>\n",
       "      <td>MIGRANT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CVL</td>\n",
       "      <td>AUS</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>AUS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>10</td>\n",
       "      <td>2.222222</td>\n",
       "      <td>[australia, people, smuggling, rising]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2012.2493</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BUS</td>\n",
       "      <td>COMPANY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BUS</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10</td>\n",
       "      <td>3.521127</td>\n",
       "      <td>[pakistans, ambitious, program, educate, milit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    FractionDate Actor1Code  Actor1Name Actor1CountryCode Actor1Type1Code  \\\n",
       "0      2003.2575        AUS   AUSTRALIA               AUS             NaN   \n",
       "1      2003.2575        BUS  SHOP OWNER               NaN             BUS   \n",
       "2      2003.2575        BUS  SHOP OWNER               NaN             BUS   \n",
       "3      2003.2575        CVL     MIGRANT               NaN             CVL   \n",
       "10     2012.2493        NaN         NaN               NaN             NaN   \n",
       "\n",
       "   Actor2Code    Actor2Name Actor2CountryCode Actor2Type1Code  EventCode  \\\n",
       "0         CVL       MIGRANT               NaN             CVL         43   \n",
       "1         CVL  NEIGHBORHOOD               NaN             CVL        172   \n",
       "2         CVL  NEIGHBORHOOD               NaN             CVL        172   \n",
       "3         AUS     AUSTRALIA               AUS             NaN         42   \n",
       "10        BUS       COMPANY               NaN             BUS         20   \n",
       "\n",
       "    QuadClass  GoldsteinScale  NumMentions   AvgTone  \\\n",
       "0           1             2.8           10  2.222222   \n",
       "1           4            -5.0            8  2.167369   \n",
       "2           4            -5.0            2  2.167369   \n",
       "3           1             1.9           10  2.222222   \n",
       "10          1             3.0           10  3.521127   \n",
       "\n",
       "                                                  url  \n",
       "0              [australia, people, smuggling, rising]  \n",
       "1   [hong, kong, businesses, vanish, rents, soar, ...  \n",
       "2   [hong, kong, businesses, vanish, rents, soar, ...  \n",
       "3              [australia, people, smuggling, rising]  \n",
       "10  [pakistans, ambitious, program, educate, milit...  "
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=re.split(r'\".\"|/',df.iloc[i,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http:',\n",
       " '',\n",
       " 'www.channelnewsasia.com',\n",
       " 'news',\n",
       " 'world',\n",
       " 'us-urges-serbia-kosovo-to-reach-agreemen',\n",
       " '624136.html']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FractionDate</th>\n",
       "      <th>Actor1Code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003.2575</td>\n",
       "      <td>BUS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003.2575</td>\n",
       "      <td>CVL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2003.2575</td>\n",
       "      <td>HLH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   FractionDate Actor1Code\n",
       "1     2003.2575        BUS\n",
       "3     2003.2575        CVL\n",
       "4     2003.2575        HLH"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[[1,3,4],['FractionDate','Actor1Code']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [2, 3]\n",
       "1       [1]\n",
       "2    [1, 2]\n",
       "dtype: object"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([[2,3],[1],[1,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ddf['url']=my_ser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-206-71c44b3bbeed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0men\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0men\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m//anaconda/envs/py3k/lib/python3.5/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1842\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1843\u001b[0m         \"\"\"\n\u001b[0;32m-> 1844\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1845\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py3k/lib/python3.5/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1900\u001b[0m         \"\"\"\n\u001b[1;32m   1901\u001b[0m         return _transform_selected(X, self._fit_transform,\n\u001b[0;32m-> 1902\u001b[0;31m                                    self.categorical_features, copy=True)\n\u001b[0m\u001b[1;32m   1903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1904\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py3k/lib/python3.5/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36m_transform_selected\u001b[0;34m(X, transform, selected, copy)\u001b[0m\n\u001b[1;32m   1695\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msparse\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1696\u001b[0m     \"\"\"\n\u001b[0;32m-> 1697\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1699\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mselected\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"all\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py3k/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    405\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py3k/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     56\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     57\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 58\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "en=OneHotEncoder()\n",
    "en.fit([[0,1],[3,np.nan]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  1.,  0.]])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en.transform([[2,0]]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mask=[]\n",
    "ser=[]\n",
    "for i in range(len(df)):\n",
    "    url=df.iloc[i,-1]\n",
    "    #print(url)\n",
    "    c=[]\n",
    "    d=[]\n",
    "    if url!='BBC Monitoring':\n",
    "        #print(str(i)+'=========')\n",
    "        a=urlparse(url)[2].split('.')[0].split('/')[-1]\n",
    "        b = re_tokenizer.tokenize(a.lower())\n",
    "        for word in b:\n",
    "            c+=[punctuation.sub(\"\", word)]\n",
    "        for word in c:\n",
    "            if word not in stop_words:\n",
    "                d+=[word]\n",
    "        if len(d)>1:\n",
    "            mask+=[i]\n",
    "            ser+=[d]\n",
    "            #print(d)\n",
    "#print(mask)\n",
    "my_ser=pd.Series(ser)\n",
    "#print(my_ser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "com/breakingnews/343522/australia-people-smuggling-rising\n",
      "['bangkokpost', 'com/breakingnews/343522/australia-people-smuggling-rising']\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    stro=df.iloc[i,-1].split('.')[-2:]\n",
    "    stri=\"\"\n",
    "    if len(stro)==2:\n",
    "        if len(stro[0]) > len(stro[1]):\n",
    "            stri=stro[0]\n",
    "        elif len(stro[0])<len(stro[1]):\n",
    "            stri=stro[1]\n",
    "        else:\n",
    "            stri=\"**\"+stro[0]+stro[1]\n",
    "    print(stri)\n",
    "    print(stro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2b2f5692c284>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'all'"
     ]
    }
   ],
   "source": [
    "isinstance([3,2].all,int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gde=nlpp.CorpusGDELT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gde.url_tokenizer('www.fakesite.net/hey-dude/poo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParseResult(scheme='http', netloc='www.globalsecurity.org', path='/breakingnews/343522/australia-people-smuggling-rising', params='', query='', fragment='')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urlparse('http://www.globalsecurity.org/breakingnews/343522/australia-people-smuggling-rising')#[2]#.split('.')[-1].split('/')#[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading news for 20130401\n"
     ]
    }
   ],
   "source": [
    "gde.load_urls('20130401','20130401')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2894736842105263"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "11/38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7105263157894737"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "27/38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py3k]",
   "language": "python",
   "name": "Python [py3k]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
