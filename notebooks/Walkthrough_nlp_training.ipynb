{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Predicting financial indicators is definitely a holy grail for our society at its present stage. There is a vast literature on how to do this and the general approach is a time-series one, that is, predict the future of one quantity based on that quantity's past.\n",
    "\n",
    "We are trying to see if it's possible to complement this approach with data coming from news sources, reasoning that news from the world should directly and indirectly weigh on the performance of such indicators as stocks, employment rate, or inflation.\n",
    "\n",
    "Please keep in mind that we do not expect to make any significant improvement over state-of-the-art financial analyses (which involve much more complex and refined models). Rather, we are interested in building a scalable and dynamic pipeline that in the future might supplement those already-existing models or give interesting insights.\n",
    "\n",
    "### This notebook\n",
    "\n",
    "This is a walkthrough illustrating the typical usage of our package. We will try to predict future S&P500 closing values based on past S&P500 values along with NLP features extracted from the daily-updted GDELT 1.0 (http://www.gdeltproject.org/) event database.\n",
    "\n",
    "In particular, to scope down the analysis to a minimally viable scalable pipeline, I extract features from the source urls contained in the database (one associated to each event).\n",
    "\n",
    "For each day, all urls get parsed, tokenized, and stemmed, and then conflated together into a single bag of words. This will constitute one document. After that I may apply a tf-idf or word2vec vectorization (this latter being much favored).\n",
    "\n",
    "I use the extracted features (plus the same day's closing S&P500) to try and fit various regression models to predict the next day's S&P500 and compare them to a benchmark model (a simple naive model predicting the same for tomorrow as today, plus the average increase or decrease over the last few days).\n",
    "\n",
    "I also try to predict if tomorrow's index value will rise or fall, given today's news.\n",
    "\n",
    "For both tasks random forest regressors/classifiers seem promising approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import sys\n",
    "import os\n",
    "sourcedir=os.getcwd()+\"/../source\"\n",
    "if sourcedir not in sys.path:\n",
    "    sys.path.append(sourcedir)\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'model_training' from '/Users/Maxos/Desktop/Insight_stuff/bigsnippyrepo/maqro/notebooks/../source/model_training.py'>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing our nlp proprocessing module, the reload command is for development\n",
    "import nlp_preprocessing as nlpp\n",
    "importlib.reload(nlpp)\n",
    "#importing our model training module, the reload command is for development\n",
    "import model_training as mdlt\n",
    "importlib.reload(mdlt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The nlp-preprocessing module\n",
    "\n",
    "The module has two classes for now: one deals with the nlp preprocessing of Google News articles, which are talked about in much more depth in another notebook; the other is the analog for GDELT url data.\n",
    "\n",
    "Let's explore these classes and their contents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The CorpusGoogleNews class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#del datagnews\n",
    "datagnews=nlpp.CorpusGoogleNews() # nlpp.CorpusGoogleNews('some/data/directory') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the attributes of the initialized class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datagnews.raw_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datagnews.datadirectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one public method for now: it loads files from the data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Inc\n",
      "Apple Inc 1-26-17\n",
      "Apple Inc 1-27-17\n",
      "Apple Inc 1-30-17\n",
      "Apple Inc 1-31-17\n",
      "Apple Inc 2-1-17\n"
     ]
    }
   ],
   "source": [
    "datagnews.data_directory_crawl('AAPL',verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which populates datagnews.raw_articles with dataframes like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The first day of public trading with President...</td>\n",
       "      <td>Apple Inc</td>\n",
       "      <td>3 Stocks to Watch on Tuesday: Apple Inc. (AAPL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The first day of public trading with President...</td>\n",
       "      <td>Apple Inc</td>\n",
       "      <td>3 Stocks to Watch on Tuesday: Apple Inc. (AAPL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The smart home market continues to heat up, an...</td>\n",
       "      <td>Apple Inc</td>\n",
       "      <td>Alphabet Inc (GOOGL) Steals AI Expert Back Fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Reportedly, Apple Inc.’s AAPL management is sc...</td>\n",
       "      <td>Apple Inc</td>\n",
       "      <td>Apple (AAPL) Set to Meet Government Officials ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Apple Inc. (AAPL) executives were in India tod...</td>\n",
       "      <td>Apple Inc</td>\n",
       "      <td>Apple Close to Signing Deal With Indian Govern...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body   category  \\\n",
       "0  The first day of public trading with President...  Apple Inc   \n",
       "1  The first day of public trading with President...  Apple Inc   \n",
       "2  The smart home market continues to heat up, an...  Apple Inc   \n",
       "3  Reportedly, Apple Inc.’s AAPL management is sc...  Apple Inc   \n",
       "4  Apple Inc. (AAPL) executives were in India tod...  Apple Inc   \n",
       "\n",
       "                                               title  \n",
       "0  3 Stocks to Watch on Tuesday: Apple Inc. (AAPL...  \n",
       "1  3 Stocks to Watch on Tuesday: Apple Inc. (AAPL...  \n",
       "2  Alphabet Inc (GOOGL) Steals AI Expert Back Fro...  \n",
       "3  Apple (AAPL) Set to Meet Government Officials ...  \n",
       "4  Apple Close to Signing Deal With Indian Govern...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datagnews.raw_articles['Apple Inc 1-30-17'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The CorpusGDELT class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#del datagdelt\n",
    "datagdelt=nlpp.CorpusGDELT(min_ment=500) # min_ment defaults to 1 and cuts off events that have a low number of mentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the several attributes that the class contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum number of mentions: 500\n",
      "Current directory: ../data/GDELT_1.0/\n",
      "Dates loaded so far: []\n",
      "Corpus of raw urls []\n",
      "Corpus of tfidf-vectorized docs:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#minimum number of mentions for one event to be used\n",
    "print('Minimum number of mentions:',datagdelt.minimum_ment)\n",
    "print('Current directory:',datagdelt.currentdir) # current directory\n",
    "print('Dates loaded so far:',datagdelt.dates) # dates for which data has been loaded so far\n",
    "print('Corpus of raw urls',datagdelt.url_corpus)\n",
    "print('Corpus of tfidf-vectorized docs:')\n",
    "print(datagdelt.vect_corpus_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vowels: ['a', 'e', 'i', 'o', 'u', 'y']\n",
      "Consonants: ['b', 'c', 'd', 'f', 'g', 'h', 'j', 'k', 'l', 'm', 'n', 'p', 'q', 'r', 's', 't', 'v', 'w', 'x', 'z'] \n",
      "Stemmer: <PorterStemmer>\n",
      "Punctuation: re.compile('[-.?!,\":;()|0-9]')\n",
      "Tokenizer: RegexpTokenizer(pattern='\\\\w+', gaps=False, discard_empty=True, flags=56)\n",
      "Filter for spurious url beginnings: re.compile('idind.|idus.|iduk.')\n",
      "Filter for stop words: {'', 'had', 'what', 'these', 'why', 'y', 'over', 'shouldn', 'is', 'himself', 'him', 'won', 'll', 'itself', 'should', 'now', 'herself', 'am', 'too', 'while', 'ourselves', 'here', 'that', 'there', 'some', 'mustn', 'being', 'yourself', 'been', 'by', 'themselves', 'how', 'weren', 'nor', 'each', 'aren', 'between', 'isn', 'yourselves', 'me', 'i', 'once', 'doesn', 'a', 'with', 'your', 'does', 'up', 'or', 'has', 're', 'whom', 'couldn', 'you', 'yours', 'mightn', 'and', 'myself', 'd', 'from', 'more', 'through', 'again', 'if', 'then', 'its', 'other', 'ma', 'for', 'but', 'so', 'hadn', 'do', 'such', 'only', 'when', 'did', 'own', 'o', 'above', 'all', 'be', 'to', 'she', 'of', 'our', 'my', 'further', 's', 'no', 'during', 'wasn', 'as', 'where', 'hers', 'those', 'any', 'same', 'was', 'will', 'have', 'in', 'against', 'after', 'doing', 'very', 'having', 'her', 'both', 'theirs', 'their', 'out', 'shan', 'ain', 'who', 'them', 'didn', 'don', 'wouldn', 've', 'not', 'before', 'they', 'm', 'few', 'can', 'because', 'are', 'until', 'about', 'were', 'into', 'he', 'which', 'off', 'haven', 'this', 'hasn', 'ours', 'just', 'needn', 'it', 'an', 'most', 'we', 'on', 'his', 'under', 'below', 'down', 'than', 't', 'at', 'the'}\n"
     ]
    }
   ],
   "source": [
    "#vowels and consonants\n",
    "print('Vowels:',datagdelt.vowels)\n",
    "print('Consonants:',datagdelt.consonants,end=' ')\n",
    "print()\n",
    "print('Stemmer:',datagdelt.porter) #stemmer of choice\n",
    "print('Punctuation:',datagdelt.punctuation) #punctuation regular expression\n",
    "print('Tokenizer:',datagdelt.re_tokenizer) \n",
    "print('Filter for spurious url beginnings:',datagdelt.spurious_beginnings)\n",
    "print('Filter for stop words:',datagdelt.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GLOBALEVENTID', 'SQLDATE', 'MonthYear', 'Year', 'FractionDate', 'Actor1Code', 'Actor1Name', 'Actor1CountryCode', 'Actor1KnownGroupCode', 'Actor1EthnicCode', 'Actor1Religion1Code', 'Actor1Religion2Code', 'Actor1Type1Code', 'Actor1Type2Code', 'Actor1Type3Code', 'Actor2Code', 'Actor2Name', 'Actor2CountryCode', 'Actor2KnownGroupCode', 'Actor2EthnicCode', 'Actor2Religion1Code', 'Actor2Religion2Code', 'Actor2Type1Code', 'Actor2Type2Code', 'Actor2Type3Code', 'IsRootEvent', 'EventCode', 'EventBaseCode', 'EventRootCode', 'QuadClass', 'GoldsteinScale', 'NumMentions', 'NumSources', 'NumArticles', 'AvgTone', 'Actor1Geo_Type', 'Actor1Geo_FullName', 'Actor1Geo_CountryCode', 'Actor1Geo_ADM1Code', 'Actor1Geo_Lat', 'Actor1Geo_Long', 'Actor1Geo_FeatureID', 'Actor2Geo_Type', 'Actor2Geo_FullName', 'Actor2Geo_CountryCode', 'Actor2Geo_ADM1Code', 'Actor2Geo_Lat', 'Actor2Geo_Long', 'Actor2Geo_FeatureID', 'ActionGeo_Type', 'ActionGeo_FullName', 'ActionGeo_CountryCode', 'ActionGeo_ADM1Code', 'ActionGeo_Lat', 'ActionGeo_Long', 'ActionGeo_FeatureID', 'DATEADDED', 'SOURCEURL'] "
     ]
    }
   ],
   "source": [
    "print(datagdelt.header,end=' ') #GDELT csv files header, notice the last field has the urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what methods are available and what the pipeline is like.\n",
    "\n",
    "First we load the urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Done!"
     ]
    }
   ],
   "source": [
    "datagdelt.load_urls('20161001','20170219') #the earliest available date is April 1st 2013 = 20130401"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at what the url_corpus attribute looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 140 elements in it, because we loaded 140 days!\n",
      "The loaded day n. 5 had 653 events in it that were mentioned more than 500 times:\n",
      " [[1972, 'http://www.philippinetimes.com/index.php/sid/248243461'], [1115, 'http://www.capradio.org/news/npr/story?storyid=496552413'], [970, 'http://thecabin.net/news/2016-10-04/dazzle-daze-raffle-tickets-sale'], [660, 'http://www.princegeorgecitizen.com/celebrity-chef-jamie-oliver-hopes-to-discuss-child-health-issues-with-trudeau-1.2358050'], [748, 'http://1045snx.iheart.com/articles/trending-104650/spencer-pratt-mocks-kim-kardashian-robbery-15169059/'], [746, 'https://in.news.yahoo.com/may-woo-labour-voters-pitch-britains-center-ground-230645420.html'], [754, 'http://www.stuff.co.nz/entertainment/84945059/Veteran-broadcaster-Mark-Sainsbury-takes-Rocky-Horror-Stage-in-Hamilton'], [1951, 'http://www.whio.com/news/national-govt--politics/clinton-reaches-out-women-while-trump-defends-taxes/xnmN5QugmLzeGkEBR64y9I/'], [1965, 'http://wgno.com/2016/10/04/this-robot-is-so-realistic-that-it-helps-train-first-responders/'], [1012, 'https://www.stgeorgeutah.com/news/archive/2016/10/04/bureau-of-indian-affairs-discussion-kicks-off-free-brown-bag-lecture-series-full-schedule/']] \n",
      " etc...\n",
      "The first event was mentioned 1972 times, the second 1115 times, etc...\n"
     ]
    }
   ],
   "source": [
    "day=5 #select one day\n",
    "print('There are',len(datagdelt.url_corpus),'elements in it, because we loaded',len(datagdelt.dates),'days!')\n",
    "print('The loaded day n.',day,'had',len(datagdelt.url_corpus[day-1]) ,'events in it that were mentioned more than',datagdelt.minimum_ment,'times:\\n', datagdelt.url_corpus[day-1][:10],'\\n etc...')\n",
    "print('The first event was mentioned',datagdelt.url_corpus[day-1][0][0],'times, the second',datagdelt.url_corpus[day-1][1][0],'times, etc...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that many of those urls contain wordings that can be very informative on what's happening in the world and therefore might tell us something about the near future of the markets!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's process these messy raw urls! Let's use word2vec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using word2vec vectorization procedure\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-20 05:02:52,161 : INFO : collecting all words and their counts\n",
      "2017-02-20 05:02:52,162 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-02-20 05:02:52,260 : INFO : collected 20461 word types from a corpus of 487933 raw words and 142 sentences\n",
      "2017-02-20 05:02:52,260 : INFO : Loading a fresh vocabulary\n",
      "2017-02-20 05:02:52,332 : INFO : min_count=1 retains 20461 unique words (100% of original 20461, drops 0)\n",
      "2017-02-20 05:02:52,333 : INFO : min_count=1 leaves 487933 word corpus (100% of original 487933, drops 0)\n",
      "2017-02-20 05:02:52,429 : INFO : deleting the raw counts dictionary of 20461 items\n",
      "2017-02-20 05:02:52,430 : INFO : sample=0.001 downsamples 24 most-common words\n",
      "2017-02-20 05:02:52,431 : INFO : downsampling leaves estimated 463332 word corpus (95.0% of prior 487933)\n",
      "2017-02-20 05:02:52,432 : INFO : estimated required memory for 20461 words and 8 dimensions: 11540004 bytes\n",
      "2017-02-20 05:02:52,511 : INFO : resetting layer weights\n",
      "2017-02-20 05:02:52,924 : INFO : training model with 3 workers on 20461 vocabulary and 8 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-02-20 05:02:52,924 : INFO : expecting 142 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-02-20 05:02:53,927 : INFO : PROGRESS: at 77.89% examples, 1795510 words/s, in_qsize 4, out_qsize 1\n",
      "2017-02-20 05:02:54,209 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-02-20 05:02:54,215 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-02-20 05:02:54,217 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-02-20 05:02:54,217 : INFO : training on 2439665 raw words (2316759 effective words) took 1.3s, 1794412 effective words/s\n"
     ]
    }
   ],
   "source": [
    "datagdelt.gdelt_preprocess(vectrz='word2vec',size_w2v=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which gives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w2v_1</th>\n",
       "      <th>w2v_2</th>\n",
       "      <th>w2v_3</th>\n",
       "      <th>w2v_4</th>\n",
       "      <th>w2v_5</th>\n",
       "      <th>w2v_6</th>\n",
       "      <th>w2v_7</th>\n",
       "      <th>w2v_8</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news_date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20161001</th>\n",
       "      <td>-0.178140</td>\n",
       "      <td>-0.295545</td>\n",
       "      <td>0.241965</td>\n",
       "      <td>-0.507727</td>\n",
       "      <td>-0.646261</td>\n",
       "      <td>0.367630</td>\n",
       "      <td>0.067822</td>\n",
       "      <td>0.084742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20161002</th>\n",
       "      <td>-0.189584</td>\n",
       "      <td>-0.286524</td>\n",
       "      <td>0.220864</td>\n",
       "      <td>-0.534220</td>\n",
       "      <td>-0.642891</td>\n",
       "      <td>0.348691</td>\n",
       "      <td>-0.060732</td>\n",
       "      <td>0.095956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20161003</th>\n",
       "      <td>-0.209694</td>\n",
       "      <td>-0.279247</td>\n",
       "      <td>0.237140</td>\n",
       "      <td>-0.498854</td>\n",
       "      <td>-0.646216</td>\n",
       "      <td>0.376962</td>\n",
       "      <td>-0.015636</td>\n",
       "      <td>0.114103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20161004</th>\n",
       "      <td>-0.207614</td>\n",
       "      <td>-0.265522</td>\n",
       "      <td>0.239953</td>\n",
       "      <td>-0.487596</td>\n",
       "      <td>-0.669452</td>\n",
       "      <td>0.346230</td>\n",
       "      <td>0.013481</td>\n",
       "      <td>0.151145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20161005</th>\n",
       "      <td>-0.165409</td>\n",
       "      <td>-0.258032</td>\n",
       "      <td>0.254543</td>\n",
       "      <td>-0.476076</td>\n",
       "      <td>-0.676383</td>\n",
       "      <td>0.369045</td>\n",
       "      <td>0.009359</td>\n",
       "      <td>0.144369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20161006</th>\n",
       "      <td>-0.222821</td>\n",
       "      <td>-0.320958</td>\n",
       "      <td>0.221421</td>\n",
       "      <td>-0.497328</td>\n",
       "      <td>-0.634485</td>\n",
       "      <td>0.371804</td>\n",
       "      <td>-0.032747</td>\n",
       "      <td>0.095357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20161007</th>\n",
       "      <td>-0.195355</td>\n",
       "      <td>-0.296706</td>\n",
       "      <td>0.226934</td>\n",
       "      <td>-0.470243</td>\n",
       "      <td>-0.663831</td>\n",
       "      <td>0.372546</td>\n",
       "      <td>-0.017702</td>\n",
       "      <td>0.146284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20161008</th>\n",
       "      <td>-0.176664</td>\n",
       "      <td>-0.248983</td>\n",
       "      <td>0.161096</td>\n",
       "      <td>-0.537510</td>\n",
       "      <td>-0.637041</td>\n",
       "      <td>0.390565</td>\n",
       "      <td>-0.082044</td>\n",
       "      <td>0.163815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20161009</th>\n",
       "      <td>-0.259883</td>\n",
       "      <td>-0.281645</td>\n",
       "      <td>0.171616</td>\n",
       "      <td>-0.457675</td>\n",
       "      <td>-0.687972</td>\n",
       "      <td>0.360261</td>\n",
       "      <td>0.063804</td>\n",
       "      <td>0.083991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20161010</th>\n",
       "      <td>-0.062354</td>\n",
       "      <td>-0.174777</td>\n",
       "      <td>0.166919</td>\n",
       "      <td>-0.518312</td>\n",
       "      <td>-0.657563</td>\n",
       "      <td>0.382400</td>\n",
       "      <td>-0.149366</td>\n",
       "      <td>0.261012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              w2v_1     w2v_2     w2v_3     w2v_4     w2v_5     w2v_6  \\\n",
       "news_date                                                               \n",
       "20161001  -0.178140 -0.295545  0.241965 -0.507727 -0.646261  0.367630   \n",
       "20161002  -0.189584 -0.286524  0.220864 -0.534220 -0.642891  0.348691   \n",
       "20161003  -0.209694 -0.279247  0.237140 -0.498854 -0.646216  0.376962   \n",
       "20161004  -0.207614 -0.265522  0.239953 -0.487596 -0.669452  0.346230   \n",
       "20161005  -0.165409 -0.258032  0.254543 -0.476076 -0.676383  0.369045   \n",
       "20161006  -0.222821 -0.320958  0.221421 -0.497328 -0.634485  0.371804   \n",
       "20161007  -0.195355 -0.296706  0.226934 -0.470243 -0.663831  0.372546   \n",
       "20161008  -0.176664 -0.248983  0.161096 -0.537510 -0.637041  0.390565   \n",
       "20161009  -0.259883 -0.281645  0.171616 -0.457675 -0.687972  0.360261   \n",
       "20161010  -0.062354 -0.174777  0.166919 -0.518312 -0.657563  0.382400   \n",
       "\n",
       "              w2v_7     w2v_8  \n",
       "news_date                      \n",
       "20161001   0.067822  0.084742  \n",
       "20161002  -0.060732  0.095956  \n",
       "20161003  -0.015636  0.114103  \n",
       "20161004   0.013481  0.151145  \n",
       "20161005   0.009359  0.144369  \n",
       "20161006  -0.032747  0.095357  \n",
       "20161007  -0.017702  0.146284  \n",
       "20161008  -0.082044  0.163815  \n",
       "20161009   0.063804  0.083991  \n",
       "20161010  -0.149366  0.261012  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datagdelt.word2vec_corpus.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOOM! Now we have all of our datapoints with their nlp features neatly arranged in a pandas dataframe, ready for processing. Mission accomplished!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to run this expensive preprocessing again on the same exact data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using word2vec vectorization procedure\n",
      "Nothing to be done, dataframes are up to date\n"
     ]
    }
   ],
   "source": [
    "datagdelt.gdelt_preprocess(vectrz='word2vec',size_w2v=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay for savings!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we initialize the model training class, feeding it the dataframe from the nlp processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## The model training module\n",
    "This section covers model training, validation, and testing, from our model_training module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize a class instance by loading into it two lists: one of names of your choosing and one of dataframes, which in this case is the output form the previous module above, datagdelt.vect_corpus_tfidf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import model_training as mdlt\n",
    "importlib.reload(mdlt)\n",
    "tet=mdlt.StockPrediction([['word2vec'],[datagdelt.word2vec_corpus],[datagdelt.w2vec_model]],update=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try an L1 linear regressor which is trying to predict the increase/decrease of tomorrow's S&P index over today's. We test on the last 20 days out of 50 and validate/tune, for every testing case, over the previous 10 days. As for the hyperparameters, we are letting our regularization parameter be searched for in the 0.001-3000 range and we allow for 40 iterations of the optimal parameter search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py3k/lib/python3.5/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameter choices: (80.55273365340669,)\n",
      "model_test_rmse: 4.911 benchmark_test_rmse: 7.466\n",
      "best parameter choices: (77.7076393080325,)\n",
      "model_test_rmse: 15.977 benchmark_test_rmse: 13.611\n",
      "best parameter choices: (46.47802028916847,)\n",
      "model_test_rmse: 18.671 benchmark_test_rmse: 16.867\n",
      "best parameter choices: (46.47802028916847,)\n",
      "model_test_rmse: 1.674 benchmark_test_rmse: 3.337\n",
      "best parameter choices: (33.13449456383699,)\n",
      "model_test_rmse: 2.280 benchmark_test_rmse: 3.595\n",
      "best parameter choices: (32.08075591740211,)\n",
      "model_test_rmse: 14.027 benchmark_test_rmse: 15.351\n",
      "best parameter choices: (32.08075591740211,)\n",
      "model_test_rmse: 1.953 benchmark_test_rmse: 3.403\n",
      "best parameter choices: (32.08075591740211,)\n",
      "model_test_rmse: 0.787 benchmark_test_rmse: 0.653\n",
      "best parameter choices: (32.08075591740211,)\n",
      "model_test_rmse: 1.405 benchmark_test_rmse: 0.025\n",
      "best parameter choices: (32.08075591740211,)\n",
      "model_test_rmse: 16.667 benchmark_test_rmse: 15.245\n",
      "best parameter choices: (31.2377650002542,)\n",
      "model_test_rmse: 4.999 benchmark_test_rmse: 6.362\n",
      "best parameter choices: (32.08075591740211,)\n",
      "model_test_rmse: 0.508 benchmark_test_rmse: 0.909\n",
      "best parameter choices: (31.2377650002542,)\n",
      "model_test_rmse: 1.559 benchmark_test_rmse: 0.172\n",
      "best parameter choices: (2.7868215465124404,)\n",
      "model_test_rmse: 12.517 benchmark_test_rmse: 11.780\n",
      "best parameter choices: (2.7868215465124404,)\n",
      "model_test_rmse: 7.231 benchmark_test_rmse: 6.679\n",
      "best parameter choices: (2.7868215465124404,)\n",
      "model_test_rmse: 10.916 benchmark_test_rmse: 10.525\n",
      "best parameter choices: (2.7868215465124404,)\n",
      "model_test_rmse: 7.670 benchmark_test_rmse: 7.591\n",
      "best parameter choices: (0.8900919829296523,)\n",
      "model_test_rmse: 9.592 benchmark_test_rmse: 9.849\n",
      "best parameter choices: (0.8900919829296523,)\n",
      "model_test_rmse: 4.632 benchmark_test_rmse: 3.955\n",
      "best parameter choices: (0.15247493042523594,)\n",
      "model_test_rmse: 1.855 benchmark_test_rmse: 2.056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(6.9915571716431488, 5.7916109207521451),\n",
       " (6.9715473175452187, 5.3271630939674157)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tet.auto_ts_val_test_reg('word2vec','lasso',[['alpha',[0.001,7000.,60.]]],parm_search_iter=50,n_folds_val=20,\n",
    "                         past_depth=50,n_folds_test=20,scaling=True,differential=False,notest=False,verbose=False,\n",
    "                         eqdiff=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, the model clearly falls back on the benchmark. The coefficients are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.        , -0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        , -0.        ,  1.41475612,  1.00397025])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_imp=tet.models['word2vec'].coef_\n",
    "feat_imp\n",
    "#these are the feature importances for the lasso classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...which isn't surprising. As we said at the beginning, the most important feature should have been today's closing, and it was, entirely offuscating everything else. (Except for the engineered after_weekend feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If one had gotten a reasonable result, they might want to play with feature importances to try and see which stems actually were the most significant. It can be done as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-20 06:46:15,943 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('salli', 0.9973966479301453),\n",
       " ('hiv', 0.997178316116333),\n",
       " ('elig', 0.9961667060852051),\n",
       " ('diabet', 0.9960787296295166),\n",
       " ('globe', 0.9957556128501892),\n",
       " ('beyonc', 0.9957464933395386),\n",
       " ('diana', 0.995519757270813),\n",
       " ('anxieti', 0.9954898953437805),\n",
       " ('payment', 0.9949756860733032),\n",
       " ('bump', 0.9947608709335327)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=tet.w2v_models['word2vec']\n",
    "model.similar_by_word('appl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('redlin', 0.0),\n",
       " ('aep', 0.0),\n",
       " ('curt', 0.0),\n",
       " ('disd', 0.0),\n",
       " ('oussid', 0.0),\n",
       " ('astrazeneca', 0.0),\n",
       " ('gsfqo', 0.0),\n",
       " ('uzel', 0.0),\n",
       " ('layton', 0.0),\n",
       " ('idinkbntiz', 0.0)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_vector(feat_imp[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameter choices: (178499.97550000023,)\n",
      "model_test_rmse: 7.466 benchmark_test_rmse: 7.466\n",
      "best parameter choices: (178499.97550000023,)\n",
      "model_test_rmse: 13.611 benchmark_test_rmse: 13.611\n",
      "best parameter choices: (178499.97550000023,)\n",
      "model_test_rmse: 16.867 benchmark_test_rmse: 16.867\n",
      "best parameter choices: (178499.97550000023,)\n",
      "model_test_rmse: 3.337 benchmark_test_rmse: 3.337\n",
      "best parameter choices: (178499.97550000023,)\n",
      "model_test_rmse: 3.595 benchmark_test_rmse: 3.595\n",
      "best parameter choices: (178499.97550000023,)\n",
      "model_test_rmse: 15.351 benchmark_test_rmse: 15.351\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-129-66c6b187ebd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m tet.auto_ts_val_test_reg('word2vec','ridge',[['alpha',[0.001,7000.,60.]]],parm_search_iter=50,n_folds_val=20,\n\u001b[1;32m      2\u001b[0m                          \u001b[0mpast_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_folds_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscaling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdifferential\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnotest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                          eqdiff=False)\n\u001b[0m",
      "\u001b[0;32m/Users/Maxos/Desktop/Insight_stuff/bigsnippyrepo/maqro/notebooks/../source/model_training.py\u001b[0m in \u001b[0;36mauto_ts_val_test_reg\u001b[0;34m(self, dataset_id, regressor, parm_ranges, parm_search_iter, n_folds_val, past_depth, n_folds_test, scaling, differential, eqdiff, verbose, notest)\u001b[0m\n\u001b[1;32m    667\u001b[0m                         \u001b[0my_trval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbreak_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbreak_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbreak_index\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                         \u001b[0mmin_parm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_auto_kfold_reg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_trval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_trval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparm_search_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparm_ranges\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_folds_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpast_depth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mregressor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdifferential\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdifferential\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscaling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaling\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meqdiff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meqdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m                         \u001b[0mres\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkfold_test_reg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mregressor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_parm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdifferential\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdifferential\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscaling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaling\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meqdiff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meqdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                         \u001b[0mperformances\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Maxos/Desktop/Insight_stuff/bigsnippyrepo/maqro/notebooks/../source/model_training.py\u001b[0m in \u001b[0;36m_auto_kfold_reg\u001b[0;34m(self, x_trval, x_test, y_trval, y_test, parm_search_iter, parm_ranges, verbose, n_folds_val, past_depth, dataset_id, dataset_name, regressor, differential, scaling, eqdiff)\u001b[0m\n\u001b[1;32m    526\u001b[0m                                 \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m                                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m                                 \u001b[0mavg_rms_mod_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkfold_val_reg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_folds_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpast_depth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mregressor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdifferential\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdifferential\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscaling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaling\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meqdiff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meqdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m                                 \u001b[0;32mif\u001b[0m \u001b[0mmin_err\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mavg_rms_mod_val\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m                                         \u001b[0mmin_err\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mavg_rms_mod_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Maxos/Desktop/Insight_stuff/bigsnippyrepo/maqro/notebooks/../source/model_training.py\u001b[0m in \u001b[0;36mkfold_val_reg\u001b[0;34m(self, n_folds_val, past_depth, dataset_id, regressor, parm, seed, eqdiff, differential, scaling, verbose)\u001b[0m\n\u001b[1;32m    426\u001b[0m                 \u001b[0mdataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_id_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m                 \u001b[0mx_trainval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xretrieval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscaling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaling\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meqdiff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meqdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m                 \u001b[0mnumb_datapoints\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_trainval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Maxos/Desktop/Insight_stuff/bigsnippyrepo/maqro/notebooks/../source/model_training.py\u001b[0m in \u001b[0;36m_xretrieval\u001b[0;34m(self, dataset_name, scaling, eqdiff)\u001b[0m\n\u001b[1;32m    411\u001b[0m                                 \u001b[0mx_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mscaling\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m                         \u001b[0mx_trainval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_trainval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m                                 \u001b[0mx_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py3k/lib/python3.5/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mscale\u001b[0;34m(X, axis, with_mean, with_std, copy)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0mXr\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mscale_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwith_mean\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                 \u001b[0mmean_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m                 \u001b[0;31m# If mean_2 is not 'close to zero', it comes from the fact that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0;31m# scale_ is very small so that mean_2 = mean_1/scale_ > 0, even\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py3k/lib/python3.5/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         ret = um.true_divide(\n\u001b[0;32m---> 68\u001b[0;31m                 ret, rcount, out=ret, casting='unsafe', subok=False)\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dtype'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mrcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tet.auto_ts_val_test_reg('word2vec','ridge',[['alpha',[0.001,7000.,60.]]],parm_search_iter=50,n_folds_val=20,\n",
    "                         past_depth=50,n_folds_test=20,scaling=True,differential=True,notest=False,verbose=False,\n",
    "                         eqdiff=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2356.00999525])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tet.auto_ts_val_test_reg('word2vec','ridge',[['alpha',[0.001,7000.,60.]]],parm_search_iter=50,n_folds_val=20,\n",
    "                         past_depth=50,n_folds_test=20,scaling=False,differential=False,notest=True,verbose=False,\n",
    "                         eqdiff=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm... Not sure what to make of this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about we try a random forest regressor instead? We are letting our tuning select any combination among 5 values for the number of estimators, 5 for the maximum number of features used for splitting, and we allow a maximum depth from 5 to 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameter choices: (6, 6, 8)\n",
      "model_test_rmse: 7.792 benchmark_test_rmse: 7.466\n",
      "best parameter choices: (5, 6, 4)\n",
      "model_test_rmse: 19.177 benchmark_test_rmse: 13.611\n",
      "best parameter choices: (5, 6, 7)\n",
      "model_test_rmse: 23.201 benchmark_test_rmse: 16.867\n",
      "best parameter choices: (5, 10, 8)\n",
      "model_test_rmse: 0.070 benchmark_test_rmse: 3.337\n",
      "best parameter choices: (5, 7, 9)\n",
      "model_test_rmse: 3.530 benchmark_test_rmse: 3.595\n",
      "best parameter choices: (5, 9, 8)\n",
      "model_test_rmse: 29.664 benchmark_test_rmse: 15.351\n",
      "best parameter choices: (7, 5, 4)\n",
      "model_test_rmse: 8.364 benchmark_test_rmse: 3.403\n",
      "best parameter choices: (6, 8, 5)\n",
      "model_test_rmse: 2.911 benchmark_test_rmse: 0.653\n",
      "best parameter choices: (6, 5, 6)\n",
      "model_test_rmse: 4.993 benchmark_test_rmse: 0.025\n",
      "best parameter choices: (5, 8, 8)\n",
      "model_test_rmse: 18.863 benchmark_test_rmse: 15.245\n",
      "best parameter choices: (5, 10, 5)\n",
      "model_test_rmse: 9.581 benchmark_test_rmse: 6.362\n",
      "best parameter choices: (7, 10, 5)\n",
      "model_test_rmse: 6.436 benchmark_test_rmse: 0.909\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-90bb0d37ff99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m tet.auto_ts_val_test_reg('word2vec','rfreg',[['n_estim',{5,6,7}],['max_feat',{5,6,7,8,9,10}],['max_depth',{4,5,6,7,8,9}]],\n\u001b[1;32m      2\u001b[0m                          \u001b[0mparm_search_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_folds_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpast_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_folds_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscaling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdifferential\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                          verbose=False,notest=False,eqdiff=False)\n\u001b[0m",
      "\u001b[0;32m/Users/Maxos/Desktop/Insight_stuff/bigsnippyrepo/maqro/notebooks/../source/model_training.py\u001b[0m in \u001b[0;36mauto_ts_val_test_reg\u001b[0;34m(self, dataset_id, regressor, parm_ranges, parm_search_iter, n_folds_val, past_depth, n_folds_test, scaling, differential, eqdiff, verbose, notest, seed)\u001b[0m\n\u001b[1;32m    653\u001b[0m                         \u001b[0my_trval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbreak_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbreak_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbreak_index\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m                         \u001b[0mmin_parm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_auto_kfold_reg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_trval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_trval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparm_search_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparm_ranges\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_folds_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpast_depth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mregressor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdifferential\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdifferential\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscaling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaling\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meqdiff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meqdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m                         \u001b[0mres\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkfold_test_reg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mregressor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_parm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdifferential\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdifferential\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscaling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaling\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meqdiff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meqdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m                         \u001b[0mperformances\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Maxos/Desktop/Insight_stuff/bigsnippyrepo/maqro/notebooks/../source/model_training.py\u001b[0m in \u001b[0;36m_auto_kfold_reg\u001b[0;34m(self, x_trval, x_test, y_trval, y_test, parm_search_iter, parm_ranges, verbose, n_folds_val, past_depth, dataset_id, dataset_name, regressor, seed, differential, scaling, eqdiff)\u001b[0m\n\u001b[1;32m    518\u001b[0m                                 \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m                                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m                                 \u001b[0mavg_rms_mod_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkfold_val_reg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_folds_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpast_depth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mregressor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdifferential\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdifferential\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscaling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaling\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meqdiff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meqdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m                                 \u001b[0;32mif\u001b[0m \u001b[0mmin_err\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mavg_rms_mod_val\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m                                         \u001b[0mmin_err\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mavg_rms_mod_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Maxos/Desktop/Insight_stuff/bigsnippyrepo/maqro/notebooks/../source/model_training.py\u001b[0m in \u001b[0;36mkfold_val_reg\u001b[0;34m(self, n_folds_val, past_depth, dataset_id, regressor, parm, seed, eqdiff, differential, scaling, verbose)\u001b[0m\n\u001b[1;32m    443\u001b[0m                         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_init_reg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregressor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m                         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m                         \u001b[0;31m#storing the train/validation rmse performances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m                         \u001b[0mavg_rms_mod_val\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py3k/lib/python3.5/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_more_estimators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                 tree = self._make_estimator(append=False,\n\u001b[0;32m--> 314\u001b[0;31m                                             random_state=random_state)\n\u001b[0m\u001b[1;32m    315\u001b[0m                 \u001b[0mtrees\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py3k/lib/python3.5/site-packages/sklearn/ensemble/base.py\u001b[0m in \u001b[0;36m_make_estimator\u001b[0;34m(self, append, random_state)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_estimator_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         estimator.set_params(**dict((p, getattr(self, p))\n\u001b[0;32m--> 121\u001b[0;31m                                     for p in self.estimator_params))\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py3k/lib/python3.5/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mset_params\u001b[0;34m(self, **params)\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0mvalid_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0msplit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0;31m# nested objects case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tet.auto_ts_val_test_reg('word2vec','rfreg',[['n_estim',{5,6,7}],['max_feat',{5,6,7,8,9,10}],['max_depth',{4,5,6,7,8,9}]],\n",
    "                         parm_search_iter=1,n_folds_val=25,past_depth=20,n_folds_test=20,scaling=True,differential=True,\n",
    "                         verbose=False,notest=False,eqdiff=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we want to get a prediction for today, we toggle the 'notest' attribute to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2230.36855433])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tet.auto_ts_val_test_reg('word2vec','rfreg',[['n_estim',{5,6,7}],['max_feat',{5,6,7,8,9,10}],['max_depth',{4,5,6,7,8,9}]],\n",
    "                         parm_search_iter=1,n_folds_val=25,past_depth=20,n_folds_test=20,scaling=True,differential=True,\n",
    "                         verbose=False,notest=True,eqdiff=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.1341678 ,  0.15907709,  0.07990877,  0.06699914,  0.03191157,\n",
       "        0.09950022,  0.13786384,  0.11339288,  0.01543178,  0.16174691])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_imp=tet.models['word2vec'].feature_importances_\n",
    "feat_imp\n",
    "#these are the feature importances for a random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can't predict for tomorrow, because the stock market will be closed\n"
     ]
    }
   ],
   "source": [
    "tet.auto_ts_val_test_reg('word2vec','rfreg',[['n_estim',{1,2,3,5,7}],['max_feat',{21,22,23,24}],['max_depth',{5,6,7}]],\n",
    "                         parm_search_iter=1,n_folds_val=6,n_folds_test=20,scaling=True,differential=True,verbose=False,\n",
    "                         notest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameter choices: (4,)\n",
      "model_test_rmse: 14.150 benchmark_test_rmse: 15.351\n",
      "best parameter choices: (4,)\n",
      "model_test_rmse: 2.390 benchmark_test_rmse: 3.403\n",
      "best parameter choices: (4,)\n",
      "model_test_rmse: 0.320 benchmark_test_rmse: 0.653\n",
      "best parameter choices: (4,)\n",
      "model_test_rmse: 0.940 benchmark_test_rmse: 0.025\n",
      "best parameter choices: (5,)\n",
      "model_test_rmse: 13.330 benchmark_test_rmse: 15.245\n",
      "best parameter choices: (4,)\n",
      "model_test_rmse: 5.220 benchmark_test_rmse: 6.362\n",
      "best parameter choices: (4,)\n",
      "model_test_rmse: 0.160 benchmark_test_rmse: 0.909\n",
      "best parameter choices: (4,)\n",
      "model_test_rmse: 1.230 benchmark_test_rmse: 0.172\n",
      "best parameter choices: (4,)\n",
      "model_test_rmse: 11.695 benchmark_test_rmse: 11.780\n",
      "best parameter choices: (6,)\n",
      "model_test_rmse: 3.307 benchmark_test_rmse: 6.679\n",
      "best parameter choices: (6,)\n",
      "model_test_rmse: 7.227 benchmark_test_rmse: 10.525\n",
      "best parameter choices: (6,)\n",
      "model_test_rmse: 4.407 benchmark_test_rmse: 7.591\n",
      "best parameter choices: (6,)\n",
      "model_test_rmse: 6.747 benchmark_test_rmse: 9.849\n",
      "best parameter choices: (6,)\n",
      "model_test_rmse: 6.953 benchmark_test_rmse: 3.955\n",
      "best parameter choices: (6,)\n",
      "model_test_rmse: 0.983 benchmark_test_rmse: 2.056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(5.2705113333334035, 4.5526692805490301),\n",
       " (6.3036606676414824, 5.1252792112507803)]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tet.auto_ts_val_test_reg('word2vec','knnreg',[['numb_nn',{3,4,5,6}]],parm_search_iter=10,n_folds_val=15,\n",
    "                         n_folds_test=15,past_depth=30,scaling=True,differential=True,notest=False,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2360.38659633])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tet.auto_ts_val_test_reg('word2vec','knnreg',[['numb_nn',{3,4,5,6}]],parm_search_iter=10,n_folds_val=15,\n",
    "                         n_folds_test=15,past_depth=30,scaling=True,differential=True,notest=True,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameter choices: ('l1', 0.1)\n",
      "test_rec,prec,F1: [1.0, 1.0, 1.0] benchmark_rec,prec,F1: [1.0, 1.0, 1.0]\n",
      "best parameter choices: ('l1', 0.1)\n",
      "test_rec,prec,F1: [1.0, 1.0, 1.0] benchmark_rec,prec,F1: [1.0, 1.0, 1.0]\n",
      "best parameter choices: ('l1', 0.1)\n",
      "test_rec,prec,F1: [0.0, 1.0, 0.0] benchmark_rec,prec,F1: [0.0, 1.0, 0.0]\n",
      "best parameter choices: ('l1', 0.1)\n",
      "test_rec,prec,F1: [0.0, 1.0, 0.0] benchmark_rec,prec,F1: [0.0, 1.0, 0.0]\n",
      "best parameter choices: ('l1', 0.1)\n",
      "test_rec,prec,F1: [0.0, 1.0, 0.0] benchmark_rec,prec,F1: [0.0, 1.0, 0.0]\n",
      "best parameter choices: ('l1', 0.1)\n",
      "test_rec,prec,F1: [1.0, 0.0, 0.0] benchmark_rec,prec,F1: [1.0, 1.0, 1.0]\n",
      "best parameter choices: ('l1', 0.1)\n",
      "test_rec,prec,F1: [0.0, 1.0, 0.0] benchmark_rec,prec,F1: [0.0, 1.0, 0.0]\n",
      "best parameter choices: ('l1', 0.1)\n",
      "test_rec,prec,F1: [1.0, 1.0, 1.0] benchmark_rec,prec,F1: [0.0, 1.0, 0.0]\n",
      "best parameter choices: ('l1', 0.1)\n",
      "test_rec,prec,F1: [1.0, 1.0, 1.0] benchmark_rec,prec,F1: [1.0, 1.0, 1.0]\n",
      "best parameter choices: ('l1', 0.1)\n",
      "test_rec,prec,F1: [1.0, 1.0, 1.0] benchmark_rec,prec,F1: [1.0, 1.0, 1.0]\n",
      "best parameter choices: ('l1', 0.1)\n",
      "test_rec,prec,F1: [1.0, 1.0, 1.0] benchmark_rec,prec,F1: [1.0, 1.0, 1.0]\n",
      "best parameter choices: ('l1', 0.1)\n",
      "test_rec,prec,F1: [1.0, 1.0, 1.0] benchmark_rec,prec,F1: [1.0, 1.0, 1.0]\n",
      "best parameter choices: ('l1', 0.1)\n",
      "test_rec,prec,F1: [1.0, 1.0, 1.0] benchmark_rec,prec,F1: [1.0, 1.0, 1.0]\n",
      "best parameter choices: ('l1', 0.1)\n",
      "test_rec,prec,F1: [1.0, 0.0, 0.0] benchmark_rec,prec,F1: [1.0, 0.0, 0.0]\n",
      "best parameter choices: ('l1', 0.1)\n",
      "test_rec,prec,F1: [1.0, 1.0, 1.0] benchmark_rec,prec,F1: [1.0, 1.0, 1.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(array([[ 0.73333333,  0.86666667,  0.6       ]]),\n",
       "  array([[ 0.44221664,  0.33993463,  0.48989795]])),\n",
       " (array([[ 0.66666667,  0.93333333,  0.6       ]]),\n",
       "  array([[ 0.47140452,  0.24944383,  0.48989795]]))]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tet.auto_ts_val_test_class('word2vec','logreg',[['l1orl2?',{'l1',}],\n",
    "                                                ['C',[0.1,1.,0.3]]],\n",
    "                           parm_search_iter=30,n_folds_val=15,n_folds_test=15,past_depth=50,scaling=False,notest=False,\n",
    "                           verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if classifying tomorrow's value going up or down will do us and better...\n",
    "N.B. We need to specify a decision threshold which I recommend leaving at 0.5 for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameter choices: (1, 9, 3)\n",
      "test_rec,prec,F1: [1.0, 1.0, 1.0] benchmark_rec,prec,F1: [1.0, 1.0, 1.0]\n",
      "best parameter choices: (3, 10, 5)\n",
      "test_rec,prec,F1: [1.0, 1.0, 1.0] benchmark_rec,prec,F1: [1.0, 1.0, 1.0]\n",
      "best parameter choices: (1, 9, 3)\n",
      "test_rec,prec,F1: [0.0, 1.0, 0.0] benchmark_rec,prec,F1: [0.0, 1.0, 0.0]\n",
      "best parameter choices: (1, 9, 3)\n",
      "test_rec,prec,F1: [0.0, 1.0, 0.0] benchmark_rec,prec,F1: [0.0, 1.0, 0.0]\n",
      "best parameter choices: (2, 9, 5)\n",
      "test_rec,prec,F1: [1.0, 1.0, 1.0] benchmark_rec,prec,F1: [0.0, 1.0, 0.0]\n",
      "best parameter choices: (3, 8, 5)\n",
      "test_rec,prec,F1: [1.0, 1.0, 1.0] benchmark_rec,prec,F1: [1.0, 1.0, 1.0]\n",
      "best parameter choices: (2, 8, 5)\n",
      "test_rec,prec,F1: [1.0, 1.0, 1.0] benchmark_rec,prec,F1: [0.0, 1.0, 0.0]\n",
      "best parameter choices: (5, 9, 4)\n",
      "test_rec,prec,F1: [0.0, 1.0, 0.0] benchmark_rec,prec,F1: [0.0, 1.0, 0.0]\n",
      "best parameter choices: (3, 9, 4)\n",
      "test_rec,prec,F1: [0.0, 1.0, 0.0] benchmark_rec,prec,F1: [1.0, 1.0, 1.0]\n",
      "best parameter choices: (3, 8, 3)\n",
      "test_rec,prec,F1: [1.0, 1.0, 1.0] benchmark_rec,prec,F1: [1.0, 1.0, 1.0]\n",
      "best parameter choices: (1, 9, 4)\n",
      "test_rec,prec,F1: [1.0, 1.0, 1.0] benchmark_rec,prec,F1: [1.0, 1.0, 1.0]\n",
      "best parameter choices: (3, 8, 3)\n",
      "test_rec,prec,F1: [1.0, 1.0, 1.0] benchmark_rec,prec,F1: [1.0, 1.0, 1.0]\n",
      "best parameter choices: (1, 9, 5)\n",
      "test_rec,prec,F1: [1.0, 1.0, 1.0] benchmark_rec,prec,F1: [1.0, 1.0, 1.0]\n",
      "best parameter choices: (4, 9, 4)\n",
      "test_rec,prec,F1: [1.0, 1.0, 1.0] benchmark_rec,prec,F1: [1.0, 0.0, 0.0]\n",
      "best parameter choices: (3, 10, 2)\n",
      "test_rec,prec,F1: [0.0, 1.0, 0.0] benchmark_rec,prec,F1: [1.0, 1.0, 1.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(array([[ 0.66666667,  1.        ,  0.66666667]]),\n",
       "  array([[ 0.47140452,  0.        ,  0.47140452]])),\n",
       " (array([[ 0.66666667,  0.93333333,  0.6       ]]),\n",
       "  array([[ 0.47140452,  0.24944383,  0.48989795]]))]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tet.auto_ts_val_test_class('word2vec','rfclass',[['n_estim',{1,2,3,4,5}],['max_feat',{8,9,10}],\n",
    "                                                 ['max_depth',{2,3,4,5}]],parm_search_iter=1,n_folds_val=20,\n",
    "                           past_depth=40,n_folds_test=15,scaling=False,notest=False,verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good! Our model overperform the benchmark as for accuracy (=F1 in this case): 0.67 vs 0.60.\n",
    "\n",
    "Now let's predict what today's closing is going to be!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tet.auto_ts_val_test_class('word2vec','rfclass',[['n_estim',{1,2,3,4,5}],['max_feat',{8,9,10}],\n",
    "                                                 ['max_depth',{2,3,4,5}]],parm_search_iter=1,n_folds_val=20,\n",
    "                           past_depth=40,n_folds_test=15,scaling=False,notest=True,verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will go up, apparently!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01150188,  0.        ,  0.        ,  0.04061601,  0.        ,\n",
       "        0.16298408,  0.32183145,  0.06389972,  0.        ,  0.06375228,\n",
       "        0.12311446,  0.21230012])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_imp=tet.models['word2vec'].feature_importances_\n",
    "feat_imp\n",
    "#remember: only the first n-2 features are nlp, the (n-1)-th is being after a weekend and the n-th is today's closing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tet.auto_ts_val_test_class('word2vec','svmclass',[['C',[0.000001,1.,0.001]],['kernel',{'poly','linear'}]],\n",
    "                           parm_search_iter=15,past_depth=30,n_folds_val=15,n_folds_test=15,scaling=False,notest=False,\n",
    "                           verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, for the real deal: k-fold training and validation!\n",
    "The following method performs that in a very general manner. It lets you decide what regression model to choose, as well as the values of the hyperparameters (please see the module documentation in model_training.py for details on how to pass the hyperparameters), also you need to supply the number of folds you want your data split into, and a seed, for reproducibility. There is also an option to scale and normalize the features but it doesn't quite perform well in general.\n",
    "\n",
    "The method returns the model average performance over the k training iterations. In short, tuning will consist of choosing the value for the hyperparameters that optimizes avg_validation_rmse (that is minimize the average root mean squared on the validation datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By chance, in this one case we outperform the benchmark model with a lower rmse, but this procedure should be performed a couple of time and an average final performance should be quoted instead.\n",
    "\n",
    "Out of curiosity, let's see what the most important features were."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the method returns again average validation performances which are now measured in terms of recall, precision, and F1 score. In lack of a specific metric we want to optimize, we are going to use the F1 score for tuning.\n",
    "\n",
    "The performance plateaus and is optimal for alpha ~1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bingo! Our model predicts all 1's. Not much gained...\n",
    "\n",
    "Incidentally anyway, that's how you pull the predictions vector for a specific dataset.\n",
    "In the future I'll give the option to save a specific model run instead of overwriting. Good for free exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch from now on, please ignore!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out different regressors on the data, no luck so far :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=0.8, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='poly',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 896,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model=LogisticRegression(penalty='l2', C=1.) #, dual=False, tol=0.0001,, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='liblinear', max_iter=100, multi_class='ovr', verbose=0, warm_start=False, n_jobs=1)\n",
    "#model=RandomForestClassifier(n_estimators=10,max_features=4000,max_depth=5)#,max_features=7000)\n",
    "#model=AdaBoostClassifier(n_estimators=20)\n",
    "#model=MLPClassifier(activation='logistic',hidden_layer_sizes=(100,10,5))\n",
    "#model=KNeighborsClassifier(n_neighbors=15)\n",
    "model=SVC(C=.8,kernel='poly')\n",
    "model.fit(x_tfidf_class_trainval,y_tfidf_class_trainval)\n",
    "#model.fit(x_bow_class_trainval,y_bow_class_trainval)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py3k]",
   "language": "python",
   "name": "Python [py3k]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
