{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time price prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO: integrate all data, split into test/train cases, separate functions into their own .py files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ijson\n",
    "import json\n",
    "import gensim\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test articles from 2 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Jan25_article = 'Earnings_Preview_Facebook_Inc_NASDAQFB_moved_up_10_times_out_of_last_17_quarters__The_Independent_Republic.json'\n",
    "Jan26_article = 'Facebook_Inc_FB_Hires_Former_Google_Exec_to_Lead_Oculus.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframe from JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Earnings_Preview_Facebook_Inc_NASDAQFB_moved_up_10_times_out_of_last_17_quarters__The_Independent_Republic.json',\n",
       " 'Facebook_Inc_FB_Hires_Former_Google_Exec_to_Lead_Oculus.json']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articlelist = [Jan25_article, Jan26_article]\n",
    "articlelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_Google_articles(articlelist):\n",
    "    \"\"\" \n",
    "    Converts Google News JSON file into a data frame. Takes in\n",
    "    a .json file and returns a dataframe using the json's dictionary-like\n",
    "    structure \n",
    "    \"\"\"\n",
    "    \n",
    "    with open(articlelist[0],'r') as first:\n",
    "        firstdict = json.load(first)\n",
    "        combined_df = pd.DataFrame.from_dict(firstdict, orient = 'index')\n",
    "        combined_df = combined_df.T\n",
    "    \n",
    "    for article in articlelist:\n",
    "        with open(article, 'r') as fin:\n",
    "            mydict = json.load(fin)\n",
    "        current_df = pd.DataFrame.from_dict(mydict, orient = 'index')\n",
    "        current_df = current_df.T\n",
    "    \n",
    "    # USE CONCAT WITH .APPEND DOESN'T WORK!!!\n",
    "    final_df = pd.concat([combined_df, current_df])\n",
    "        \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Facebook, Inc. (NASDAQ:FB) is projected to dec...</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>Earnings Preview: Facebook, Inc. (NASDAQ:FB) m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Facebook Inc’s (NASDAQ: ) virtual reality divi...</td>\n",
       "      <td>Facebook 1-26-17</td>\n",
       "      <td>Facebook Inc (FB) Hires Former Google Exec to ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body          category  \\\n",
       "0  Facebook, Inc. (NASDAQ:FB) is projected to dec...          Facebook   \n",
       "0  Facebook Inc’s (NASDAQ: ) virtual reality divi...  Facebook 1-26-17   \n",
       "\n",
       "                                               title  \n",
       "0  Earnings Preview: Facebook, Inc. (NASDAQ:FB) m...  \n",
       "0  Facebook Inc (FB) Hires Former Google Exec to ...  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_articles_df = read_Google_articles(articlelist)\n",
    "all_articles_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess these articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_article_content(text_df):\n",
    "    \"\"\"\n",
    "    Simple preprocessing pipeline which uses RegExp, sets basic token requirements, and removes stop words.\n",
    "    Set up to work with df files created from JSONs\n",
    "    \"\"\"\n",
    "    print 'preprocessing article text...'\n",
    "\n",
    "    # tokenizer, stops, and stemmer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    stop_words = set(stopwords.words('english'))  # can add more stop words to this set\n",
    "    stemmer = SnowballStemmer('english')\n",
    "\n",
    "    # process articles\n",
    "    article_list = []\n",
    "    for row, article in enumerate(text_df['body']):\n",
    "        cleaned_tokens = []\n",
    "\n",
    "        letters_only = re.sub(\"[^a-zA-Z]\", \" \", article)\n",
    "        lower_case = letters_only.lower()\n",
    "        tokens = tokenizer.tokenize(lower_case)\n",
    "\n",
    "    #         tokens = tokenizer.tokenize(article.decode('utf-8').lower())\n",
    "        for token in tokens:\n",
    "            if token not in stop_words:\n",
    "                if len(token) > 0 and len(token) < 20: # removes non words\n",
    "                    if not token[0].isdigit() and not token[-1].isdigit(): # removes numbers\n",
    "                        stemmed_tokens = stemmer.stem(token)\n",
    "                        cleaned_tokens.append(stemmed_tokens)\n",
    "        # add process article\n",
    "        article_list.append(' '.join(wd for wd in cleaned_tokens))\n",
    "\n",
    "    # echo results and return\n",
    "    print 'preprocessed content for %d articles' % len(article_list)\n",
    "    return article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing article text...\n",
      "preprocessed content for 2 articles\n"
     ]
    }
   ],
   "source": [
    "cleaned_tokenized_stemmed_lemmatized_articles = preprocess_article_content(all_articles_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_tokenized_stemmed_lemmatized_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize the bag of words (token count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# can do feature(token) reduction later for too rare and too common words\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# perform count-based vectorization\n",
    "article_vect = vectorizer.fit_transform(cleaned_tokenized_stemmed_lemmatized_articles)\n",
    "\n",
    "# Kaggle method \n",
    "train_data_features = article_vect.toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a look at the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ad\n",
      "1 adr\n",
      "1 afford\n",
      "1 alphabet\n",
      "5 analyst\n",
      "2 android\n",
      "7 announc\n",
      "1 answer\n",
      "1 approach\n",
      "1 april\n",
      "1 around\n",
      "1 augment\n",
      "2 averag\n",
      "1 back\n",
      "1 bad\n",
      "7 barra\n",
      "1 base\n",
      "1 beat\n",
      "1 beaten\n",
      "1 began\n",
      "1 begin\n",
      "1 beij\n",
      "1 belief\n",
      "1 billion\n",
      "1 bought\n",
      "1 brendan\n",
      "2 bring\n",
      "1 call\n",
      "2 came\n",
      "1 cap\n",
      "2 ceo\n",
      "1 ces\n",
      "3 chang\n",
      "1 china\n",
      "1 chines\n",
      "1 claim\n",
      "1 climb\n",
      "1 close\n",
      "3 compani\n",
      "1 compar\n",
      "1 compel\n",
      "1 complaint\n",
      "1 comput\n",
      "8 consensus\n",
      "3 consum\n",
      "1 continu\n",
      "1 control\n",
      "1 copyright\n",
      "1 corp\n",
      "1 cover\n",
      "1 critic\n",
      "2 data\n",
      "1 date\n",
      "6 day\n",
      "1 decemb\n",
      "1 declar\n",
      "1 declin\n",
      "1 detail\n",
      "3 develop\n",
      "1 devic\n",
      "4 divis\n",
      "2 drop\n",
      "16 earn\n",
      "3 effort\n",
      "1 electron\n",
      "2 end\n",
      "1 ep\n",
      "1 equiti\n",
      "6 estim\n",
      "1 excit\n",
      "4 expect\n",
      "1 experienc\n",
      "6 facebook\n",
      "1 fade\n",
      "1 familiar\n",
      "6 fb\n",
      "1 februari\n",
      "1 feel\n",
      "1 fell\n",
      "1 financi\n",
      "1 firm\n",
      "1 fiscal\n",
      "1 fit\n",
      "4 follow\n",
      "2 founder\n",
      "1 four\n",
      "1 fourth\n",
      "2 gain\n",
      "1 game\n",
      "1 gave\n",
      "2 giant\n",
      "1 given\n",
      "1 go\n",
      "1 googl\n",
      "2 headset\n",
      "2 help\n",
      "1 high\n",
      "1 higher\n",
      "2 hire\n",
      "2 histori\n",
      "7 hugo\n",
      "1 hype\n",
      "1 imag\n",
      "4 inc\n",
      "2 includ\n",
      "2 industri\n",
      "1 innov\n",
      "1 irib\n",
      "1 januari\n",
      "2 join\n",
      "1 juli\n",
      "1 kickstart\n",
      "2 known\n",
      "9 last\n",
      "1 lawsuit\n",
      "4 lead\n",
      "1 leav\n",
      "1 level\n",
      "1 like\n",
      "1 limitless\n",
      "1 long\n",
      "1 look\n",
      "1 low\n",
      "2 luckey\n",
      "1 made\n",
      "2 major\n",
      "1 manag\n",
      "1 mark\n",
      "5 market\n",
      "1 may\n",
      "1 met\n",
      "1 million\n",
      "3 month\n",
      "1 move\n",
      "1 much\n",
      "1 name\n",
      "5 nasdaq\n",
      "2 near\n",
      "2 new\n",
      "1 news\n",
      "2 next\n",
      "1 night\n",
      "1 novemb\n",
      "1 nyse\n",
      "1 occas\n",
      "9 oculus\n",
      "1 offici\n",
      "1 oper\n",
      "1 order\n",
      "1 palmer\n",
      "1 part\n",
      "1 particular\n",
      "4 past\n",
      "1 peopl\n",
      "6 per\n",
      "5 percent\n",
      "2 period\n",
      "1 platform\n",
      "1 playstat\n",
      "1 polish\n",
      "1 polit\n",
      "4 posit\n",
      "1 post\n",
      "1 present\n",
      "1 presid\n",
      "1 press\n",
      "10 price\n",
      "1 prior\n",
      "1 probabl\n",
      "2 product\n",
      "2 project\n",
      "1 promot\n",
      "1 prospect\n",
      "1 public\n",
      "2 push\n",
      "8 quarter\n",
      "3 rang\n",
      "1 reaction\n",
      "1 readi\n",
      "5 realiti\n",
      "1 rebound\n",
      "2 recent\n",
      "1 record\n",
      "1 releas\n",
      "1 relev\n",
      "4 report\n",
      "1 respond\n",
      "3 result\n",
      "1 return\n",
      "1 reveal\n",
      "7 revenu\n",
      "2 rift\n",
      "2 right\n",
      "1 root\n",
      "1 rough\n",
      "1 said\n",
      "1 secret\n",
      "2 seem\n",
      "1 send\n",
      "1 serv\n",
      "1 session\n",
      "13 share\n",
      "2 short\n",
      "1 show\n",
      "1 silicon\n",
      "1 sinc\n",
      "2 six\n",
      "1 skill\n",
      "1 someon\n",
      "1 soni\n",
      "1 sound\n",
      "1 start\n",
      "1 step\n",
      "6 stock\n",
      "1 stole\n",
      "1 street\n",
      "1 struggl\n",
      "5 surpris\n",
      "1 system\n",
      "1 target\n",
      "1 team\n",
      "3 technolog\n",
      "1 tell\n",
      "3 th\n",
      "1 thing\n",
      "1 though\n",
      "1 tie\n",
      "5 time\n",
      "1 togeth\n",
      "1 took\n",
      "3 top\n",
      "1 touch\n",
      "1 traction\n",
      "4 trade\n",
      "1 trader\n",
      "1 turn\n",
      "1 understand\n",
      "1 unexpect\n",
      "1 us\n",
      "1 valley\n",
      "2 versus\n",
      "1 veteran\n",
      "1 vice\n",
      "1 view\n",
      "1 violat\n",
      "5 virtual\n",
      "1 volum\n",
      "6 vr\n",
      "1 wall\n",
      "1 want\n",
      "3 week\n",
      "2 well\n",
      "1 western\n",
      "1 work\n",
      "1 wrote\n",
      "3 xiaomi\n",
      "2 year\n",
      "2 zuckerberg\n"
     ]
    }
   ],
   "source": [
    "# Look at the vocabulary\n",
    "vocab = vectorizer.get_feature_names()\n",
    "\n",
    "# Look at the counts for each word\n",
    "dist = np.sum(train_data_features, axis = 0)\n",
    "for tag, count in zip(vocab, dist):\n",
    "    print count, tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit a Random Forest Model to Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FB_jan25_26 = np.asarray([131.48, 132.78], dtype=\"|S6\")\n",
    "\n",
    "# Try random forest on bag of words\n",
    "forest = RandomForestClassifier(n_estimators=100)\n",
    "# fit the forest to the training set, using the bag of words features\n",
    "forest = forest.fit(train_data_features, FB_jan25_26)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
